{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to DBMS Chapters Chapter 1: Introduction Chapter 2: Introduction to Relational Languages Chapter 3: Introduction to SQL Chapter 4: Intermediate SQL Chapter 5: Advanced SQL Sections 5.4 onwards may be omitted. Chapter 6: Entity-Relationship Model Chapter 7: Relational Database Design Chapter 8: Complex Data Types Chapter 9: Application Design Chapter 10: Big Data Chapter 11: Data Analytics Chapter 12: Physical Storage Systems Chapter 13: Storage and File Structure Chapter 14: Indexing Chapter 15: Query Processing Chapter 16: Query Optimization Chapter 17: Transactions Chapter 18: Concurrency Control Section 18.8 (Snapshot Isolation), Section 18.9 (Weak Levels of Consistency) may be omitted. Chapter 19: Recovery System Section 19.8 (ARIES) may be omitted. PostgreSQL Cheatsheet (Markdown) This cheatsheet summarizes some essential PostgreSQL commands for managing databases, users, tables, and data. Connection: psql -h <hostname> -p <port> -d <database> -U <username> : Connect to a PostgreSQL server (replace placeholders with actual values). Databases: CREATE DATABASE <database_name> : Create a new database. DROP DATABASE <database_name> : Delete an existing database (use with caution!). \\l : List all databases. \\connect <database_name> : Switch to a different database within the same session. Users and Roles: CREATE ROLE <username> [WITH PASSWORD '<password>'] : Create a new user. GRANT <privilege> ON <object> TO <username> : Grant specific privileges (e.g., SELECT, INSERT, UPDATE, DELETE) on a database object (table, schema) to a user. REVOKE <privilege> ON <object> FROM <username> : Revoke privileges from a user. \\du : List all roles (users). Tables: CREATE TABLE <table_name> ( <column_name> <data_type> [CONSTRAINT], ...); : Create a new table with columns and constraints. DESCRIBE <table_name> : Show the structure of a table. DROP TABLE <table_name> : Delete a table (use with caution!). \\dt : List all tables in the current schema. Data Manipulation: INSERT INTO <table_name> (<column1>, <column2>, ...) VALUES (<value1>, <value2>, ...); : Insert data into a table. SELECT * FROM <table_name> [WHERE <condition>]; : Retrieve data from a table (all columns by default, with optional filtering). UPDATE <table_name> SET <column_name> = <new_value> [WHERE <condition>]; : Update existing data in a table. DELETE FROM <table_name> [WHERE <condition>]; : Delete rows from a table. Other Useful Commands: \\q : Quit the psql client. \\h : Get help on a specific command (e.g., \\h CREATE TABLE ). \\conninfo : Display connection information. Additional Notes: Remember to replace placeholders like <database_name> , <username> , etc. with actual values. This is a basic cheatsheet. PostgreSQL offers many more commands and functionalities. Refer to the official documentation for in-depth details: https://www.postgresql.org/docs/","title":"Welcome to DBMS"},{"location":"#welcome-to-dbms","text":"","title":"Welcome to DBMS"},{"location":"#chapters","text":"Chapter 1: Introduction Chapter 2: Introduction to Relational Languages Chapter 3: Introduction to SQL Chapter 4: Intermediate SQL Chapter 5: Advanced SQL Sections 5.4 onwards may be omitted. Chapter 6: Entity-Relationship Model Chapter 7: Relational Database Design Chapter 8: Complex Data Types Chapter 9: Application Design Chapter 10: Big Data Chapter 11: Data Analytics Chapter 12: Physical Storage Systems Chapter 13: Storage and File Structure Chapter 14: Indexing Chapter 15: Query Processing Chapter 16: Query Optimization Chapter 17: Transactions Chapter 18: Concurrency Control Section 18.8 (Snapshot Isolation), Section 18.9 (Weak Levels of Consistency) may be omitted. Chapter 19: Recovery System Section 19.8 (ARIES) may be omitted.","title":"Chapters"},{"location":"#postgresql-cheatsheet-markdown","text":"This cheatsheet summarizes some essential PostgreSQL commands for managing databases, users, tables, and data. Connection: psql -h <hostname> -p <port> -d <database> -U <username> : Connect to a PostgreSQL server (replace placeholders with actual values). Databases: CREATE DATABASE <database_name> : Create a new database. DROP DATABASE <database_name> : Delete an existing database (use with caution!). \\l : List all databases. \\connect <database_name> : Switch to a different database within the same session. Users and Roles: CREATE ROLE <username> [WITH PASSWORD '<password>'] : Create a new user. GRANT <privilege> ON <object> TO <username> : Grant specific privileges (e.g., SELECT, INSERT, UPDATE, DELETE) on a database object (table, schema) to a user. REVOKE <privilege> ON <object> FROM <username> : Revoke privileges from a user. \\du : List all roles (users). Tables: CREATE TABLE <table_name> ( <column_name> <data_type> [CONSTRAINT], ...); : Create a new table with columns and constraints. DESCRIBE <table_name> : Show the structure of a table. DROP TABLE <table_name> : Delete a table (use with caution!). \\dt : List all tables in the current schema. Data Manipulation: INSERT INTO <table_name> (<column1>, <column2>, ...) VALUES (<value1>, <value2>, ...); : Insert data into a table. SELECT * FROM <table_name> [WHERE <condition>]; : Retrieve data from a table (all columns by default, with optional filtering). UPDATE <table_name> SET <column_name> = <new_value> [WHERE <condition>]; : Update existing data in a table. DELETE FROM <table_name> [WHERE <condition>]; : Delete rows from a table. Other Useful Commands: \\q : Quit the psql client. \\h : Get help on a specific command (e.g., \\h CREATE TABLE ). \\conninfo : Display connection information. Additional Notes: Remember to replace placeholders like <database_name> , <username> , etc. with actual values. This is a basic cheatsheet. PostgreSQL offers many more commands and functionalities. Refer to the official documentation for in-depth details: https://www.postgresql.org/docs/","title":"PostgreSQL Cheatsheet (Markdown)"},{"location":"cheatsheet/","text":"PSQL Magic words: psql -U postgres Some interesting flags (to see all, use -h or --help depending on your psql version): -E : will describe the underlaying queries of the \\ commands (cool for learning!) -l : psql will list all databases and then exit (useful if the user you connect with doesn't has a default database, like at AWS RDS) Most \\d commands support additional param of __schema__.name__ and accept wildcards like *.* \\? : Show help (list of available commands with an explanation) \\q : Quit/Exit \\c __database__ : Connect to a database \\d __table__ : Show table definition (columns, etc.) including triggers \\d+ __table__ : More detailed table definition including description and physical disk size \\l : List databases \\dy : List events \\df : List functions \\di : List indexes \\dn : List schemas \\dt *.* : List tables from all schemas (if *.* is omitted will only show SEARCH_PATH ones) \\dT+ : List all data types \\dv : List views \\dx : List all extensions installed \\df+ __function__ : Show function SQL code. \\x : Pretty-format query results instead of the not-so-useful ASCII tables \\copy (SELECT * FROM __table_name__) TO 'file_path_and_name.csv' WITH CSV : Export a table as CSV \\des+ : List all foreign servers \\dE[S+] : List all foreign tables \\! __bash_command__ : execute __bash_command__ (e.g. \\! ls ) User Related: \\du : List users \\du __username__ : List a username if present. create role __test1__ : Create a role with an existing username. create role __test2__ noinherit login password __passsword__; : Create a role with username and password. set role __test__; : Change role for current session to __test__ . grant __test2__ to __test1__; : Allow __test1__ to set its role as __test2__ . \\deu+ : List all user mapping on server Create command There are many CREATE choices, like CREATE DATABASE __database_name__ , CREATE TABLE __table_name__ ... Parameters differ but can be checked at the official documentation . Handy queries SELECT * FROM pg_proc WHERE proname='__procedurename__' : List procedure/function SELECT * FROM pg_views WHERE viewname='__viewname__'; : List view (including the definition) SELECT pg_size_pretty(pg_total_relation_size('__table_name__')); : Show DB table space in use SELECT pg_size_pretty(pg_database_size('__database_name__')); : Show DB space in use show statement_timeout; : Show current user's statement timeout SELECT * FROM pg_indexes WHERE tablename='__table_name__' AND schemaname='__schema_name__'; : Show table indexes Get all indexes from all tables of a schema: SELECT t . relname AS table_name , i . relname AS index_name , a . attname AS column_name FROM pg_class t , pg_class i , pg_index ix , pg_attribute a , pg_namespace n WHERE t . oid = ix . indrelid AND i . oid = ix . indexrelid AND a . attrelid = t . oid AND a . attnum = ANY ( ix . indkey ) AND t . relnamespace = n . oid AND n . nspname = 'kartones' ORDER BY t . relname , i . relname Execution data: Queries being executed at a certain DB: SELECT datname , application_name , pid , backend_start , query_start , state_change , state , query FROM pg_stat_activity WHERE datname = '__database_name__' ; Get all queries from all dbs waiting for data (might be hung): SELECT * FROM pg_stat_activity WHERE waiting = 't' Currently running queries with process pid: SELECT pg_stat_get_backend_pid ( s . backendid ) AS procpid , pg_stat_get_backend_activity ( s . backendid ) AS current_query FROM ( SELECT pg_stat_get_backend_idset () AS backendid ) AS s ; Get Connections by Database: SELECT datname, numbackends FROM pg_stat_database; Casting: CAST (column AS type) or column::type '__table_name__'::regclass::oid : Get oid having a table name Query analysis: EXPLAIN __query__ : see the query plan for the given query EXPLAIN ANALYZE __query__ : see and execute the query plan for the given query ANALYZE [__table__] : collect statistics Generating random data ( source ): INSERT INTO some_table (a_float_value) SELECT random() * 100000 FROM generate_series(1, 1000000) i; Get sizes of tables, indexes and full DBs: select current_database () as database , pg_size_pretty ( total_database_size ) as total_database_size , schema_name , table_name , pg_size_pretty ( total_table_size ) as total_table_size , pg_size_pretty ( table_size ) as table_size , pg_size_pretty ( index_size ) as index_size from ( select table_name , table_schema as schema_name , pg_database_size ( current_database ()) as total_database_size , pg_total_relation_size ( table_name ) as total_table_size , pg_relation_size ( table_name ) as table_size , pg_indexes_size ( table_name ) as index_size from information_schema . tables where table_schema = current_schema () and table_name like 'table_%' order by total_table_size ) as sizes ; COPY command : Import/export from CSV to tables: COPY table_name [ ( column_name [, ...] ) ] FROM { 'filename' | STDIN } [ [ WITH ] ( option [, ...] ) ] COPY { table_name [ ( column_name [, ...] ) ] | ( query ) } TO { 'filename' | STDOUT } [ [ WITH ] ( option [, ...] ) ] List all grants for a specific user SELECT table_catalog , table_schema , table_name , privilege_type FROM information_schema . table_privileges WHERE grantee = 'user_to_check' ORDER BY table_name ; List all assigned user roles SELECT r . rolname , r . rolsuper , r . rolinherit , r . rolcreaterole , r . rolcreatedb , r . rolcanlogin , r . rolconnlimit , r . rolvaliduntil , ARRAY ( SELECT b . rolname FROM pg_catalog . pg_auth_members m JOIN pg_catalog . pg_roles b ON ( m . roleid = b . oid ) WHERE m . member = r . oid ) as memberof , r . rolreplication FROM pg_catalog . pg_roles r ORDER BY 1 ; Check permissions in a table: SELECT grantee , privilege_type FROM information_schema . role_table_grants WHERE table_name = 'name-of-the-table' ; Kill all Connections: SELECT pg_terminate_backend ( pg_stat_activity . pid ) FROM pg_stat_activity WHERE datname = current_database () AND pid <> pg_backend_pid (); Keyboard shortcuts CTRL + R : reverse-i-search Tools ptop and pg_top : top for PG. Available on the APT repository from apt.postgresql.org . pg_activity : Command line tool for PostgreSQL server activity monitoring. Unix-like reverse search in psql : $ echo \"bind \" ^R \" em-inc-search-prev\" > $HOME /.editrc $ source $HOME /.editrc Show IP of the DB Instance: SELECT inet_server_addr(); File to save PostgreSQL credentials and permissions (format: hostname:port:database:username:password ): chmod 600 ~/.pgpass Collect statistics of a database (useful to improve speed after a Database Upgrade as previous query plans are deleted): ANALYZE VERBOSE; To obtain the CREATE TABLE query of a table, any visual GUI like pgAdmin allows to easily, but else you can use pg_dump , e.g.: pg_dump -t '<schema>.<table>' --schema-only <database> ( source ) Resources & Documentation Operations Cheat Sheet : Official PG wiki cheat sheet with an amazing amount of explanations of many topics, features, and many many internal implementation details Postgres Weekly newsletter: The best way IMHO to keep up to date with PG news 100 psql Tips : Name says all, lots of useful tips! PostgreSQL Exercises : An awesome resource to learn to learn SQL, teaching you with simple examples in a great visual way. Highly recommended . A Performance Cheat Sheet for PostgreSQL : Great explanations of EXPLAIN , EXPLAIN ANALYZE , VACUUM , configuration parameters and more. Quite interesting if you need to tune-up a postgres setup. annotated.conf : Annotations of all 269 postgresql.conf settings for PostgreSQL 10. psql -c \"\\l+\" -H -q postgres > out.html : Generate a html report of your databases (source: Daniel Westermann ) </pre>","title":"Cheatsheet"},{"location":"cheatsheet/#psql","text":"Magic words: psql -U postgres Some interesting flags (to see all, use -h or --help depending on your psql version): -E : will describe the underlaying queries of the \\ commands (cool for learning!) -l : psql will list all databases and then exit (useful if the user you connect with doesn't has a default database, like at AWS RDS) Most \\d commands support additional param of __schema__.name__ and accept wildcards like *.* \\? : Show help (list of available commands with an explanation) \\q : Quit/Exit \\c __database__ : Connect to a database \\d __table__ : Show table definition (columns, etc.) including triggers \\d+ __table__ : More detailed table definition including description and physical disk size \\l : List databases \\dy : List events \\df : List functions \\di : List indexes \\dn : List schemas \\dt *.* : List tables from all schemas (if *.* is omitted will only show SEARCH_PATH ones) \\dT+ : List all data types \\dv : List views \\dx : List all extensions installed \\df+ __function__ : Show function SQL code. \\x : Pretty-format query results instead of the not-so-useful ASCII tables \\copy (SELECT * FROM __table_name__) TO 'file_path_and_name.csv' WITH CSV : Export a table as CSV \\des+ : List all foreign servers \\dE[S+] : List all foreign tables \\! __bash_command__ : execute __bash_command__ (e.g. \\! ls ) User Related: \\du : List users \\du __username__ : List a username if present. create role __test1__ : Create a role with an existing username. create role __test2__ noinherit login password __passsword__; : Create a role with username and password. set role __test__; : Change role for current session to __test__ . grant __test2__ to __test1__; : Allow __test1__ to set its role as __test2__ . \\deu+ : List all user mapping on server","title":"PSQL"},{"location":"cheatsheet/#create-command","text":"There are many CREATE choices, like CREATE DATABASE __database_name__ , CREATE TABLE __table_name__ ... Parameters differ but can be checked at the official documentation .","title":"Create command"},{"location":"cheatsheet/#handy-queries","text":"SELECT * FROM pg_proc WHERE proname='__procedurename__' : List procedure/function SELECT * FROM pg_views WHERE viewname='__viewname__'; : List view (including the definition) SELECT pg_size_pretty(pg_total_relation_size('__table_name__')); : Show DB table space in use SELECT pg_size_pretty(pg_database_size('__database_name__')); : Show DB space in use show statement_timeout; : Show current user's statement timeout SELECT * FROM pg_indexes WHERE tablename='__table_name__' AND schemaname='__schema_name__'; : Show table indexes Get all indexes from all tables of a schema: SELECT t . relname AS table_name , i . relname AS index_name , a . attname AS column_name FROM pg_class t , pg_class i , pg_index ix , pg_attribute a , pg_namespace n WHERE t . oid = ix . indrelid AND i . oid = ix . indexrelid AND a . attrelid = t . oid AND a . attnum = ANY ( ix . indkey ) AND t . relnamespace = n . oid AND n . nspname = 'kartones' ORDER BY t . relname , i . relname Execution data: Queries being executed at a certain DB: SELECT datname , application_name , pid , backend_start , query_start , state_change , state , query FROM pg_stat_activity WHERE datname = '__database_name__' ; Get all queries from all dbs waiting for data (might be hung): SELECT * FROM pg_stat_activity WHERE waiting = 't' Currently running queries with process pid: SELECT pg_stat_get_backend_pid ( s . backendid ) AS procpid , pg_stat_get_backend_activity ( s . backendid ) AS current_query FROM ( SELECT pg_stat_get_backend_idset () AS backendid ) AS s ; Get Connections by Database: SELECT datname, numbackends FROM pg_stat_database; Casting: CAST (column AS type) or column::type '__table_name__'::regclass::oid : Get oid having a table name Query analysis: EXPLAIN __query__ : see the query plan for the given query EXPLAIN ANALYZE __query__ : see and execute the query plan for the given query ANALYZE [__table__] : collect statistics Generating random data ( source ): INSERT INTO some_table (a_float_value) SELECT random() * 100000 FROM generate_series(1, 1000000) i; Get sizes of tables, indexes and full DBs: select current_database () as database , pg_size_pretty ( total_database_size ) as total_database_size , schema_name , table_name , pg_size_pretty ( total_table_size ) as total_table_size , pg_size_pretty ( table_size ) as table_size , pg_size_pretty ( index_size ) as index_size from ( select table_name , table_schema as schema_name , pg_database_size ( current_database ()) as total_database_size , pg_total_relation_size ( table_name ) as total_table_size , pg_relation_size ( table_name ) as table_size , pg_indexes_size ( table_name ) as index_size from information_schema . tables where table_schema = current_schema () and table_name like 'table_%' order by total_table_size ) as sizes ; COPY command : Import/export from CSV to tables: COPY table_name [ ( column_name [, ...] ) ] FROM { 'filename' | STDIN } [ [ WITH ] ( option [, ...] ) ] COPY { table_name [ ( column_name [, ...] ) ] | ( query ) } TO { 'filename' | STDOUT } [ [ WITH ] ( option [, ...] ) ] List all grants for a specific user SELECT table_catalog , table_schema , table_name , privilege_type FROM information_schema . table_privileges WHERE grantee = 'user_to_check' ORDER BY table_name ; List all assigned user roles SELECT r . rolname , r . rolsuper , r . rolinherit , r . rolcreaterole , r . rolcreatedb , r . rolcanlogin , r . rolconnlimit , r . rolvaliduntil , ARRAY ( SELECT b . rolname FROM pg_catalog . pg_auth_members m JOIN pg_catalog . pg_roles b ON ( m . roleid = b . oid ) WHERE m . member = r . oid ) as memberof , r . rolreplication FROM pg_catalog . pg_roles r ORDER BY 1 ; Check permissions in a table: SELECT grantee , privilege_type FROM information_schema . role_table_grants WHERE table_name = 'name-of-the-table' ; Kill all Connections: SELECT pg_terminate_backend ( pg_stat_activity . pid ) FROM pg_stat_activity WHERE datname = current_database () AND pid <> pg_backend_pid ();","title":"Handy queries"},{"location":"cheatsheet/#keyboard-shortcuts","text":"CTRL + R : reverse-i-search","title":"Keyboard shortcuts"},{"location":"cheatsheet/#tools","text":"ptop and pg_top : top for PG. Available on the APT repository from apt.postgresql.org . pg_activity : Command line tool for PostgreSQL server activity monitoring. Unix-like reverse search in psql : $ echo \"bind \" ^R \" em-inc-search-prev\" > $HOME /.editrc $ source $HOME /.editrc Show IP of the DB Instance: SELECT inet_server_addr(); File to save PostgreSQL credentials and permissions (format: hostname:port:database:username:password ): chmod 600 ~/.pgpass Collect statistics of a database (useful to improve speed after a Database Upgrade as previous query plans are deleted): ANALYZE VERBOSE; To obtain the CREATE TABLE query of a table, any visual GUI like pgAdmin allows to easily, but else you can use pg_dump , e.g.: pg_dump -t '<schema>.<table>' --schema-only <database> ( source )","title":"Tools"},{"location":"cheatsheet/#resources-documentation","text":"Operations Cheat Sheet : Official PG wiki cheat sheet with an amazing amount of explanations of many topics, features, and many many internal implementation details Postgres Weekly newsletter: The best way IMHO to keep up to date with PG news 100 psql Tips : Name says all, lots of useful tips! PostgreSQL Exercises : An awesome resource to learn to learn SQL, teaching you with simple examples in a great visual way. Highly recommended . A Performance Cheat Sheet for PostgreSQL : Great explanations of EXPLAIN , EXPLAIN ANALYZE , VACUUM , configuration parameters and more. Quite interesting if you need to tune-up a postgres setup. annotated.conf : Annotations of all 269 postgresql.conf settings for PostgreSQL 10. psql -c \"\\l+\" -H -q postgres > out.html : Generate a html report of your databases (source: Daniel Westermann ) </pre>","title":"Resources &amp; Documentation"},{"location":"Piyush%20Wairale/ER_model/","text":"Entity Relationship Model Entity. Strong Entity weak Entity Attributes. key attributes partial key attribute multi valued attribute derived composite attributes Relationship. one to one one to many many to one many to many Entity Entity: Distinguishable objects in the real world (e.g., student, car). entity is represented by a set of attributes Student entity will have Name attribute age attribute Roll_no attribute etc Entity Set: A set of entities with the same attributes (e.g., all students) Relationship: A relationship is a assosication among several entities.","title":"Entity Relationship Model"},{"location":"Piyush%20Wairale/ER_model/#entity-relationship-model","text":"Entity. Strong Entity weak Entity Attributes. key attributes partial key attribute multi valued attribute derived composite attributes Relationship. one to one one to many many to one many to many","title":"Entity Relationship Model"},{"location":"Piyush%20Wairale/ER_model/#entity","text":"Entity: Distinguishable objects in the real world (e.g., student, car). entity is represented by a set of attributes Student entity will have Name attribute age attribute Roll_no attribute etc Entity Set: A set of entities with the same attributes (e.g., all students) Relationship: A relationship is a assosication among several entities.","title":"Entity"},{"location":"Piyush%20Wairale/Relational_algebra/","text":"Introduction to Relational Algebra : The lecture covers relational algebra, a key topic for GATE exam preparation in database management systems. It distinguishes between procedural (relational algebra) and non-procedural (relational calculus) query languages. Query Language Basics : SQL (Structured Query Language) is introduced, highlighting the difference between procedural (specifies how to get data) and non-procedural (specifies what data is needed) languages. - Key Operators in Relational Algebra : Projection (\u03c0) : Used to select specific columns from a table. Selection (\u03c3) : Applies conditions to filter rows. Rename (\u03c1) : Renames relations and attributes. Logical and Comparison Operators : The lecture reviews logical (AND, OR, NOT) and comparison (>, <, =, !=, etc.) operators, essential for forming relational algebra expressions. Examples and Application : Fundamental operators (projection, selection, rename) are explained with examples. Future lectures will cover set operations and joins in more detail.","title":"Relational algebra"},{"location":"Piyush%20Wairale/candiatekey/","text":"Given Relation and Functional Dependencies: Relation: \\( R(K, J, L, O, M, N) \\) Functional Dependencies: ( K \\(\\rightarrow\\) J, L ) ( L \\(\\rightarrow\\) K ) \\( J $\\rightarrow$ O, \\) \\( M $\\rightarrow$ N \\) Steps to Find Candidate Keys: Identify attributes not present on the right-hand side (RHS) of any functional dependency. These attributes must be part of the candidate key. Attributes Analysis: \\( M, K \\) and \\( M, L \\) are identified as potential candidate keys. The closure of these sets is calculated to verify if they can determine all attributes in \\( R \\) . Prime and Non-Prime Attributes: Prime attributes (part of candidate keys): \\( M, K, L \\) Non-prime attributes (not part of candidate keys): \\( J, N, O \\) Closure Calculations: \\( (M)^+ = \\{M, N\\} \\) \\( (MJ)^+ = \\{M, J, N, O\\} \\) \\( (MK)^+ = \\{M, K, N, J, L, O\\} = R \\) (Candidate Key) \\( (ML)^+ = \\{M, L, N, K, J, O\\} = R \\) (Candidate Key) \\( (MN)^+ = \\{M, N\\} \\) \\( (MO)^+ = \\{M, O, N\\} \\) Conclusion: The candidate keys for the relation \\( R \\) are \\( MK \\) and \\( ML \\) . Problem Statement Relation R : Has eight attributes \\( A, B, C, D, E, F, G, H \\) . Functional Dependencies (FDs) : \\( CH $\\rightarrow$ G \\) \\( A $\\rightarrow$ BC \\) \\( B $\\rightarrow$ CFH \\) \\( E $\\rightarrow$ A \\) \\( F $\\rightarrow$ EG \\) Objective Determine how many candidate keys the relation \\( R \\) has. Steps to Solve Identify the Closure of Attributes : The closure of an attribute set \\( X \\) (denoted as \\( X^+ \\) ) is the set of attributes that can be functionally determined by \\( X \\) . Determine the Candidate Keys : A candidate key is a minimal set of attributes that can determine all other attributes in the relation. Detailed Analysis Compute Closures : Closure of \\( DA \\) : \\( DA^+ = \\{D, A, B, C, F, H, E, G\\} \\) (from \\( A $\\rightarrow$ BC \\) , \\( B $\\rightarrow$ CFH \\) , \\( F $\\rightarrow$ EG \\) , and \\( E $\\rightarrow$ A \\) ) Closure of \\( DB \\) : \\( DB^+ = \\{D, B, C, F, H, E, G, A\\} \\) (from \\( B $\\rightarrow$ CFH \\) , \\( F $\\rightarrow$ EG \\) , and \\( E $\\rightarrow$ A \\) ) Closure of \\( DC \\) : \\( DC^+ = \\{D, C, G\\} \\) (from \\( CH $\\rightarrow$ G \\) ) Closure of \\( DE \\) : \\( DE^+ = \\{D, E, A, B, C, F, H, G\\} \\) (from \\( E $\\rightarrow$ A \\) , \\( A $\\rightarrow$ BC \\) , \\( B $\\rightarrow$ CFH \\) , and \\( F $\\rightarrow$ EG \\) ) Closure of \\( DF \\) : \\( DF^+ = \\{D, F, E, G, A, B, C, H\\} \\) (from \\( F $\\rightarrow$ EG \\) , \\( E $\\rightarrow$ A \\) , \\( A $\\rightarrow$ BC \\) , and \\( B $\\rightarrow$ CFH \\) ) Closure of \\( DG \\) : \\( DG^+ = \\{D, G\\} \\) Closure of \\( DH \\) : \\( DH^+ = \\{D, H, G\\} \\) (from \\( CH $\\rightarrow$ G \\) ) Check for Superkeys : A superkey is a set of attributes that can determine all attributes in the relation. From the closures computed, we see that \\( DA^+ \\) , \\( DB^+ \\) , \\( DE^+ \\) , and \\( DF^+ \\) include all attributes \\( A, B, C, D, E, F, G, H \\) . Therefore, \\( DA \\) , \\( DB \\) , \\( DE \\) , and \\( DF \\) are superkeys. Minimal Superkeys (Candidate Keys) : To find candidate keys, we need to check minimal superkeys. From the given FDs, we can infer that: \\( DA \\) , \\( DB \\) , \\( DE \\) , and \\( DF \\) are candidate keys because their closures include all attributes and they are minimal. Conclusion The relation \\( R \\) has four candidate keys , which are \\( DA \\) , \\( DB \\) , \\( DE \\) , and \\( DF \\) . Thus, the relation \\( R \\) has exactly four candidate keys .","title":"Candiatekey"},{"location":"Piyush%20Wairale/candiatekey/#problem-statement","text":"Relation R : Has eight attributes \\( A, B, C, D, E, F, G, H \\) . Functional Dependencies (FDs) : \\( CH $\\rightarrow$ G \\) \\( A $\\rightarrow$ BC \\) \\( B $\\rightarrow$ CFH \\) \\( E $\\rightarrow$ A \\) \\( F $\\rightarrow$ EG \\)","title":"Problem Statement"},{"location":"Piyush%20Wairale/candiatekey/#objective","text":"Determine how many candidate keys the relation \\( R \\) has.","title":"Objective"},{"location":"Piyush%20Wairale/candiatekey/#steps-to-solve","text":"Identify the Closure of Attributes : The closure of an attribute set \\( X \\) (denoted as \\( X^+ \\) ) is the set of attributes that can be functionally determined by \\( X \\) . Determine the Candidate Keys : A candidate key is a minimal set of attributes that can determine all other attributes in the relation.","title":"Steps to Solve"},{"location":"Piyush%20Wairale/candiatekey/#detailed-analysis","text":"Compute Closures : Closure of \\( DA \\) : \\( DA^+ = \\{D, A, B, C, F, H, E, G\\} \\) (from \\( A $\\rightarrow$ BC \\) , \\( B $\\rightarrow$ CFH \\) , \\( F $\\rightarrow$ EG \\) , and \\( E $\\rightarrow$ A \\) ) Closure of \\( DB \\) : \\( DB^+ = \\{D, B, C, F, H, E, G, A\\} \\) (from \\( B $\\rightarrow$ CFH \\) , \\( F $\\rightarrow$ EG \\) , and \\( E $\\rightarrow$ A \\) ) Closure of \\( DC \\) : \\( DC^+ = \\{D, C, G\\} \\) (from \\( CH $\\rightarrow$ G \\) ) Closure of \\( DE \\) : \\( DE^+ = \\{D, E, A, B, C, F, H, G\\} \\) (from \\( E $\\rightarrow$ A \\) , \\( A $\\rightarrow$ BC \\) , \\( B $\\rightarrow$ CFH \\) , and \\( F $\\rightarrow$ EG \\) ) Closure of \\( DF \\) : \\( DF^+ = \\{D, F, E, G, A, B, C, H\\} \\) (from \\( F $\\rightarrow$ EG \\) , \\( E $\\rightarrow$ A \\) , \\( A $\\rightarrow$ BC \\) , and \\( B $\\rightarrow$ CFH \\) ) Closure of \\( DG \\) : \\( DG^+ = \\{D, G\\} \\) Closure of \\( DH \\) : \\( DH^+ = \\{D, H, G\\} \\) (from \\( CH $\\rightarrow$ G \\) ) Check for Superkeys : A superkey is a set of attributes that can determine all attributes in the relation. From the closures computed, we see that \\( DA^+ \\) , \\( DB^+ \\) , \\( DE^+ \\) , and \\( DF^+ \\) include all attributes \\( A, B, C, D, E, F, G, H \\) . Therefore, \\( DA \\) , \\( DB \\) , \\( DE \\) , and \\( DF \\) are superkeys. Minimal Superkeys (Candidate Keys) : To find candidate keys, we need to check minimal superkeys. From the given FDs, we can infer that: \\( DA \\) , \\( DB \\) , \\( DE \\) , and \\( DF \\) are candidate keys because their closures include all attributes and they are minimal.","title":"Detailed Analysis"},{"location":"Piyush%20Wairale/candiatekey/#conclusion","text":"The relation \\( R \\) has four candidate keys , which are \\( DA \\) , \\( DB \\) , \\( DE \\) , and \\( DF \\) . Thus, the relation \\( R \\) has exactly four candidate keys .","title":"Conclusion"},{"location":"Piyush%20Wairale/checklist/","text":"[ ] Lecture: 01 | Database Management & Warehousing | GATE Data Science & AI #gate2024 #datascience [ ] Lecture: 02 | Database Management & Warehousing | GATE Data Science & AI #gate2024 #datascience [X] Lecture: 03- Intro to ER Model| Database Management & Warehousing | GATE Data Science & AI #gate2024 [X] Lecture: 04- Previous Year Questions | Database Management & Warehousing | GATE Data Science & AI [X] Lecture: 05- ER Model | Database Management & Warehousing | GATE Data Science & AI #gate2024 [X] Lecture: 06- ER Model | Database Management & Warehousing | GATE Data Science & AI #gate2024 [X] Relational Algebra | Database Management & Warehousing | GATE Data Science & AI | Lec : 07 [ ] Tutorial: Relational Algebra | Database Management & Warehousing | GATE Data Science & AI | [ ] Relational Algebra-Part 2| Database Management & Warehousing | GATE Data Science & AI | Lec : 08 [ ] Relational Algebra-Part 3| Database Management & Warehousing | GATE Data Science & AI | Lec : 09 [ ] Tuple Relational Calculus | DBMS | GATE DA [ ] Intro to SQL | Database Management & Warehousing | GATE Data Science & AI #gate2024 [ ] Datatypes and Constraints | Database Management & Warehousing | GATE Data Science & AI #gate2024 [ ] Tutorial: Datatype and Constraints Example | Database Management & Warehousing | GATE DA [ ] SQL command part 1 | Database Management & Warehousing | GATE Data Science & AI #gate2024 [ ] SQL command part 2 | Database Management & Warehousing | GATE Data Science & AI #gate2024 [ ] SQL: SELECT Command |Database Management & Warehousing | GATE Data Science & AI #gate2024 [X] Functional Dependency | DBMS and Warehousing | GATE Data Science & AI [ ] PYQ: SQL, Relational Algebra & Calculus | Database Management & Warehousing | GATE DA #gate2025 [ ] Lec 01: Intro to Data Warehouse | Database Management & Warehousing | GATE DA #gate2025 [ ] Lecture 02: What is Data Warehouse | Database Management & Warehousing | GATE DA #gate2025 [ ] Normalization | Database Management & Warehousing | GATE DA PYQ #gate2025 | GATE Data Science [ ] Lecture 03: Data Warehouse, Data Lake and Data LakeHouse |Database Management & Warehousing| GATE DA","title":"Checklist"},{"location":"Piyush%20Wairale/normalization/","text":"3nf vs BCNFG","title":"Normalization"},{"location":"Piyush%20Wairale/sql/","text":"SQL Datatype and Constraints CREATE DATABASE databasename ;","title":"SQL"},{"location":"Piyush%20Wairale/sql/#sql","text":"","title":"SQL"},{"location":"Piyush%20Wairale/sql/#datatype-and-constraints","text":"CREATE DATABASE databasename ;","title":"Datatype and Constraints"},{"location":"Week%201/Lecture%201.1%20-%20Course%20Overview/","text":"Lecture 1.1 - Course Overview.pdf (PDF file) Summary Database Management Systems (DBMSs) are crucial in modern applications, providing organized access to large volumes of interconnected data through a user-friendly interface. They offer advantages such as eliminating data redundancy, enhancing data accessibility, promoting data security, and ensuring data integrity. This course provides an overview of DBMS concepts and applications, including fundamental sets, relations, functions, propositional and predicate logic, data structures, object-oriented analysis, and Python programming. The essential prerequisites for the course include set theory, relations and functions, propositional logic, predicate logic, data structures, programming in Python, and algorithms and programming in C. Desirable prerequisites include object-oriented analysis and design. The course outline covers: Why Databases? Know Your Course (KYC) KYC: Course Prerequisite KYC: Course Outline KYC: Course Text Book Module Summary The textbook for the course is \"Database System Concepts\" by Abraham Silberschatz, Henry Korth, and S. Sudarshan.","title":"Lecture 1.1 - Course Overview.pdf (PDF file)"},{"location":"Week%201/Lecture%201.1%20-%20Course%20Overview/#lecture-11-course-overviewpdf-pdf-file","text":"Summary Database Management Systems (DBMSs) are crucial in modern applications, providing organized access to large volumes of interconnected data through a user-friendly interface. They offer advantages such as eliminating data redundancy, enhancing data accessibility, promoting data security, and ensuring data integrity. This course provides an overview of DBMS concepts and applications, including fundamental sets, relations, functions, propositional and predicate logic, data structures, object-oriented analysis, and Python programming. The essential prerequisites for the course include set theory, relations and functions, propositional logic, predicate logic, data structures, programming in Python, and algorithms and programming in C. Desirable prerequisites include object-oriented analysis and design. The course outline covers: Why Databases? Know Your Course (KYC) KYC: Course Prerequisite KYC: Course Outline KYC: Course Text Book Module Summary The textbook for the course is \"Database System Concepts\" by Abraham Silberschatz, Henry Korth, and S. Sudarshan.","title":"Lecture 1.1 - Course Overview.pdf (PDF file)"},{"location":"Week%201/Lecture%201.2%20-%20Why%20DBMS1/","text":"Lecture 1.2 - Why DBMS1.pdf (PDF file) Summary Module 2 of the course focuses on the evolution of data management and the history of database management systems (DBMSs). It highlights the need for DBMSs from a historical perspective, tracing the evolution of data management practices. The module covers the history of DBMSs, starting with physical data management (book keeping) using ledgers and journals. It discusses the significant advancement in 1886 when Henry Brown patented a device for storing and preserving papers. The invention of punch cards by Herman Hollerith in 1890 for use in tabulating machines further propelled electronic data management. The module explores key parameters for electronic data management, including durability, scalability, security, ease of use, consistency, efficiency, cost, and others. It examines the limitations of traditional file systems in meeting growing data needs and discusses the transition to DBMSs. The module reviews the history of DBMSs, starting with the use of magnetic tapes in the 1950s and early 1960s. It highlights the development of hard disks in the late 1960s and 1970s, enabling direct access to data. The introduction of the relational data model by Ted Codd and the development of commercial relational database systems in the 1980s are also mentioned. The module concludes with a summary of the evolution of data models, DB technology, and DB architecture. Data Management Storage Retrieval Transaction Audit Archival for : individual Small/ big enterprise global Major Approach: Physical also know as Book keeping Electronic Electronic Data or Records management depends on various parameters including: \u2022 Durability \u2022 Scalability \u2022 Security \u2022 Retrieval \u2022 Ease of Use \u2022 Consistency \u2022 Efficiency \u2022 Cost Problems with such an approach of book-keeping: \u2022 Durability: Physical damage to these registers is a possibility due to rodents, humidity, wear and tear \u2022 Scalability: Very difficult to maintain for many years, some shops have numerous registers spanning over years \u2022 Security: Susceptible to tampering by outsiders \u2022 Retrieval: Time consuming process to search for a previous entry \u2022 Consistency: Prone to human errors Spreadsheet files - a better solutionm to natural file creation","title":"Lecture 1.2 - Why DBMS1.pdf (PDF file)"},{"location":"Week%201/Lecture%201.2%20-%20Why%20DBMS1/#lecture-12-why-dbms1pdf-pdf-file","text":"Summary Module 2 of the course focuses on the evolution of data management and the history of database management systems (DBMSs). It highlights the need for DBMSs from a historical perspective, tracing the evolution of data management practices. The module covers the history of DBMSs, starting with physical data management (book keeping) using ledgers and journals. It discusses the significant advancement in 1886 when Henry Brown patented a device for storing and preserving papers. The invention of punch cards by Herman Hollerith in 1890 for use in tabulating machines further propelled electronic data management. The module explores key parameters for electronic data management, including durability, scalability, security, ease of use, consistency, efficiency, cost, and others. It examines the limitations of traditional file systems in meeting growing data needs and discusses the transition to DBMSs. The module reviews the history of DBMSs, starting with the use of magnetic tapes in the 1950s and early 1960s. It highlights the development of hard disks in the late 1960s and 1970s, enabling direct access to data. The introduction of the relational data model by Ted Codd and the development of commercial relational database systems in the 1980s are also mentioned. The module concludes with a summary of the evolution of data models, DB technology, and DB architecture. Data Management Storage Retrieval Transaction Audit Archival for : individual Small/ big enterprise global Major Approach: Physical also know as Book keeping Electronic Electronic Data or Records management depends on various parameters including: \u2022 Durability \u2022 Scalability \u2022 Security \u2022 Retrieval \u2022 Ease of Use \u2022 Consistency \u2022 Efficiency \u2022 Cost Problems with such an approach of book-keeping: \u2022 Durability: Physical damage to these registers is a possibility due to rodents, humidity, wear and tear \u2022 Scalability: Very difficult to maintain for many years, some shops have numerous registers spanning over years \u2022 Security: Susceptible to tampering by outsiders \u2022 Retrieval: Time consuming process to search for a previous entry \u2022 Consistency: Prone to human errors Spreadsheet files - a better solutionm to natural file creation","title":"Lecture 1.2 - Why DBMS1.pdf (PDF file)"},{"location":"Week%201/Lecture%201.3%20-%20Why%20DBMS2/","text":"Lecture 1.3 - Why DBMS2.pdf (PDF file) Summary This module introduces file systems and database management systems (DBMSs) and compares their features. File Systems vs. Databases File systems are less efficient for data management, especially with increasing data volume and structural changes. DBMSs are scalable and provide built-in mechanisms for data handling. Python vs. SQL Python is easier to implement for file handling, while SQL provides faster execution in milliseconds even for large datasets. Parameterized Comparison Scalability: DBMSs are more scalable in terms of both data volume and structural changes. Time and Efficiency: DBMSs provide faster data processing through built-in mechanisms like indexing. However, for small datasets, the setup time of a DBMS may outweigh its advantages. Persistence, Robustness, and Security: DBMSs ensure automatic data persistence, provide mechanisms for backup and recovery, and offer user-specific security. Programmer's Productivity: DBMSs reduce coding effort by providing built-in mechanisms for data consistency and relationship maintenance. Arithmetic Operations: Python offers extensive arithmetic and logical operations, while SQL has limited support for these. Costs and Complexity: File systems are less expensive to implement and maintain, while DBMSs require specialized hardware, software, and personnel, leading to higher costs.","title":"Lecture 1.3 - Why DBMS2.pdf (PDF file)"},{"location":"Week%201/Lecture%201.3%20-%20Why%20DBMS2/#lecture-13-why-dbms2pdf-pdf-file","text":"Summary This module introduces file systems and database management systems (DBMSs) and compares their features. File Systems vs. Databases File systems are less efficient for data management, especially with increasing data volume and structural changes. DBMSs are scalable and provide built-in mechanisms for data handling. Python vs. SQL Python is easier to implement for file handling, while SQL provides faster execution in milliseconds even for large datasets. Parameterized Comparison Scalability: DBMSs are more scalable in terms of both data volume and structural changes. Time and Efficiency: DBMSs provide faster data processing through built-in mechanisms like indexing. However, for small datasets, the setup time of a DBMS may outweigh its advantages. Persistence, Robustness, and Security: DBMSs ensure automatic data persistence, provide mechanisms for backup and recovery, and offer user-specific security. Programmer's Productivity: DBMSs reduce coding effort by providing built-in mechanisms for data consistency and relationship maintenance. Arithmetic Operations: Python offers extensive arithmetic and logical operations, while SQL has limited support for these. Costs and Complexity: File systems are less expensive to implement and maintain, while DBMSs require specialized hardware, software, and personnel, leading to higher costs.","title":"Lecture 1.3 - Why DBMS2.pdf (PDF file)"},{"location":"Week%201/Lecture%201.4%20-%20Intro%20to%20DBMS1/","text":"Lecture 1.4 - Intro to DBMS1.pdf (PDF file) Summary This document provides an introduction to Database Management Systems (DBMS) by discussing various concepts and components. Levels of Abstraction: Physical level: Database storage and organization. Logical level: Data representation and relationships.= type instructor = record ID : string; name : string; dept name : string; salary : integer; end; View level: Application-specific data presentation and security. Schema and Instance: Schema: Logical structure of the database, specifying data types, constraints, and relationships. logical schema - Analogous to type information of a variable in a program. Physical schema - The overall physical structure of the database. Instance: Actual data stored in the database at a specific time. Data Models: Tools for describing data, relationships, semantics, and constraints. Relational model: Stores data in tables, with rows representing records and columns representing attributes. DDL and DML: Data Definition Language (DDL): Used to create, modify, and delete database structures (e.g., tables). Data Manipulation Language (DML): Used to access and manipulate data (e.g., insert, update, delete). SQL (Structured Query Language): Commercial DML widely used in database systems. Not Turing-machine equivalent, but often embedded in other programming languages. Database Design: Process of creating the database schema and physical layout. Logical design: Deciding on the schema, including attributes and relationships. Physical design: Determining the physical storage and optimization strategies.","title":"Lecture 1.4 - Intro to DBMS1.pdf (PDF file)"},{"location":"Week%201/Lecture%201.4%20-%20Intro%20to%20DBMS1/#lecture-14-intro-to-dbms1pdf-pdf-file","text":"Summary This document provides an introduction to Database Management Systems (DBMS) by discussing various concepts and components. Levels of Abstraction: Physical level: Database storage and organization. Logical level: Data representation and relationships.= type instructor = record ID : string; name : string; dept name : string; salary : integer; end; View level: Application-specific data presentation and security. Schema and Instance: Schema: Logical structure of the database, specifying data types, constraints, and relationships. logical schema - Analogous to type information of a variable in a program. Physical schema - The overall physical structure of the database. Instance: Actual data stored in the database at a specific time. Data Models: Tools for describing data, relationships, semantics, and constraints. Relational model: Stores data in tables, with rows representing records and columns representing attributes. DDL and DML: Data Definition Language (DDL): Used to create, modify, and delete database structures (e.g., tables). Data Manipulation Language (DML): Used to access and manipulate data (e.g., insert, update, delete). SQL (Structured Query Language): Commercial DML widely used in database systems. Not Turing-machine equivalent, but often embedded in other programming languages. Database Design: Process of creating the database schema and physical layout. Logical design: Deciding on the schema, including attributes and relationships. Physical design: Determining the physical storage and optimization strategies.","title":"Lecture 1.4 - Intro to DBMS1.pdf (PDF file)"},{"location":"Week%201/Lecture%201.5%20-%20Intro%20to%20DBMS2/","text":"Lecture 1.5 - Intro to DBMS2.pdf (PDF file) Summary Module Overview This module provides an introduction to Database Management Systems (DBMS). Objectives Understand models of DBMS Learn about key components of a database engine Familiarize with database internals and architecture Topics Database Design: Logical design (schema design) Physical design (data layout) Design methodologies (Entity-Relationship Model, Normalization Theory) Object-Relational Data Models: Extension of relational models to include object-oriented concepts Features: complex types, non-atomic values XML: Extensible Markup Language: Overview and uses Data exchange format Database Engine: Storage management: file interaction, data storage/retrieval Query processing: parsing, optimization, evaluation Transaction management: ensuring data integrity and consistency Database System Internals: Database architecture: centralized, client-server, distributed, cloud Database Users and Administrators: Different types of database users and their roles","title":"Lecture 1.5 - Intro to DBMS2.pdf (PDF file)"},{"location":"Week%201/Lecture%201.5%20-%20Intro%20to%20DBMS2/#lecture-15-intro-to-dbms2pdf-pdf-file","text":"Summary Module Overview This module provides an introduction to Database Management Systems (DBMS). Objectives Understand models of DBMS Learn about key components of a database engine Familiarize with database internals and architecture Topics Database Design: Logical design (schema design) Physical design (data layout) Design methodologies (Entity-Relationship Model, Normalization Theory) Object-Relational Data Models: Extension of relational models to include object-oriented concepts Features: complex types, non-atomic values XML: Extensible Markup Language: Overview and uses Data exchange format Database Engine: Storage management: file interaction, data storage/retrieval Query processing: parsing, optimization, evaluation Transaction management: ensuring data integrity and consistency Database System Internals: Database architecture: centralized, client-server, distributed, cloud Database Users and Administrators: Different types of database users and their roles","title":"Lecture 1.5 - Intro to DBMS2.pdf (PDF file)"},{"location":"Week%201/summary/","text":"Summary The query processor subsystem compiles and executes DDL and DML state- ments. Transaction management ensures that the database remains in a consistent (cor- rect) state despite system failures. The transaction manager ensures that concur- rent transaction executions proceed without con\ufb02icts. The architecture of a database system is greatly in\ufb02uenced by the underlying com- puter system on which the database system runs. Database systems can be central- ized, or parallel, involving multiple machines. Distributed databases span multiple geographically separated machines. Database applications are typically broken up into a front-end part that runs at client machines and a part that runs at the backend. In two-tier architectures, the front end directly communicates with a database running at the back end. In three- tier architectures, the back end part is itself broken up into an application server and a database server. There are four di\ufb00erent types of database-system users, di\ufb00erentiated by the way they expect to interact with the system. Di\ufb00erent types of user interfaces have been designed for the di\ufb00erent types of users. Data-analysis techniques attempt to automatically discover rules and patterns from data. The \ufb01eld of data mining combines knowledge-discovery techniques invented by arti\ufb01cial intelligence researchers and statistical analysts with e\ufb03cient imple- mentation techniques that enable them to be used on extremely large databases. Database-management system (DBMS) Database-system applications Online transaction processing Data analytics File-processing systems Data inconsistency Consistency constraints Data abstraction Physical level Logical level View level Instance Schema Physical schema Logical schema Subschema Physical data independence Data models Entity-relationship model Relational data model Semi-structured data model Object-based data model Database languages Data-de\ufb01nition language Data-manipulation language Procedural DML Declarative DML nonprocedural DML Query language Data-de\ufb01nition language Domain Constraints Referential Integrity Authorization Read authorization Insert authorization Update authorization Delete authorization Metadata Application program Database design Conceptual design Normalization Speci\ufb01cation of functional re- quirements Physical-design phase Database Engine Storage manager Authorization and integrity manager Transaction manager File manager Bu\ufb00er manager Data \ufb01les Data dictionary Indices Query processor DDL interpreter DML compiler Query optimization Query evaluation engine Transaction Atomicity Consistency Durability Recovery manager Failure recovery Concurrency-control manager Database Architecture Centralized Parallel Distributed Database Application Architecture Two-tier Three-tier Application server Database administrator (DBA) This chapter has described several major advantages of a database system. What are two disadvantages? List \ufb01ve ways in which the type declaration system of a language such as Java or C++ di\ufb00ers from the data de\ufb01nition language used in a database. 3 List six major steps that you would take in setting up a database for a particular enterprise. 4Suppose you want to build a video site similar to YouTube. Consider each of the points listed in Section - 2 as disadvantages of keeping data in a \ufb01le-processing system. Discuss the relevance of each of these points to the storage of actual video data, and to metadata about the video, such as title, the user who uploaded it, tags, and which users viewed it. 5Keyword queries used in web search are quite di\ufb00erent from database queries. List key di\ufb00erences between the two, in terms of the way the queries are speci\ufb01ed and in terms of what is the result of a query. List four applications you have used that most likely employed a database system to store persistent data. 7List four signi\ufb01cant di\ufb00erences between a \ufb01le-processing system and a DBMS. 8Explain the concept of physical data independence and its importance in database systems. 9List \ufb01ve responsibilities of a database-management system. For each responsi- bility, explain the problems that would arise if the responsibility were not dis- charged. 10List at least two reasons why database systems support data manipulation using a declarative query language such as SQL, instead of just providing a library of C or C++ functions to carry out data manipulation. 11Assume that two students are trying to register for a course in which there is only one open seat. What component of a database system prevents both students from being given that last seat? 12Explain the di\ufb00erence between two-tier and three-tier application architectures. Which is better suited for web applications? Why? 13List two features developed in the 2000s and that help database systems handle data-analytics workloads. 14Explain why NoSQL systems emerged in the 2000s, and brie\ufb02y contrast their features with traditional database systems. 15Describe at least three tables that might be used to store information in a social- networking system such as Facebook.","title":"Summary"},{"location":"Week%201/textbook/","text":"","title":"Textbook"},{"location":"Week%201/week1/","text":"[X] lec class [X] PPA [X] GRPA [X] GA [X] instructor section 1 [X] instructor section 2 [X] TA session 1 [X] TA session 2 [X] Text book","title":"Week1"},{"location":"Week%202/Lecture%202.1%20-%20Introduction%20to%20Relational%20Model1/","text":"Lecture 2.1 - Introduction to Relational Model1 Summary This module introduces the relational model for database management systems. It covers: Attributes and Types: Attributes represent data characteristics and have specific types, such as alphanumeric strings alpha strings dates numbers alpha strings each attributes has to atomic - same datatype that is indivisble null - spl value for every domain indicates that value is unknown domain is the allowed value for each attribute Schema and Instance: Schema defines the structure of a relation (table) specifying attributes and their types. Instance is the current state of a relation, represented as a table with rows (tuples). A1 , A2 , \u00b7 \u00b7 \u00b7 , An are attributes R = (A1 , A2 , \u00b7 \u00b7 \u00b7 , An ) is a relation schema Example: instructor = (ID, name, dept name, salary ) Formally, given sets D1 , D2 , \u00b7 \u00b7 \u00b7 , Dn a relation r is a subset of D 1 \u00d7 D2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Dn instructor \u2261 (String (5) \u00d7 String \u00d7 String \u00d7 Number +), where D \u2208 String (5), name \u2208 String , dept name \u2208 String , and salary \u2208 Number + imp thing to note : Order of tuples / rows is irrelevant (tuples may be stored in an arbitrary order) No two tuples / rows may be identical Keys: A superkey uniquely identifies tuples in a relation. Example: {ID} and {ID, name} are both superkeys of instructor A candidate key is a minimal superkey. Example: {ID} is a candidate key for instructor A primary key is the selected candidate key used for unique identification. A surrogate key (or synthetic key) in a database is a unique identifier for either an entity in the modeled world or an object in the database eg : transaction id of amazon the transaction id is only valid in the order life time . Secondary / Alternate Key: {First Name, Last Name}, Aadhaar # Simple Key: Consists of a single attribute Composite Key: {First Name, Last Name} Consists of more than one attribute to uniquely identify an entity occurrence One or more of the attributes, which make up the key, are not simple keys in their own right Relational Query Languages: Relational algebra, a procedural language, is introduced as a tool for manipulating relations using basic operations. Other \"pure\" languages exist but are not covered here.","title":"Lecture 2.1 - Introduction to Relational Model1"},{"location":"Week%202/Lecture%202.1%20-%20Introduction%20to%20Relational%20Model1/#lecture-21-introduction-to-relational-model1","text":"Summary This module introduces the relational model for database management systems. It covers: Attributes and Types: Attributes represent data characteristics and have specific types, such as alphanumeric strings alpha strings dates numbers alpha strings each attributes has to atomic - same datatype that is indivisble null - spl value for every domain indicates that value is unknown domain is the allowed value for each attribute Schema and Instance: Schema defines the structure of a relation (table) specifying attributes and their types. Instance is the current state of a relation, represented as a table with rows (tuples). A1 , A2 , \u00b7 \u00b7 \u00b7 , An are attributes R = (A1 , A2 , \u00b7 \u00b7 \u00b7 , An ) is a relation schema Example: instructor = (ID, name, dept name, salary ) Formally, given sets D1 , D2 , \u00b7 \u00b7 \u00b7 , Dn a relation r is a subset of D 1 \u00d7 D2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Dn instructor \u2261 (String (5) \u00d7 String \u00d7 String \u00d7 Number +), where D \u2208 String (5), name \u2208 String , dept name \u2208 String , and salary \u2208 Number + imp thing to note : Order of tuples / rows is irrelevant (tuples may be stored in an arbitrary order) No two tuples / rows may be identical Keys: A superkey uniquely identifies tuples in a relation. Example: {ID} and {ID, name} are both superkeys of instructor A candidate key is a minimal superkey. Example: {ID} is a candidate key for instructor A primary key is the selected candidate key used for unique identification. A surrogate key (or synthetic key) in a database is a unique identifier for either an entity in the modeled world or an object in the database eg : transaction id of amazon the transaction id is only valid in the order life time . Secondary / Alternate Key: {First Name, Last Name}, Aadhaar # Simple Key: Consists of a single attribute Composite Key: {First Name, Last Name} Consists of more than one attribute to uniquely identify an entity occurrence One or more of the attributes, which make up the key, are not simple keys in their own right Relational Query Languages: Relational algebra, a procedural language, is introduced as a tool for manipulating relations using basic operations. Other \"pure\" languages exist but are not covered here.","title":"Lecture 2.1 - Introduction to Relational Model1"},{"location":"Week%202/Lecture%202.2%20-%20Introduction%20to%20Relational%20Model2/","text":"Lecture 2.2 - Introduction to Relational Model2 Summary Introduction to Relational Model Module Objectives To understand relational algebra To familiarize with the operators of relational algebra Module Outline Operations Select Project Union Difference Intersection Cartesian Product Natural Join Aggregate Operations Relational Operators Select Operation: Selects rows (tuples) that meet a specified condition. \u2022 Relation r \u2022 \u03c3A=B\u2227D>5(r) \u2227 means and student = Roll Name Age 1 A 20 2 B 21 3 C 19 \\[ \\sigma_{roll=2}(student) \\] ROLL Name Age 2 B 21 \\[ \\pi_{name} \\sigma_{roll=2}(student) \\] Name B first condition to select and then projection Project Operation: Selects columns (Attributes) of a relation. \u2022 Relation r \u2022 \u03c0A,C (r) student = Roll Name Age 1 A 20 2 B 21 3 C 19 \\[ \\pi_{Roll}(student) = \\] Roll 1 2 3 \\[ \\pi_{Roll, Name}(student) = \\] Roll Name 1 A 2 B 3 C The projection will always give distinct rows Union of two relations: Combines the rows of two relations, eliminating duplicates. Relation r,s r \u222a s Union Operation Basics : The union operator in relational algebra functions similarly to set theory, combining elements from two sets and eliminating duplicates, ensuring each element appears only once. Requirements for Union : To perform a union on two tables in a database, they must have the same number of columns, and the data types (domains) of these columns must match. Order and Matching : The order of columns is crucial when performing a union. If the order is mismatched between the tables (e.g., numeric first in one and character first in the other), the union will fail, resulting in null values. Maintaining the correct order ensures the union operates correctly. Set difference of two relations: Removes rows from the first relation that are also in the second relation. Relation r,s (r \u2212 s) No of col or attriubutes have to same Set intersection of two relations: Returns rows that are common to both relations. \u2022 Relation r,s r \u2229 s Joining two relations \u2013 Cartesian-product: Combines all rows from the first relation with all rows from the second relation. \u2022 Relation r,s \u2022 r \u00d7 s when you have two atrributes with same name we remane the atrributes in cartseian product \\[ R_1 = \\] A B C 1 2 3 2 1 4 \\[ R_2 = \\] C D E 3 4 5 2 1 2 \\[ R_1 \\ X \\ R_2 = \\] A B \\(R_1.C\\) \\(R_2.C\\) D E 1 2 3 3 4 5 1 2 3 2 1 2 2 1 4 3 4 5 2 1 4 2 1 2 Natural Join: Joins two relations on the common attributes, eliminating duplicate columns. \u2022 Let r and s be relations on schemas R and S respectively. Then, the \u201cnatural join\u201d of relations R and S is a relation on schema R \u222a S obtained as follows: \u25e6 Consider each pair of tuples tr from r and ts from s. \u25e6 If tr and ts have the same value on each of the attributes in R \u2229 S, add a tuple t to the result, where . t has the same value as tr on r . t has the same value as ts on s Aggregation Operators Aggregate Operators: Perform calculations on groups of rows. SUM: Computes the sum of a specified column. AVG: Computes the average of a specified column. MAX: Computes the maximum value of a specified column. MIN: Computes the minimum value of a specified column. Notes about Relational Languages Each query input is a table (or set of tables). Each query output is a table. All data in the output table appears in one of the input tables. Relational Algebra is not Turing complete. Summary of Relational Algebra Operators Operator Description \u03c3 Select \u03c0 Project \u222a Union \u2212 Difference \u2229 Intersection \u00d7 Cartesian Product $ \\rho$ Rename \\(\\bowtie\\) Natural Join SUM Computes the sum of a specified column AVG Computes the average of a specified column MAX Computes the maximum value of a specified column MIN Computes the minimum value of a specified column Module Summary Introduced relational algebra Familiarized with the operators of relational algebra","title":"Lecture 2.2 - Introduction to Relational Model2"},{"location":"Week%202/Lecture%202.2%20-%20Introduction%20to%20Relational%20Model2/#lecture-22-introduction-to-relational-model2","text":"Summary Introduction to Relational Model Module Objectives To understand relational algebra To familiarize with the operators of relational algebra Module Outline Operations Select Project Union Difference Intersection Cartesian Product Natural Join Aggregate Operations Relational Operators","title":"Lecture 2.2 - Introduction to Relational Model2"},{"location":"Week%202/Lecture%202.2%20-%20Introduction%20to%20Relational%20Model2/#select-operation-selects-rows-tuples-that-meet-a-specified-condition","text":"\u2022 Relation r \u2022 \u03c3A=B\u2227D>5(r) \u2227 means and student = Roll Name Age 1 A 20 2 B 21 3 C 19 \\[ \\sigma_{roll=2}(student) \\] ROLL Name Age 2 B 21 \\[ \\pi_{name} \\sigma_{roll=2}(student) \\] Name B first condition to select and then projection","title":"Select Operation: Selects rows (tuples) that meet a specified condition."},{"location":"Week%202/Lecture%202.2%20-%20Introduction%20to%20Relational%20Model2/#project-operation-selects-columns-attributes-of-a-relation","text":"\u2022 Relation r \u2022 \u03c0A,C (r) student = Roll Name Age 1 A 20 2 B 21 3 C 19 \\[ \\pi_{Roll}(student) = \\] Roll 1 2 3 \\[ \\pi_{Roll, Name}(student) = \\] Roll Name 1 A 2 B 3 C The projection will always give distinct rows","title":"Project Operation: Selects columns (Attributes) of a relation."},{"location":"Week%202/Lecture%202.2%20-%20Introduction%20to%20Relational%20Model2/#union-of-two-relations-combines-the-rows-of-two-relations-eliminating-duplicates","text":"Relation r,s r \u222a s Union Operation Basics : The union operator in relational algebra functions similarly to set theory, combining elements from two sets and eliminating duplicates, ensuring each element appears only once. Requirements for Union : To perform a union on two tables in a database, they must have the same number of columns, and the data types (domains) of these columns must match. Order and Matching : The order of columns is crucial when performing a union. If the order is mismatched between the tables (e.g., numeric first in one and character first in the other), the union will fail, resulting in null values. Maintaining the correct order ensures the union operates correctly.","title":"Union of two relations: Combines the rows of two relations, eliminating duplicates."},{"location":"Week%202/Lecture%202.2%20-%20Introduction%20to%20Relational%20Model2/#set-difference-of-two-relations-removes-rows-from-the-first-relation-that-are-also-in-the-second-relation","text":"Relation r,s (r \u2212 s) No of col or attriubutes have to same","title":"Set difference of two relations: Removes rows from the first relation that are also in the second relation."},{"location":"Week%202/Lecture%202.2%20-%20Introduction%20to%20Relational%20Model2/#set-intersection-of-two-relations-returns-rows-that-are-common-to-both-relations","text":"\u2022 Relation r,s r \u2229 s","title":"Set intersection of two relations: Returns rows that are common to both relations."},{"location":"Week%202/Lecture%202.2%20-%20Introduction%20to%20Relational%20Model2/#joining-two-relations-cartesian-product-combines-all-rows-from-the-first-relation-with-all-rows-from-the-second-relation","text":"\u2022 Relation r,s \u2022 r \u00d7 s when you have two atrributes with same name we remane the atrributes in cartseian product \\[ R_1 = \\] A B C 1 2 3 2 1 4 \\[ R_2 = \\] C D E 3 4 5 2 1 2 \\[ R_1 \\ X \\ R_2 = \\] A B \\(R_1.C\\) \\(R_2.C\\) D E 1 2 3 3 4 5 1 2 3 2 1 2 2 1 4 3 4 5 2 1 4 2 1 2 Natural Join: Joins two relations on the common attributes, eliminating duplicate columns. \u2022 Let r and s be relations on schemas R and S respectively. Then, the \u201cnatural join\u201d of relations R and S is a relation on schema R \u222a S obtained as follows: \u25e6 Consider each pair of tuples tr from r and ts from s. \u25e6 If tr and ts have the same value on each of the attributes in R \u2229 S, add a tuple t to the result, where . t has the same value as tr on r . t has the same value as ts on s Aggregation Operators Aggregate Operators: Perform calculations on groups of rows. SUM: Computes the sum of a specified column. AVG: Computes the average of a specified column. MAX: Computes the maximum value of a specified column. MIN: Computes the minimum value of a specified column. Notes about Relational Languages Each query input is a table (or set of tables). Each query output is a table. All data in the output table appears in one of the input tables. Relational Algebra is not Turing complete. Summary of Relational Algebra Operators Operator Description \u03c3 Select \u03c0 Project \u222a Union \u2212 Difference \u2229 Intersection \u00d7 Cartesian Product $ \\rho$ Rename \\(\\bowtie\\) Natural Join SUM Computes the sum of a specified column AVG Computes the average of a specified column MAX Computes the maximum value of a specified column MIN Computes the minimum value of a specified column Module Summary Introduced relational algebra Familiarized with the operators of relational algebra","title":"Joining two relations \u2013 Cartesian-product: Combines all rows from the first relation with all rows from the second relation."},{"location":"Week%202/Lecture%202.3%20-%20Introduction%20to%20SQL1/","text":"Lecture 2.3 - Introduction to SQL1 Summary Module 08: Introduction to SQL Objectives Understand relational query language Understand data definition and basic query structure Outline History of SQL Data Definition Language (DDL) Create Table Integrity Constraints Update Table Data Manipulation Language (DML): Query Structure Select Clause Where Clause From Clause History of SQL IBM developed Structured English Query Language (SEQUEL) as part of System R project. Renamed Structured Query Language (SQL: pronounced still as SEQUEL) ANSI and ISO standard SQL: SQL-86 First formalized by ANSI SQL-89 + Integrity Constraints SQL-92 Major revision (ISO/IEC 9075 standard), De-facto Industry Standard SQL:1999 + Regular Expression Matching, Recursive Queries, Triggers, Support for Procedural and Control Flow Statements, Nonscalar types (Arrays), and Some OO features (structured types), Embedding SQL in Java (SQL/OLB), and Embedding Java in SQL (SQL/JRT) SQL:2003 + XML features (SQL/XML), Window Functions, Standardized Sequences, and Columns with Auto-generated Values (identity columns) SQL:2006 + Ways of importing and storing XML data in an SQL database, manipulating it within the database, and publishing both XML and conventional SQL-data in XML form SQL:2008 Legalizes ORDER BY outside Cursor Definitions INSTEAD OF Triggers, TRUNCATE Statement, and FETCH Clause SQL:2011 + Temporal Data (PERIOD FOR) Enhancements for Window Functions and FETCH Clause SQL:2016 + Row Pattern Matching, Polymorphic Table Functions, and JSON SQL:2019 + Multidimensional Arrays (MDarray type and operators) History of Query Language: Compliance SQL is the de facto industry standard today for relational or structred data systems (that is, SQL as a protocol), but there are many alternatives to writing SQL in the applications These alternatives have been implemented in the form of frontends for working with relational databases. Some examples of a frontend include (for a section of languages): \u25e6 SchemeQL and CLSQL, which are probably the most flexible, owing to their Lisp heritage, but they also look like a lot more like SQL than other frontends \u25e6 LINQ (in .Net) \u25e6 ScalaQL and ScalaQuery (in Scala) \u25e6 SqlStatement, ActiveRecord and many others in Ruby \u25e6 HaskellDB \u25e6 ...the list goes on for many other languages. History of Query Language: Alternatives There aren\u2019t any alternatives to SQL for speaking to relational databases (that is, SQL as a protocol), but there are many alternatives to writing SQL in the applications These alternatives have been implemented in the form of frontends for working with relational databases. Some examples of a frontend include (for a section of languages): \u25e6 SchemeQL and CLSQL, which are probably the most flexible, owing to their Lisp heritage, but they also look like a lot more like SQL than other frontends \u25e6 LINQ (in .Net) \u25e6 ScalaQL and ScalaQuery (in Scala) \u25e6 SqlStatement, ActiveRecord and many others in Ruby \u25e6 HaskellDB \u25e6 ...the list goes on for many other languages. Source: What are good alternatives to SQL (the language)? History of Query Language: Derivatives There are several query languages that are derived from or inspired by SQL. Of these, the most popular and effective is SPARQL. \u25e6 SPARQL (pronounced sparkle, a recursive acronym for SPARQL Protocol and RDF Query Language) is an RDF query language . A semantic query language for databases - able to retrieve and manipulate data stored in Resource Description Framework (RDF) format. . It has been standardized by the W3C Consortium as key technology of the semantic web . Versions: \u2212 SPARQL 1.0 (January 2008) \u2212 SPARQL 1.1 (March, 2013) . Used as the query languages for several NoSQL systems - particularly the Graph Databases that use RDF as store Data Definition Language (DDL) The SQL data-definition language (DDL) allows the specification of information about relations, including: The Schema for each Relation The Domain of values associated with each Attribute Integrity Constraints And, as we will see later, also other information such as \u25e6 The set of Indices to be maintained for each relations \u25e6 Security and Authorization information for each relation \u25e6 The Physical Storage Structure of each relation on disk Domain Types in SQL char(n). Fixed length character string, with user-specified length n varchar(n). Variable length character strings, with user-specified maximum length n int. Integer (a finite subset of the integers that is machine-dependent) smallint(n). Small integer (a machine-dependent subset of the integer domain type) numeric(p, d). Fixed point number, with user-specified precision of p digits, with d digits to the right of decimal point. (ex., numeric(3, 1), allows 44.5 to be stores exactly, but not 444.5 or 0.32) real, double precision. Floating point and double-precision floating point numbers, with machine-dependent precision float(n). Floating point number, with user-specified precision of at least n digits More are covered in Chapter 4 Create Table Construct An SQL relation is defined using the create table command: create table r (A1D1, A2D2, . . . , AnDn), (integrity-constraint1), . . . (integrity-constraintk )); \u25e6 r is the name of the relation \u25e6 each Ai is an attribute name in the schema of relation r \u25e6 Di is the data type of values in the domain of attribute Ai Create Table Construct (2) create table instructor ( ID char(5), name varchar(20) dept name varchar(20) salary numeric(8, 2)); Create Table Construct (3): Integrity Constraints not null primary key (A1, . . . , An) foreign key (Am, . . . , An) references r create table instructor (ID char(5),name varchar(20) not null,dept name varchar(20),salary numeric(8, 2),primary key (ID),foreign key (dept name) references department)); primary key declaration on an attribute automatically ensures not null University Schema Create Table Construct (4): More Relations create table student ( ID varchar(5), name varchar(20) not null, dept name varchar(20), tot cred numeric(3, 0), primary key (ID), foreign key (dept name) references department); create table course ( course id varchar(8), title varchar(50), dept name varchar(20), credits numeric(2, 0), primary key (course id), foreign key (dept name) references department); create table takes ( ID varchar(5), course id varchar(8), sec id varchar(8), semester varchar(6), year numeric(4, 0), grade varchar(2), primary key (ID, course id, sec id, semester, year ), foreign key (ID) references student foreign key (course id, sec id, semester, year ) references section); \u2022 Note: sec id can be dropped from primary key above, to ensure a student cannot be registered for two sections of the same course in the same semester Update Tables Insert (DML command) \u25e6 insert into instructor values (\u201810211\u2019, \u2018Smith\u2019, \u2018Biology\u2019, 66000); Delete (DML command) \u25e6 Remove all tuples from the student relation delete from student Drop Table (DDL command) \u25e6 drop table r Alter (DDL command) \u25e6 alter table r add A D . Where A is the name of the attribute to be added to relation r and D is the domain of A . All existing tuples in the relation are assigned null as the value for the new attribute \u25e6 alter table r drop A . Where A is the name of an attribute of relation r . Dropping of attributes not supported by many databases Data Manipulation Language (DML): Query Structure A typical SQL query has the form: select A1, A2, . . . , An,from r1,r2, ...,rmwhere P \u25e6 Ai represents an attribute from ri \u2019s \u25e6 ri represents a relation \u25e6 P is a predicate The result of an SQL query is a relation Basic Query Structure The select clause lists the attributes desired in the result of a query \u25e6 Corresponds to the projection operation of the relational algebra","title":"Lecture 2.3 - Introduction to SQL1"},{"location":"Week%202/Lecture%202.3%20-%20Introduction%20to%20SQL1/#lecture-23-introduction-to-sql1","text":"Summary Module 08: Introduction to SQL Objectives Understand relational query language Understand data definition and basic query structure Outline History of SQL Data Definition Language (DDL) Create Table Integrity Constraints Update Table Data Manipulation Language (DML): Query Structure Select Clause Where Clause From Clause History of SQL IBM developed Structured English Query Language (SEQUEL) as part of System R project. Renamed Structured Query Language (SQL: pronounced still as SEQUEL) ANSI and ISO standard SQL: SQL-86 First formalized by ANSI SQL-89 + Integrity Constraints SQL-92 Major revision (ISO/IEC 9075 standard), De-facto Industry Standard SQL:1999 + Regular Expression Matching, Recursive Queries, Triggers, Support for Procedural and Control Flow Statements, Nonscalar types (Arrays), and Some OO features (structured types), Embedding SQL in Java (SQL/OLB), and Embedding Java in SQL (SQL/JRT) SQL:2003 + XML features (SQL/XML), Window Functions, Standardized Sequences, and Columns with Auto-generated Values (identity columns) SQL:2006 + Ways of importing and storing XML data in an SQL database, manipulating it within the database, and publishing both XML and conventional SQL-data in XML form SQL:2008 Legalizes ORDER BY outside Cursor Definitions INSTEAD OF Triggers, TRUNCATE Statement, and FETCH Clause SQL:2011 + Temporal Data (PERIOD FOR) Enhancements for Window Functions and FETCH Clause SQL:2016 + Row Pattern Matching, Polymorphic Table Functions, and JSON SQL:2019 + Multidimensional Arrays (MDarray type and operators) History of Query Language: Compliance SQL is the de facto industry standard today for relational or structred data systems (that is, SQL as a protocol), but there are many alternatives to writing SQL in the applications These alternatives have been implemented in the form of frontends for working with relational databases. Some examples of a frontend include (for a section of languages): \u25e6 SchemeQL and CLSQL, which are probably the most flexible, owing to their Lisp heritage, but they also look like a lot more like SQL than other frontends \u25e6 LINQ (in .Net) \u25e6 ScalaQL and ScalaQuery (in Scala) \u25e6 SqlStatement, ActiveRecord and many others in Ruby \u25e6 HaskellDB \u25e6 ...the list goes on for many other languages. History of Query Language: Alternatives There aren\u2019t any alternatives to SQL for speaking to relational databases (that is, SQL as a protocol), but there are many alternatives to writing SQL in the applications These alternatives have been implemented in the form of frontends for working with relational databases. Some examples of a frontend include (for a section of languages): \u25e6 SchemeQL and CLSQL, which are probably the most flexible, owing to their Lisp heritage, but they also look like a lot more like SQL than other frontends \u25e6 LINQ (in .Net) \u25e6 ScalaQL and ScalaQuery (in Scala) \u25e6 SqlStatement, ActiveRecord and many others in Ruby \u25e6 HaskellDB \u25e6 ...the list goes on for many other languages. Source: What are good alternatives to SQL (the language)? History of Query Language: Derivatives There are several query languages that are derived from or inspired by SQL. Of these, the most popular and effective is SPARQL. \u25e6 SPARQL (pronounced sparkle, a recursive acronym for SPARQL Protocol and RDF Query Language) is an RDF query language . A semantic query language for databases - able to retrieve and manipulate data stored in Resource Description Framework (RDF) format. . It has been standardized by the W3C Consortium as key technology of the semantic web . Versions: \u2212 SPARQL 1.0 (January 2008) \u2212 SPARQL 1.1 (March, 2013) . Used as the query languages for several NoSQL systems - particularly the Graph Databases that use RDF as store Data Definition Language (DDL) The SQL data-definition language (DDL) allows the specification of information about relations, including: The Schema for each Relation The Domain of values associated with each Attribute Integrity Constraints And, as we will see later, also other information such as \u25e6 The set of Indices to be maintained for each relations \u25e6 Security and Authorization information for each relation \u25e6 The Physical Storage Structure of each relation on disk Domain Types in SQL char(n). Fixed length character string, with user-specified length n varchar(n). Variable length character strings, with user-specified maximum length n int. Integer (a finite subset of the integers that is machine-dependent) smallint(n). Small integer (a machine-dependent subset of the integer domain type) numeric(p, d). Fixed point number, with user-specified precision of p digits, with d digits to the right of decimal point. (ex., numeric(3, 1), allows 44.5 to be stores exactly, but not 444.5 or 0.32) real, double precision. Floating point and double-precision floating point numbers, with machine-dependent precision float(n). Floating point number, with user-specified precision of at least n digits More are covered in Chapter 4 Create Table Construct An SQL relation is defined using the create table command: create table r (A1D1, A2D2, . . . , AnDn), (integrity-constraint1), . . . (integrity-constraintk )); \u25e6 r is the name of the relation \u25e6 each Ai is an attribute name in the schema of relation r \u25e6 Di is the data type of values in the domain of attribute Ai Create Table Construct (2) create table instructor ( ID char(5), name varchar(20) dept name varchar(20) salary numeric(8, 2)); Create Table Construct (3): Integrity Constraints not null primary key (A1, . . . , An) foreign key (Am, . . . , An) references r create table instructor (ID char(5),name varchar(20) not null,dept name varchar(20),salary numeric(8, 2),primary key (ID),foreign key (dept name) references department)); primary key declaration on an attribute automatically ensures not null University Schema Create Table Construct (4): More Relations create table student ( ID varchar(5), name varchar(20) not null, dept name varchar(20), tot cred numeric(3, 0), primary key (ID), foreign key (dept name) references department); create table course ( course id varchar(8), title varchar(50), dept name varchar(20), credits numeric(2, 0), primary key (course id), foreign key (dept name) references department); create table takes ( ID varchar(5), course id varchar(8), sec id varchar(8), semester varchar(6), year numeric(4, 0), grade varchar(2), primary key (ID, course id, sec id, semester, year ), foreign key (ID) references student foreign key (course id, sec id, semester, year ) references section); \u2022 Note: sec id can be dropped from primary key above, to ensure a student cannot be registered for two sections of the same course in the same semester Update Tables Insert (DML command) \u25e6 insert into instructor values (\u201810211\u2019, \u2018Smith\u2019, \u2018Biology\u2019, 66000); Delete (DML command) \u25e6 Remove all tuples from the student relation delete from student Drop Table (DDL command) \u25e6 drop table r Alter (DDL command) \u25e6 alter table r add A D . Where A is the name of the attribute to be added to relation r and D is the domain of A . All existing tuples in the relation are assigned null as the value for the new attribute \u25e6 alter table r drop A . Where A is the name of an attribute of relation r . Dropping of attributes not supported by many databases Data Manipulation Language (DML): Query Structure A typical SQL query has the form: select A1, A2, . . . , An,from r1,r2, ...,rmwhere P \u25e6 Ai represents an attribute from ri \u2019s \u25e6 ri represents a relation \u25e6 P is a predicate The result of an SQL query is a relation Basic Query Structure The select clause lists the attributes desired in the result of a query \u25e6 Corresponds to the projection operation of the relational algebra","title":"Lecture 2.3 - Introduction to SQL1"},{"location":"Week%202/Lecture%202.4%20-%20Introduction%20to%20SQL2/","text":"Lecture 2.4 - Introduction to SQL2 Summary Additional Basic Operations Cartesian Product Finds all possible combinations of tuples from two or more relations. Useful when selecting data based on relationships between tables. Example: Select all possible instructor-course pairs using Cartesian product: select * from instructor , teaches Rename AS Operation Allows renaming of relations and attributes using the as clause: old name as new name Optional keyword as may be omitted. String Values SQL includes a string-matching operator ( like ) for character string comparisons. Uses special characters: Percent (%) matches any substring Underscore (_) matches any character Example: Find instructors with \"dar\" in their name: select name from instructor where name like '%dar%' \u2022 Patterns are case sensitive \u2022 Pattern matching examples: \u25e6 \u2019Intro%\u2019 matches any string beginning with \u201cIntro\u201d \u25e6 \u2019%Comp%\u2019 matches any string containing \u201cComp\u201d as a substring \u25e6 \u2019- - - \u2019 matches any string of exactly three characters \u25e6 \u2019 _ _%\u2019 matches any string of at least two characters \u2022 SQL supports a variety of string operations such as \u25e6 concatenation (using \u201c||\u201d) \u25e6 converting from upper to lower case (and vice versa) \u25e6 finding string length, extracting substrings, et Order By Clause Orders the display of tuples in ascending or descending order. By default, it is in ascending order. Can order by multiple attributes. Example: Order by name in descending order: select distinct name from instructor order by name desc Select Top / Fetch Clause Limits the number of records returned in the output. Useful for large tables to avoid performance impact. Example: Select the top 10 distinct names from the instructor table: select top 10 distinct name from instructor Where Clause Predicates Allows selecting tuples that meet specified conditions. Uses comparison operators, logical operators, and functions. Between operator allows specifying a range of values. Example: Find instructors with salaries between $90,000 and $100,000: select name from instructor where salary between 90000 and 100000 In Operator Allows specifying multiple values in a where clause. Shorthand for multiple or conditions. Example: Find instructors in the 'Comp. Sci.' or 'Biology' departments: select name from instructor where dept name in ( 'Comp. Sci.' , 'Biology' ) Tuple comparison select name, course_id from instructor , teaches where (instructor.ID, dept name) = (teaches.ID, \u2019Biology\u2019); Duplicates ???? SQL handles duplicates based on multiset semantics. Number of copies of tuples in the result depends on the number of copies in the input relations. For example, if there are two copies of tuple (1, a) in r1 and three copies of tuple (2) in r2, then the expression: \u03a0 B ( r1 ) x r2 would result in six copies of tuple (a, 2).","title":"Lecture 2.4 - Introduction to SQL2"},{"location":"Week%202/Lecture%202.4%20-%20Introduction%20to%20SQL2/#lecture-24-introduction-to-sql2","text":"Summary","title":"Lecture 2.4 - Introduction to SQL2"},{"location":"Week%202/Lecture%202.4%20-%20Introduction%20to%20SQL2/#additional-basic-operations","text":"","title":"Additional Basic Operations"},{"location":"Week%202/Lecture%202.4%20-%20Introduction%20to%20SQL2/#cartesian-product","text":"Finds all possible combinations of tuples from two or more relations. Useful when selecting data based on relationships between tables. Example: Select all possible instructor-course pairs using Cartesian product: select * from instructor , teaches","title":"Cartesian Product"},{"location":"Week%202/Lecture%202.4%20-%20Introduction%20to%20SQL2/#rename-as-operation","text":"Allows renaming of relations and attributes using the as clause: old name as new name Optional keyword as may be omitted.","title":"Rename AS Operation"},{"location":"Week%202/Lecture%202.4%20-%20Introduction%20to%20SQL2/#string-values","text":"SQL includes a string-matching operator ( like ) for character string comparisons. Uses special characters: Percent (%) matches any substring Underscore (_) matches any character Example: Find instructors with \"dar\" in their name: select name from instructor where name like '%dar%' \u2022 Patterns are case sensitive \u2022 Pattern matching examples: \u25e6 \u2019Intro%\u2019 matches any string beginning with \u201cIntro\u201d \u25e6 \u2019%Comp%\u2019 matches any string containing \u201cComp\u201d as a substring \u25e6 \u2019- - - \u2019 matches any string of exactly three characters \u25e6 \u2019 _ _%\u2019 matches any string of at least two characters \u2022 SQL supports a variety of string operations such as \u25e6 concatenation (using \u201c||\u201d) \u25e6 converting from upper to lower case (and vice versa) \u25e6 finding string length, extracting substrings, et","title":"String Values"},{"location":"Week%202/Lecture%202.4%20-%20Introduction%20to%20SQL2/#order-by-clause","text":"Orders the display of tuples in ascending or descending order. By default, it is in ascending order. Can order by multiple attributes. Example: Order by name in descending order: select distinct name from instructor order by name desc","title":"Order By Clause"},{"location":"Week%202/Lecture%202.4%20-%20Introduction%20to%20SQL2/#select-top-fetch-clause","text":"Limits the number of records returned in the output. Useful for large tables to avoid performance impact. Example: Select the top 10 distinct names from the instructor table: select top 10 distinct name from instructor","title":"Select Top / Fetch Clause"},{"location":"Week%202/Lecture%202.4%20-%20Introduction%20to%20SQL2/#where-clause-predicates","text":"Allows selecting tuples that meet specified conditions. Uses comparison operators, logical operators, and functions. Between operator allows specifying a range of values. Example: Find instructors with salaries between $90,000 and $100,000: select name from instructor where salary between 90000 and 100000","title":"Where Clause Predicates"},{"location":"Week%202/Lecture%202.4%20-%20Introduction%20to%20SQL2/#in-operator","text":"Allows specifying multiple values in a where clause. Shorthand for multiple or conditions. Example: Find instructors in the 'Comp. Sci.' or 'Biology' departments: select name from instructor where dept name in ( 'Comp. Sci.' , 'Biology' ) Tuple comparison select name, course_id from instructor , teaches where (instructor.ID, dept name) = (teaches.ID, \u2019Biology\u2019);","title":"In Operator"},{"location":"Week%202/Lecture%202.4%20-%20Introduction%20to%20SQL2/#duplicates","text":"SQL handles duplicates based on multiset semantics. Number of copies of tuples in the result depends on the number of copies in the input relations. For example, if there are two copies of tuple (1, a) in r1 and three copies of tuple (2) in r2, then the expression: \u03a0 B ( r1 ) x r2 would result in six copies of tuple (a, 2).","title":"Duplicates ????"},{"location":"Week%202/Lecture%202.5%20-%20Introduction%20to%20SQL3/","text":"Lecture 2.5 - Introduction to SQL3 Summary Introduction This module provides an overview of set operations, null values, and aggregation in SQL/3. It introduces the concepts of set union, intersection, and exception, as well as the handling of null values and aggregate functions. The module concludes with a summary of the key points covered. Objectives Upon completion of this module, students will be able to: Understand and apply set operations (union, intersection, except) Handle null values in SQL queries Use aggregate functions (avg, min, max, sum, count) Group data using the GROUP BY clause Filter grouped data using the HAVING clause Outline Set Operations Null Values Three-Valued Logic Aggregate Functions GROUP BY HAVING Null Values with Aggregates Set Operations Set operations in SQL/3 allow you to combine or compare the results of two or more queries. The three most common set operations are union, intersection, and except. Union: Returns all unique rows from both input queries. Intersection: Returns only the rows that are common to both input queries. Except: Returns all rows from the first input query that are not present in the second input query. Null Values Null values represent the absence of a value or an unknown value in SQL/3. They are distinct from zero or empty strings. usage : is null or is not null Three-Valued Logic Due to the inclusion of null values, SQL/3 uses a three-valued logic system, where a predicate can be evaluated as true, false, or unknown. True: If the predicate is satisfied without any null values. False: If the predicate is not satisfied, or if it involves null values. Unknown: If the predicate involves a comparison with a null value. Aggregate Functions Aggregate functions operate on a group of values and return a single result. Some common aggregate functions include: AVG: Average value MIN: Minimum value MAX: Maximum value SUM: Sum of values COUNT: Number of values GROUP BY The GROUP BY clause is used to group the results of a query based on one or more attributes. It allows for aggregation and filtering of data within each group. HAVING The HAVING clause is similar to the WHERE clause, but it is used to filter grouped data. It applies predicates to the aggregate values rather than the individual rows. Null Values with Aggregates Aggregate functions ignore null values unless the COUNT( ) function is used. COUNT( ) counts all rows, regardless of whether they contain null values. Module Summary This module has covered the following key concepts: Set operations (union, intersection, except) Null values and three-valued logic Aggregate functions (avg, min, max, sum, count) GROUP BY and HAVING clauses Handling null values with aggregates These concepts are essential for working with data in SQL/3. Understanding these concepts will enable you to perform complex data analysis and manipulation tasks efficiently.","title":"Lecture 2.5 - Introduction to SQL3"},{"location":"Week%202/Lecture%202.5%20-%20Introduction%20to%20SQL3/#lecture-25-introduction-to-sql3","text":"Summary Introduction This module provides an overview of set operations, null values, and aggregation in SQL/3. It introduces the concepts of set union, intersection, and exception, as well as the handling of null values and aggregate functions. The module concludes with a summary of the key points covered. Objectives Upon completion of this module, students will be able to: Understand and apply set operations (union, intersection, except) Handle null values in SQL queries Use aggregate functions (avg, min, max, sum, count) Group data using the GROUP BY clause Filter grouped data using the HAVING clause Outline Set Operations Null Values Three-Valued Logic Aggregate Functions GROUP BY HAVING Null Values with Aggregates Set Operations Set operations in SQL/3 allow you to combine or compare the results of two or more queries. The three most common set operations are union, intersection, and except. Union: Returns all unique rows from both input queries. Intersection: Returns only the rows that are common to both input queries. Except: Returns all rows from the first input query that are not present in the second input query. Null Values Null values represent the absence of a value or an unknown value in SQL/3. They are distinct from zero or empty strings. usage : is null or is not null Three-Valued Logic Due to the inclusion of null values, SQL/3 uses a three-valued logic system, where a predicate can be evaluated as true, false, or unknown. True: If the predicate is satisfied without any null values. False: If the predicate is not satisfied, or if it involves null values. Unknown: If the predicate involves a comparison with a null value. Aggregate Functions Aggregate functions operate on a group of values and return a single result. Some common aggregate functions include: AVG: Average value MIN: Minimum value MAX: Maximum value SUM: Sum of values COUNT: Number of values GROUP BY The GROUP BY clause is used to group the results of a query based on one or more attributes. It allows for aggregation and filtering of data within each group. HAVING The HAVING clause is similar to the WHERE clause, but it is used to filter grouped data. It applies predicates to the aggregate values rather than the individual rows. Null Values with Aggregates Aggregate functions ignore null values unless the COUNT( ) function is used. COUNT( ) counts all rows, regardless of whether they contain null values. Module Summary This module has covered the following key concepts: Set operations (union, intersection, except) Null values and three-valued logic Aggregate functions (avg, min, max, sum, count) GROUP BY and HAVING clauses Handling null values with aggregates These concepts are essential for working with data in SQL/3. Understanding these concepts will enable you to perform complex data analysis and manipulation tasks efficiently.","title":"Lecture 2.5 - Introduction to SQL3"},{"location":"Week%202/TRC/","text":"Tuple Relational Calculus (TRC) is a non-procedural query language used in relational databases, focusing on tuples (rows) and utilizing first-order logic to express queries. TRC uses the format { T | P(T) }, where T represents the resulting tuple and P(T) is a predicate or condition that must be satisfied for T to be included in the result. Unlike relational algebra, which is procedural, TRC specifies what to retrieve without detailing how to retrieve it, making it non-procedural. TRC supports operations similar to relational algebra, such as conjunction (AND), disjunction (OR), and negation (NOT), and employs existential (\u2203) and universal (\u2200) quantifiers to form conditions. Both TRC and relational algebra have the same expressive power, allowing the same queries to be written in either language, although TRC requires careful handling of unsafe expressions to avoid infinite loops.","title":"TRC"},{"location":"Week%202/check_list/","text":"[ ] Text Book [X] slides [ ] weekly assigment [X] lecture [ ] live session [ ] text book activity","title":"Check list"},{"location":"Week%202/inst_sec/","text":"columns - attribute rows - tuple Primary keys: not null uniquie Table will have one Primary key multiple candiates key Domain set of allowed values for each attribute is called domain primary key is not nullable Atomic values - it cant mean broken down into smaller junks eg : Schema $$ R = (A_1, A_2, ..... A_n) \\ \\ \\\\ Domain\\ D = (D_1 , D_2, ....D_n) $$ order of rows or tuple is irrelevant no two tuple or row may be identical Keys A superkey uniquely identifies tuples in a relation. Example: {ID} and {ID, name} are both superkeys of instructor A candidate key is a minimal superkey. min no of super key to indentify a tuple. Example: {ID} is a candidate key for instructor A primary key is the selected candidate key used for unique identification. A surrogate key (or synthetic key) in a database is a unique identifier for either an entity in the modeled world or an object in the database eg : transaction id of amazon the transaction id is only valid in the order life time . Super key. Candidate key. Primary key. Surrogate key. Secondary / Alternate Key: {First Name, Last Name}, Aadhaar # Simple Key: Consists of a single attribute Aadhaar # Composite Key: {First Name, Last Name} Foreign key constrain : Value in a relation must appear in another he matches becomes the referencing relationship and host_team_id , guest_team_id is foreign key. Match_num is primary key in match_referees and matches. Compound key = two foreign key + simple key to used to identify an entity occurrence . Procedural and Non Procedural Language Operations Select Project Union Difference Intersection Cartesian Product Natural Join A relation is set. Selection \u2022 Relation r \\(\u03c3\\_{A=B\u2227D>5(r)}\\) \u2227 means and (A = B) intersect D > 5 Projection \u2022 Relation r \u2022 \\(\u03c0\\_{A,C}\\) (r) fetch the columns no dulipcates Union \u2022 Relation r,s \u2022 \\(r \u222a s\\) same nuumber of attributes or degree domains of the attributes has to be same Difference \\[ r-s \\] Intersection \\[ r \u2229 s = r- (r-s) \\] rows that are common to both relations. Cartesian Product \\[ R \\\\bowtie S \\] when you have two atrributes with same name we remane the atrributes in cartseian product. we rename the table using \\[ P_s(r) \\] renaming the table or relationship r to s \\[ r \\\\bowtie p_s(r) \\] self join and rename r to s = $ r\\bowtie s$ \\[ \\\\sigma\\_{A=C}(r \\\\bowtie s) \\] do a cross join of r and s and the find A = C not equal \\(\\<>\\) Inner Join Natural Join \\[ r\u2229s \\]","title":"Inst sec"},{"location":"Week%202/inst_sec/#domain","text":"set of allowed values for each attribute is called domain primary key is not nullable Atomic values - it cant mean broken down into smaller junks eg : Schema $$ R = (A_1, A_2, ..... A_n) \\ \\ \\\\ Domain\\ D = (D_1 , D_2, ....D_n) $$ order of rows or tuple is irrelevant no two tuple or row may be identical","title":"Domain"},{"location":"Week%202/inst_sec/#keys","text":"A superkey uniquely identifies tuples in a relation. Example: {ID} and {ID, name} are both superkeys of instructor A candidate key is a minimal superkey. min no of super key to indentify a tuple. Example: {ID} is a candidate key for instructor A primary key is the selected candidate key used for unique identification. A surrogate key (or synthetic key) in a database is a unique identifier for either an entity in the modeled world or an object in the database eg : transaction id of amazon the transaction id is only valid in the order life time . Super key. Candidate key. Primary key. Surrogate key. Secondary / Alternate Key: {First Name, Last Name}, Aadhaar # Simple Key: Consists of a single attribute Aadhaar # Composite Key: {First Name, Last Name} Foreign key constrain : Value in a relation must appear in another he matches becomes the referencing relationship and host_team_id , guest_team_id is foreign key. Match_num is primary key in match_referees and matches. Compound key = two foreign key + simple key to used to identify an entity occurrence .","title":"Keys"},{"location":"Week%202/inst_sec/#procedural-and-non-procedural-language","text":"","title":"Procedural and Non Procedural Language"},{"location":"Week%202/inst_sec/#operations","text":"Select Project Union Difference Intersection Cartesian Product Natural Join A relation is set.","title":"Operations"},{"location":"Week%202/inst_sec/#selection","text":"\u2022 Relation r \\(\u03c3\\_{A=B\u2227D>5(r)}\\) \u2227 means and (A = B) intersect D > 5","title":"Selection"},{"location":"Week%202/inst_sec/#projection","text":"\u2022 Relation r \u2022 \\(\u03c0\\_{A,C}\\) (r) fetch the columns no dulipcates","title":"Projection"},{"location":"Week%202/inst_sec/#union","text":"\u2022 Relation r,s \u2022 \\(r \u222a s\\) same nuumber of attributes or degree domains of the attributes has to be same","title":"Union"},{"location":"Week%202/inst_sec/#difference","text":"\\[ r-s \\]","title":"Difference"},{"location":"Week%202/inst_sec/#intersection","text":"\\[ r \u2229 s = r- (r-s) \\] rows that are common to both relations.","title":"Intersection"},{"location":"Week%202/inst_sec/#cartesian-product","text":"\\[ R \\\\bowtie S \\] when you have two atrributes with same name we remane the atrributes in cartseian product. we rename the table using \\[ P_s(r) \\] renaming the table or relationship r to s \\[ r \\\\bowtie p_s(r) \\] self join and rename r to s = $ r\\bowtie s$ \\[ \\\\sigma\\_{A=C}(r \\\\bowtie s) \\] do a cross join of r and s and the find A = C not equal \\(\\<>\\)","title":"Cartesian Product"},{"location":"Week%202/inst_sec/#inner-join","text":"","title":"Inner Join"},{"location":"Week%202/inst_sec/#natural-join","text":"\\[ r\u2229s \\]","title":"Natural Join"},{"location":"Week%202/live%20session_sat/","text":"SELECT * FROM sales WHERE discount LIKE '100\\%' ESCAPE '\\' ;","title":"Live session sat"},{"location":"Week%202/ppa_grpa/","text":"The provided relational schema studentInfo has the following attributes: enrollment_num class section roll| name We are given that: {enrollment_num} is a candidate key. {class, section, roll} is another candidate key. Finding the Maximum Number of Superkeys: Since both {enrollment_num} and {class, section, roll} are candidate keys (minimal sets that uniquely identify a student), any superset of these sets will also be a superkey. how to find the maximum number of superkeys: Identify the number of attributes in each candidate key: {enrollment_num} has 1 attribute. {class, section, roll} has 3 attributes. Use the formula for the maximum number of superkeys in a relation with n attributes and k candidate keys (where each candidate key has m attributes): Max Superkeys = \\(2^n - (2^k - k)\\) In this case: n (total number of attributes) = 5 (enrollment_num, class, section, roll, name) k (number of candidate keys) = 2 $ 2^5 - (2^2 - 2) = 18 $ $ Max Superkey = 2^{5}-(2-1)$","title":"Ppa grpa"},{"location":"Week%202/tutorial_2.1/","text":"","title":"Tutorial 2.1"},{"location":"Week%203/Lecture%203.1%20-%20SQL%20Examples/","text":"Lecture 3.1 - SQL Examples.pdf (PDF file) Summary Introduction summary of the concepts covered in SQL Examples, a module in Database Management Systems. The focus is on providing a comprehensive overview of the material, including select statements, Cartesian products, the AS keyword, WHERE clauses with AND/OR operators, string operations, ORDER BY statements, the IN operator, set operations (UNION, INTERSECT, EXCEPT), and aggregation functions (AVG, MIN, MAX, COUNT, SUM). SELECT Statements The SELECT statement is used to retrieve data from one or more tables. The basic syntax is: SELECT column_list FROM table_name WHERE conditions; The column_list specifies the columns to be retrieved, table_name specifies the table(s) to be queried, and the WHERE clause specifies any conditions that must be met for the rows to be included in the result. Cartesian Products / AS Keyword A Cartesian product is a combination of all rows from one table with all rows from another table. The AS keyword is used to assign an alias to a column or table. The syntax for a Cartesian product is: SELECT column_list FROM table_name1, table_name2 [AS alias]; WHERE Clauses with AND/OR Operators The WHERE clause is used to specify conditions that must be met for rows to be included in the result. The AND operator is used to combine multiple conditions that must all be true, and the OR operator is used to combine multiple conditions that only one must be true. The syntax is: WHERE condition1 [AND/OR condition2]...; String Operations String operations can be used to compare strings and extract substrings. The most common string operators are: = and !=: Equality and inequality \\< and >: Less than and greater than LIKE: Pattern matching SUBSTRING(): Extracts a substring ORDER BY Statements The ORDER BY statement is used to sort the results of a query. The syntax is: ORDER BY column_name [ASC/DESC]; The ASC keyword specifies ascending order, and the DESC keyword specifies descending order. IN Operator The IN operator is used to check if a value is contained in a set of values. The syntax is: WHERE column_name IN (value1, value2, ...); Set Operations (UNION, INTERSECT, EXCEPT) Set operations are used to combine the results of multiple queries. The UNION operator combines the results of two or more queries, removing duplicate rows. The INTERSECT operator combines the results of two or more queries, keeping only the rows that are common to all queries. The EXCEPT operator combines the results of two or more queries, keeping only the rows that are not common to all queries. The syntax is: SELECT column_list FROM query1 [UNION/INTERSECT/EXCEPT] SELECT column_list FROM query2; Aggregation Functions (AVG, MIN, MAX, COUNT, SUM) Aggregation functions are used to perform calculations on groups of rows. The most common aggregation functions are: AVG(): Average MIN(): Minimum MAX(): Maximum COUNT(): Count SUM(): Sum The syntax is: SELECT aggregate_function(column_name) FROM table_name GROUP BY group_by_column; Conclusion This summary has provided a comprehensive overview of the concepts covered in SQL Examples, a module in Database Management Systems. By understanding these concepts, you can effectively use SQL to retrieve, manipulate, and analyze data stored in relational databases.","title":"Lecture 3.1 - SQL Examples.pdf (PDF file)"},{"location":"Week%203/Lecture%203.1%20-%20SQL%20Examples/#lecture-31-sql-examplespdf-pdf-file","text":"Summary Introduction summary of the concepts covered in SQL Examples, a module in Database Management Systems. The focus is on providing a comprehensive overview of the material, including select statements, Cartesian products, the AS keyword, WHERE clauses with AND/OR operators, string operations, ORDER BY statements, the IN operator, set operations (UNION, INTERSECT, EXCEPT), and aggregation functions (AVG, MIN, MAX, COUNT, SUM). SELECT Statements The SELECT statement is used to retrieve data from one or more tables. The basic syntax is: SELECT column_list FROM table_name WHERE conditions; The column_list specifies the columns to be retrieved, table_name specifies the table(s) to be queried, and the WHERE clause specifies any conditions that must be met for the rows to be included in the result. Cartesian Products / AS Keyword A Cartesian product is a combination of all rows from one table with all rows from another table. The AS keyword is used to assign an alias to a column or table. The syntax for a Cartesian product is: SELECT column_list FROM table_name1, table_name2 [AS alias]; WHERE Clauses with AND/OR Operators The WHERE clause is used to specify conditions that must be met for rows to be included in the result. The AND operator is used to combine multiple conditions that must all be true, and the OR operator is used to combine multiple conditions that only one must be true. The syntax is: WHERE condition1 [AND/OR condition2]...; String Operations String operations can be used to compare strings and extract substrings. The most common string operators are: = and !=: Equality and inequality \\< and >: Less than and greater than LIKE: Pattern matching SUBSTRING(): Extracts a substring ORDER BY Statements The ORDER BY statement is used to sort the results of a query. The syntax is: ORDER BY column_name [ASC/DESC]; The ASC keyword specifies ascending order, and the DESC keyword specifies descending order. IN Operator The IN operator is used to check if a value is contained in a set of values. The syntax is: WHERE column_name IN (value1, value2, ...); Set Operations (UNION, INTERSECT, EXCEPT) Set operations are used to combine the results of multiple queries. The UNION operator combines the results of two or more queries, removing duplicate rows. The INTERSECT operator combines the results of two or more queries, keeping only the rows that are common to all queries. The EXCEPT operator combines the results of two or more queries, keeping only the rows that are not common to all queries. The syntax is: SELECT column_list FROM query1 [UNION/INTERSECT/EXCEPT] SELECT column_list FROM query2; Aggregation Functions (AVG, MIN, MAX, COUNT, SUM) Aggregation functions are used to perform calculations on groups of rows. The most common aggregation functions are: AVG(): Average MIN(): Minimum MAX(): Maximum COUNT(): Count SUM(): Sum The syntax is: SELECT aggregate_function(column_name) FROM table_name GROUP BY group_by_column; Conclusion This summary has provided a comprehensive overview of the concepts covered in SQL Examples, a module in Database Management Systems. By understanding these concepts, you can effectively use SQL to retrieve, manipulate, and analyze data stored in relational databases.","title":"Lecture 3.1 - SQL Examples.pdf (PDF file)"},{"location":"Week%203/Lecture%203.2%20-%20Intermediate%20SQL1/","text":"Lecture 3.2 - Intermediate SQL1 Summary Nested Subqueries In SQL, a subquery is a select-from-where expression that is nested within another query. SQL provides mechanisms for nesting subqueries in the following ways: As a condition in the WHERE clause As a table in the FROM clause As a value in the SELECT clause Subqueries in the WHERE Clause Subqueries in the WHERE clause are used to test conditions on the results of another query. For example, to find courses offered in both Fall 2009 and Spring 2010, you could use the following query: select distinct course_id from section where semester = 'Fall' and year = 2009 and course_id in ( select course_id from section where semester = 'Spring' and year = 2010 ); Subqueries in the WHERE clause can also be used to perform set membership, set comparisons, and test for empty relations. Subqueries in the FROM Clause Subqueries in the FROM clause are used to create temporary tables that can be used in the main query. For example, to find the average salary of instructors in each department, you could use the following query: select dept name , avg salary from ( select dept name , avg ( salary ) as avg salary from instructor group by dept name ) where avg salary > 42000 ; The subquery in the FROM clause is used to create a temporary table that contains the average salary for each department. This temporary table can then be used in the main query to find the departments with the highest average salaries. Subqueries in the SELECT Clause Subqueries in the SELECT clause are used to return a single value or a set of values as a column in the result set. For example, to list all departments along with the number of instructors in each department, you could use the following query: select dept name , ( select count ( * ) from instructor where department . dept name = instructor . dept name ) as num instructors from department ; The subquery in the SELECT clause is used to return the number of instructors in each department. This value is then included as a column in the result set. Modifications of the Database In addition to performing queries, SQL can also be used to modify the data in a database. The following are the three main types of database modifications: Deletion: Deleting tuples from a relation Insertion: Inserting new tuples into a relation Updating: Updating the values in some tuples in a relation For example, to delete all instructors whose salary is less than the average salary, you could use the following query: delete from instructor where salary < ( select avg ( salary ) from instructor ); Module Summary In this module, we have introduced nested subqueries in SQL and discussed how they can be used to perform complex queries and modify data in a database. Database Management Systems: Intermediate SQL/1 Outline Nested Subqueries Subqueries in the WHERE Clause Subqueries in the FROM Clause Subqueries in the SELECT Clause Modifications of the Database","title":"Lecture 3.2 - Intermediate SQL1"},{"location":"Week%203/Lecture%203.2%20-%20Intermediate%20SQL1/#lecture-32-intermediate-sql1","text":"Summary Nested Subqueries In SQL, a subquery is a select-from-where expression that is nested within another query. SQL provides mechanisms for nesting subqueries in the following ways: As a condition in the WHERE clause As a table in the FROM clause As a value in the SELECT clause Subqueries in the WHERE Clause Subqueries in the WHERE clause are used to test conditions on the results of another query. For example, to find courses offered in both Fall 2009 and Spring 2010, you could use the following query: select distinct course_id from section where semester = 'Fall' and year = 2009 and course_id in ( select course_id from section where semester = 'Spring' and year = 2010 ); Subqueries in the WHERE clause can also be used to perform set membership, set comparisons, and test for empty relations. Subqueries in the FROM Clause Subqueries in the FROM clause are used to create temporary tables that can be used in the main query. For example, to find the average salary of instructors in each department, you could use the following query: select dept name , avg salary from ( select dept name , avg ( salary ) as avg salary from instructor group by dept name ) where avg salary > 42000 ; The subquery in the FROM clause is used to create a temporary table that contains the average salary for each department. This temporary table can then be used in the main query to find the departments with the highest average salaries. Subqueries in the SELECT Clause Subqueries in the SELECT clause are used to return a single value or a set of values as a column in the result set. For example, to list all departments along with the number of instructors in each department, you could use the following query: select dept name , ( select count ( * ) from instructor where department . dept name = instructor . dept name ) as num instructors from department ; The subquery in the SELECT clause is used to return the number of instructors in each department. This value is then included as a column in the result set. Modifications of the Database In addition to performing queries, SQL can also be used to modify the data in a database. The following are the three main types of database modifications: Deletion: Deleting tuples from a relation Insertion: Inserting new tuples into a relation Updating: Updating the values in some tuples in a relation For example, to delete all instructors whose salary is less than the average salary, you could use the following query: delete from instructor where salary < ( select avg ( salary ) from instructor ); Module Summary In this module, we have introduced nested subqueries in SQL and discussed how they can be used to perform complex queries and modify data in a database. Database Management Systems: Intermediate SQL/1 Outline Nested Subqueries Subqueries in the WHERE Clause Subqueries in the FROM Clause Subqueries in the SELECT Clause Modifications of the Database","title":"Lecture 3.2 - Intermediate SQL1"},{"location":"Week%203/Lecture%203.4%20-%20Intermediate%20SQL3/","text":"Lecture 3.4- Intermediate SQL3 Transactions Definition and Properties A transaction is a unit of work in a database that must be either fully completed or fully rolled back. Transactions ensure atomicity , meaning they are indivisible and irreducible. If a transaction is partially completed and an error occurs, it must be rolled back entirely as if it never happened. Transactions must also maintain isolation from other transactions to prevent concurrency issues. Implementation in SQL Transactions begin implicitly and are ended by either a commit or rollback command. In most databases, each SQL statement commits automatically by default. However, auto-commit can be turned off for a session using APIs or specific SQL commands. SQL:1999 introduced the begin atomic ... end syntax to explicitly define transactions, although this is not widely supported. Integrity Constraints Purpose and Types Integrity constraints are rules applied to database columns to ensure data accuracy and consistency. Common types of constraints include: NOT NULL : Ensures that a column cannot have a NULL value. PRIMARY KEY : A unique identifier for a row, which cannot be NULL. UNIQUE : Ensures all values in a column or a set of columns are unique. CHECK : Ensures that the value in a column meets a specific condition. Examples and Usage Example of a NOT NULL constraint: name VARCHAR(20) NOT NULL . Example of a UNIQUE constraint: UNIQUE (A1, A2, ..., Am) where A1, A2, ..., Am form a candidate key. Example of a CHECK constraint to restrict semester values: CREATE TABLE section ( course_id VARCHAR ( 8 ), sec_id VARCHAR ( 8 ), semester VARCHAR ( 6 ), year NUMERIC ( 4 , 0 ), building VARCHAR ( 15 ), room_number VARCHAR ( 7 ), time_slot_id VARCHAR ( 4 ), PRIMARY KEY ( course_id , sec_id , semester , year ), CHECK ( semester IN ( 'Fall' , 'Winter' , 'Spring' , 'Summer' )) ); Referential Integrity Definition and Enforcement Referential integrity ensures that a foreign key value in one table must match a primary key value in another table, maintaining consistency between related tables. Example: If \"Biology\" is a department name in the instructor table, it must exist in the department table. Cascading Actions Cascading actions define the behavior of the database when a user attempts to delete or update a key referenced by foreign keys. Example of cascading actions: CREATE TABLE course ( course_id CHAR(5) PRIMARY KEY, title VARCHAR(20), dept_name VARCHAR(20), FOREIGN KEY (dept_name) REFERENCES department ON DELETE CASCADE ON UPDATE CASCADE ); Alternative actions include NO ACTION , SET NULL , and SET DEFAULT . SQL Data Types and Schemas Built-in Data Types SQL provides several built-in data types: DATE : Stores dates. TIME : Stores time of day. TIMESTAMP : Combines date and time. INTERVAL : Represents a period of time. Examples: DATE '2005-07-27' TIME '09:00:30' TIMESTAMP '2005-07-27 09:00:30.75' INTERVAL '1' DAY Index Creation Alias or User defined Data Type Domains BLOB and Clob Authorization Privileges and Roles SQL includes mechanisms to control user access to data and operations. Privileges can be granted to users to allow specific actions, such as SELECT , INSERT , UPDATE , and DELETE . Example of granting select privilege on a view: CREATE VIEW geo_instructor AS SELECT * FROM instructor WHERE dept_name = 'Geology' ; GRANT SELECT ON geo_instructor TO geo_staff ; References privilege is necessary to create foreign keys: GRANT REFERENCES ( dept_name ) ON department TO Mariano ; Transfer and Revocation of Privileges Privileges can be transferred from one user to another using the GRANT ... WITH GRANT OPTION . Privileges can be revoked using the REVOKE command: REVOKE SELECT ON department FROM Amit , Satoshi CASCADE ; Role Module Summary The lecture summarizes the following key points: Introduction to transactions, ensuring atomicity and isolation. Detailed exploration of integrity constraints to maintain data consistency. Referential integrity to enforce relationships between tables. Advanced SQL data types for handling various forms of data. Authorization mechanisms to manage user permissions and roles within the database. Overall, this lecture provides an in-depth look at intermediate SQL concepts essential for maintaining robust and secure databases.","title":"Lecture 3.4   Intermediate SQL3"},{"location":"Week%203/Lecture%203.4%20-%20Intermediate%20SQL3/#lecture-34-intermediate-sql3","text":"","title":"Lecture 3.4- Intermediate SQL3"},{"location":"Week%203/Lecture%203.4%20-%20Intermediate%20SQL3/#transactions","text":"Definition and Properties A transaction is a unit of work in a database that must be either fully completed or fully rolled back. Transactions ensure atomicity , meaning they are indivisible and irreducible. If a transaction is partially completed and an error occurs, it must be rolled back entirely as if it never happened. Transactions must also maintain isolation from other transactions to prevent concurrency issues. Implementation in SQL Transactions begin implicitly and are ended by either a commit or rollback command. In most databases, each SQL statement commits automatically by default. However, auto-commit can be turned off for a session using APIs or specific SQL commands. SQL:1999 introduced the begin atomic ... end syntax to explicitly define transactions, although this is not widely supported.","title":"Transactions"},{"location":"Week%203/Lecture%203.4%20-%20Intermediate%20SQL3/#integrity-constraints","text":"Purpose and Types Integrity constraints are rules applied to database columns to ensure data accuracy and consistency. Common types of constraints include: NOT NULL : Ensures that a column cannot have a NULL value. PRIMARY KEY : A unique identifier for a row, which cannot be NULL. UNIQUE : Ensures all values in a column or a set of columns are unique. CHECK : Ensures that the value in a column meets a specific condition. Examples and Usage Example of a NOT NULL constraint: name VARCHAR(20) NOT NULL . Example of a UNIQUE constraint: UNIQUE (A1, A2, ..., Am) where A1, A2, ..., Am form a candidate key. Example of a CHECK constraint to restrict semester values: CREATE TABLE section ( course_id VARCHAR ( 8 ), sec_id VARCHAR ( 8 ), semester VARCHAR ( 6 ), year NUMERIC ( 4 , 0 ), building VARCHAR ( 15 ), room_number VARCHAR ( 7 ), time_slot_id VARCHAR ( 4 ), PRIMARY KEY ( course_id , sec_id , semester , year ), CHECK ( semester IN ( 'Fall' , 'Winter' , 'Spring' , 'Summer' )) );","title":"Integrity Constraints"},{"location":"Week%203/Lecture%203.4%20-%20Intermediate%20SQL3/#referential-integrity","text":"Definition and Enforcement Referential integrity ensures that a foreign key value in one table must match a primary key value in another table, maintaining consistency between related tables. Example: If \"Biology\" is a department name in the instructor table, it must exist in the department table. Cascading Actions Cascading actions define the behavior of the database when a user attempts to delete or update a key referenced by foreign keys. Example of cascading actions: CREATE TABLE course ( course_id CHAR(5) PRIMARY KEY, title VARCHAR(20), dept_name VARCHAR(20), FOREIGN KEY (dept_name) REFERENCES department ON DELETE CASCADE ON UPDATE CASCADE ); Alternative actions include NO ACTION , SET NULL , and SET DEFAULT .","title":"Referential Integrity"},{"location":"Week%203/Lecture%203.4%20-%20Intermediate%20SQL3/#sql-data-types-and-schemas","text":"Built-in Data Types SQL provides several built-in data types: DATE : Stores dates. TIME : Stores time of day. TIMESTAMP : Combines date and time. INTERVAL : Represents a period of time. Examples: DATE '2005-07-27' TIME '09:00:30' TIMESTAMP '2005-07-27 09:00:30.75' INTERVAL '1' DAY","title":"SQL Data Types and Schemas"},{"location":"Week%203/Lecture%203.4%20-%20Intermediate%20SQL3/#index-creation","text":"","title":"Index Creation"},{"location":"Week%203/Lecture%203.4%20-%20Intermediate%20SQL3/#alias-or-user-defined-data-type","text":"","title":"Alias or User defined Data Type"},{"location":"Week%203/Lecture%203.4%20-%20Intermediate%20SQL3/#domains","text":"","title":"Domains"},{"location":"Week%203/Lecture%203.4%20-%20Intermediate%20SQL3/#blob-and-clob","text":"","title":"BLOB and Clob"},{"location":"Week%203/Lecture%203.4%20-%20Intermediate%20SQL3/#authorization","text":"Privileges and Roles SQL includes mechanisms to control user access to data and operations. Privileges can be granted to users to allow specific actions, such as SELECT , INSERT , UPDATE , and DELETE . Example of granting select privilege on a view: CREATE VIEW geo_instructor AS SELECT * FROM instructor WHERE dept_name = 'Geology' ; GRANT SELECT ON geo_instructor TO geo_staff ; References privilege is necessary to create foreign keys: GRANT REFERENCES ( dept_name ) ON department TO Mariano ; Transfer and Revocation of Privileges Privileges can be transferred from one user to another using the GRANT ... WITH GRANT OPTION . Privileges can be revoked using the REVOKE command: REVOKE SELECT ON department FROM Amit , Satoshi CASCADE ;","title":"Authorization"},{"location":"Week%203/Lecture%203.4%20-%20Intermediate%20SQL3/#role","text":"","title":"Role"},{"location":"Week%203/Lecture%203.4%20-%20Intermediate%20SQL3/#module-summary","text":"The lecture summarizes the following key points: Introduction to transactions, ensuring atomicity and isolation. Detailed exploration of integrity constraints to maintain data consistency. Referential integrity to enforce relationships between tables. Advanced SQL data types for handling various forms of data. Authorization mechanisms to manage user permissions and roles within the database. Overall, this lecture provides an in-depth look at intermediate SQL concepts essential for maintaining robust and secure databases.","title":"Module Summary"},{"location":"Week%203/Lecture%203.5%20-%20Advanced%20SQL/","text":"Lecture 3.5 - Advanced SQL Summary Functions and Procedural Constructs SQL:1999 added support for functions and procedures to SQL. These functions and procedures can be written in SQL itself or in an external programming language (such as C, Java, or Python). Functions written in an external language are particularly useful for working with specialized data types, such as images or geometric objects. SQL:1999 also supports a rich set of imperative constructs, including loops, if-then-else statements, and assignment. Many databases have proprietary procedural extensions to SQL that differ from SQL:1999. Functions Functions are used to perform calculations or retrieve data from the database. They can be defined using the CREATE FUNCTION statement. The following example creates a function that returns the count of the number of instructors in a given department: CREATE FUNCTION dept_count(dept_name VARCHAR(20)) RETURNS INTEGER BEGIN DECLARE d_count INTEGER; This function can then be used to find the department names and budgets of all departments with more than 12 instructors: SELECT dept_name, budget FROM department WHERE dept_count(dept_name) > 12; Procedures Procedures are used to perform a series of actions, such as updating multiple rows in a table or inserting a new row into a table. They can be defined using the CREATE PROCEDURE statement. The following example creates a procedure that updates the salary of an employee: CREATE PROCEDURE update_salary ( emp_id INT , new_salary NUMERIC ( 8 , 2 )) BEGIN UPDATE employee SET salary = new_salary WHERE emp_id = emp_id ; END ; This procedure can then be called to update the salary of an employee: CALL update_salary ( 1000 , 50000 ); Language Constructs SQL:1999 supports a rich set of imperative constructs, including loops, if-then-else statements, and assignment. These constructs can be used to create complex procedures and functions. The following table summarizes the most common language constructs: Construct Description BEGIN ... END Compound statement DECLARE Declare a local variable SET Assign a value to a local variable IF ... THEN ... ELSE Conditional statement WHILE Loop while a condition is true REPEAT Loop until a condition is true FOR Loop through a set of values CASE Conditional statement with multiple branches SIGNAL Signal an exception DECLARE HANDLER FOR Declare a handler for an exception External Language Routines SQL:1999 allows the definition of functions and procedures in an imperative programming language (such as C, Java, or Python) that can be invoked from SQL queries. Such functions can be more efficient than functions defined in SQL, and computations that cannot be carried out in SQL can be executed by these functions. To declare an external language function or procedure, use the CREATE FUNCTION or CREATE PROCEDURE statement and specify the language and the external name of the function or procedure. For example, the following statement creates a function that returns the count of the number of instructors in a given department: CREATE FUNCTION dept_count ( dept_name VARCHAR ( 20 )) RETURNS INTEGER LANGUAGE C EXTERNAL NAME '/usr/avi/bin/dept_count' ; Triggers Triggers are database objects that are used to automatically perform actions when certain events occur in the database. For example, a trigger can be used to automatically update a table when a row is inserted into another table. Triggers are defined using the CREATE TRIGGER statement. The following example creates a trigger that updates the total_credits column in the student table when a row is inserted into the takes table: CREATE TRIGGER update_total_credits AFTER INSERT ON takes FOR EACH ROW BEGIN UPDATE student SET total_credits = total_credits + ( SELECT credits FROM course WHERE course . course_id = NEW . course_id ) WHERE student . id = NEW . student_id ; END ; Functionality vs. Performance Triggers can be a powerful tool for automating database operations, but they can also have a negative impact on performance. The following are some factors that can affect the performance of triggers: The number of triggers: The more triggers that are defined on a table, the greater the impact on performance. The complexity of the triggers: Complex triggers that perform multiple operations can be more time-consuming to execute than simple triggers. The frequency of the triggering events: Triggers that are activated by frequent events can have a significant impact on performance. It is important to carefully consider the impact of triggers on performance before creating them. If a trigger is not necessary, it should not be created. If a trigger is necessary, it should be designed to be as efficient as possible. How to Use Triggers Triggers can be used for a variety of purposes, including: Maintaining data integrity: Triggers can be used to enforce business rules and ensure that data is consistent. Automating tasks: Triggers can be used to automate tasks that would otherwise need to be performed manually. Improving performance: Triggers can be used to improve performance by performing operations","title":"Lecture 3.5 - Advanced SQL"},{"location":"Week%203/Lecture%203.5%20-%20Advanced%20SQL/#lecture-35-advanced-sql","text":"Summary Functions and Procedural Constructs SQL:1999 added support for functions and procedures to SQL. These functions and procedures can be written in SQL itself or in an external programming language (such as C, Java, or Python). Functions written in an external language are particularly useful for working with specialized data types, such as images or geometric objects. SQL:1999 also supports a rich set of imperative constructs, including loops, if-then-else statements, and assignment. Many databases have proprietary procedural extensions to SQL that differ from SQL:1999. Functions Functions are used to perform calculations or retrieve data from the database. They can be defined using the CREATE FUNCTION statement. The following example creates a function that returns the count of the number of instructors in a given department: CREATE FUNCTION dept_count(dept_name VARCHAR(20)) RETURNS INTEGER BEGIN DECLARE d_count INTEGER; This function can then be used to find the department names and budgets of all departments with more than 12 instructors: SELECT dept_name, budget FROM department WHERE dept_count(dept_name) > 12; Procedures Procedures are used to perform a series of actions, such as updating multiple rows in a table or inserting a new row into a table. They can be defined using the CREATE PROCEDURE statement. The following example creates a procedure that updates the salary of an employee: CREATE PROCEDURE update_salary ( emp_id INT , new_salary NUMERIC ( 8 , 2 )) BEGIN UPDATE employee SET salary = new_salary WHERE emp_id = emp_id ; END ; This procedure can then be called to update the salary of an employee: CALL update_salary ( 1000 , 50000 ); Language Constructs SQL:1999 supports a rich set of imperative constructs, including loops, if-then-else statements, and assignment. These constructs can be used to create complex procedures and functions. The following table summarizes the most common language constructs: Construct Description BEGIN ... END Compound statement DECLARE Declare a local variable SET Assign a value to a local variable IF ... THEN ... ELSE Conditional statement WHILE Loop while a condition is true REPEAT Loop until a condition is true FOR Loop through a set of values CASE Conditional statement with multiple branches SIGNAL Signal an exception DECLARE HANDLER FOR Declare a handler for an exception External Language Routines SQL:1999 allows the definition of functions and procedures in an imperative programming language (such as C, Java, or Python) that can be invoked from SQL queries. Such functions can be more efficient than functions defined in SQL, and computations that cannot be carried out in SQL can be executed by these functions. To declare an external language function or procedure, use the CREATE FUNCTION or CREATE PROCEDURE statement and specify the language and the external name of the function or procedure. For example, the following statement creates a function that returns the count of the number of instructors in a given department: CREATE FUNCTION dept_count ( dept_name VARCHAR ( 20 )) RETURNS INTEGER LANGUAGE C EXTERNAL NAME '/usr/avi/bin/dept_count' ; Triggers Triggers are database objects that are used to automatically perform actions when certain events occur in the database. For example, a trigger can be used to automatically update a table when a row is inserted into another table. Triggers are defined using the CREATE TRIGGER statement. The following example creates a trigger that updates the total_credits column in the student table when a row is inserted into the takes table: CREATE TRIGGER update_total_credits AFTER INSERT ON takes FOR EACH ROW BEGIN UPDATE student SET total_credits = total_credits + ( SELECT credits FROM course WHERE course . course_id = NEW . course_id ) WHERE student . id = NEW . student_id ; END ; Functionality vs. Performance Triggers can be a powerful tool for automating database operations, but they can also have a negative impact on performance. The following are some factors that can affect the performance of triggers: The number of triggers: The more triggers that are defined on a table, the greater the impact on performance. The complexity of the triggers: Complex triggers that perform multiple operations can be more time-consuming to execute than simple triggers. The frequency of the triggering events: Triggers that are activated by frequent events can have a significant impact on performance. It is important to carefully consider the impact of triggers on performance before creating them. If a trigger is not necessary, it should not be created. If a trigger is necessary, it should be designed to be as efficient as possible. How to Use Triggers Triggers can be used for a variety of purposes, including: Maintaining data integrity: Triggers can be used to enforce business rules and ensure that data is consistent. Automating tasks: Triggers can be used to automate tasks that would otherwise need to be performed manually. Improving performance: Triggers can be used to improve performance by performing operations","title":"Lecture 3.5 - Advanced SQL"},{"location":"Week%203/livesession/","text":"The key differences between the VARCHAR(5) and CHAR(5) data types in SQL are: Length : VARCHAR(5) can store variable-length strings up to 5 characters. CHAR(5) stores fixed-length strings of exactly 5 characters. Storage Space : VARCHAR(5) only uses the exact number of bytes required to store the data, plus 1-2 bytes of overhead. CHAR(5) always uses 5 bytes of storage, regardless of the actual string length. Padding : If a VARCHAR(5) column is assigned a string shorter than 5 characters, no padding occurs. If a CHAR(5) column is assigned a string shorter than 5 characters, it is right-padded with spaces to reach the full 5 character length. Performance : Queries on CHAR columns are generally faster than on VARCHAR columns, as the fixed length allows more efficient indexing and storage. However, the space efficiency of VARCHAR can outweigh the performance difference in many cases. The key requirements for a foreign key are:1. The data types of the foreign key column(s) and the referenced column(s) must be compatible. The referenced column(s) must have a unique index, either as a primary key or a unique constraint. This means that the foreign key can reference:* A primary key column A column with a unique constraint Multiple columns with a composite unique constraint The LIKE operator in SQL is used to search for specific patterns in a column. It is often used with the WHERE clause to filter data based on a specified pattern. There are two wildcards commonly used with the LIKE operator: Percent Sign ( % ) : Represents zero, one, or multiple characters. Used to match strings that start with a specific pattern and can contain any characters after that. Underscore ( _ ) : Represents one, single character. Used to match strings that start with a specific pattern and have a single character after that. Examples Matching Strings That Start with a Specific Pattern : SELECT * FROM Customers WHERE CustomerName LIKE 'a%' ; This query will return all customers whose names start with the letter \"a\". 2. Matching Strings That Contain a Specific Pattern : SELECT * FROM Customers WHERE CustomerName LIKE '%L%' ; This query will return all customers whose names contain the letter \"L\" anywhere. 3. Matching Strings That End with a Specific Pattern : SELECT * FROM Customers WHERE CustomerName LIKE '%ia' ; This query will return all customers whose names end with the string \"ia\". 4. Matching Strings That Start with a Specific Pattern and Contain a Specific Character : SELECT * FROM Customers WHERE CustomerName LIKE 'U_' ; This query will return all customers whose names start with the letter \"U\" and have a single character after that. Using NOT LIKE The NOT LIKE operator is used to negate the results of a LIKE query. It returns all records that do not match the specified pattern. Example SELECT * FROM Customers WHERE CustomerName NOT LIKE 'USA' ; This query will return all customers whose names do not contain the string \"USA\". Using LIKE with Multiple Values The LIKE operator can be used with multiple string patterns by combining it with the OR operator. Example SELECT * FROM Customers WHERE last_name LIKE 'R%t' OR last_name LIKE '%e' ; The \"in\" and \"not in\" operators in Python and SQL are used to check if a specified value is present or absent in a sequence or a table. Here are the key points about these operators: Python \"in\" and \"not in\" Operators \"in\" Operator: The \"in\" operator checks if a specified value is present in a sequence (like a list, tuple, or string). It returns True if the value is found and False if it is not[1]. SQL \"in\" and \"not in\" Operators \"in\" Operator: The \"in\" operator is used with the SELECT , UPDATE , and DELETE statements to filter records based on a condition. It selects records where the specified column's value is present in a list of values[2][3][4]. \"not in\" Operator: The \"not in\" operator is used to filter out records where the specified column's value is not present in a list of values. It is often used with the SELECT , UPDATE , and DELETE statements to exclude specific records[2][3][4]. Key Points Both \"in\" and \"not in\" operators can be used in SQL with the WHERE clause to filter data. The \"in\" operator returns True if the value is found and False if it is not. The \"not in\" operator returns True if the value is not found and False if it is found. The \"in\" operator can be used with both numeric and non-numeric data types, such as strings. The \"not in\" operator can be used with both numeric and non-numeric data types, such as strings. The \"in\" operator can be used in combination with other operators like AND and OR . The \"not in\" operator can be used in combination with other operators like AND and OR . Examples Python: list1 = [ 1 , 2 , 3 , 4 , 5 ] string1 = \"My name is AskPython\" tuple1 = ( 11 , 22 , 33 , 44 ) print ( 5 in list1 ) # True print ( \"is\" in string1 ) # True print ( 88 in tuple1 ) # False print ( 5 not in list1 ) # False print ( \"is\" not in string1 ) # False print ( 88 not in tuple1 ) # True - SQL: SELECT Student_ID , Student_name , City , Age FROM student WHERE Age IN ( 25 , 29 ) AND City NOT IN ( 'Chennai' , 'Delhi' ); SELECT Name FROM Emp WHERE age NOT IN ( 23 , 22 , 21 ); SELECT * FROM Emp WHERE country NOT IN ( 'Australia' , 'Austria' ); SQL joins are used to combine rows from two or more tables based on a related column between them. This allows you to retrieve data from multiple tables by establishing logical relationships between them. Here are the different types of SQL joins: INNER JOIN An INNER JOIN returns all records from both tables where the join condition is satisfied. It is the most commonly used type of join and assumes that there is a match between the tables. Syntax: SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 INNER JOIN table2 ON table1 . matching_column = table2 . matching_column ; Example: SELECT StudentCourse . COURSE_ID , Student . NAME , Student . AGE FROM Student INNER JOIN StudentCourse ON Student . ROLL_NO = StudentCourse . ROLL_NO ; LEFT JOIN A LEFT JOIN returns all records from the left table and the matched records from the right table. If there is no match, the result-set will contain null values for the right table. Syntax: SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 LEFT JOIN table2 ON table1 . matching_column = table2 . matching_column ; Example: SELECT Student . NAME , StudentCourse . COURSE_ID FROM Student LEFT JOIN StudentCourse ON StudentCourse . ROLL_NO = Student . ROLL_NO ; RIGHT JOIN A RIGHT JOIN returns all records from the right table and the matched records from the left table. If there is no match, the result-set will contain null values for the left table. Syntax: SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 RIGHT JOIN table2 ON table1 . matching_column = table2 . matching_column ; Example: SELECT StudentCourse . COURSE_ID , Student . NAME FROM StudentCourse RIGHT JOIN Student ON StudentCourse . ROLL_NO = Student . ROLL_NO ; FULL JOIN A FULL JOIN returns all records when there is a match in either the left or right table. Syntax: SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 FULL JOIN table2 ON table1 . matching_column = table2 . matching_column ; Example: SELECT Student . NAME , StudentCourse . COURSE_ID FROM Student FULL JOIN StudentCourse ON StudentCourse . ROLL_NO = Student . ROLL_NO ; CROSS JOIN A CROSS JOIN returns all possible combinations of rows from both tables. Syntax: SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 CROSS JOIN table2 ; Example: SELECT Student . NAME , StudentCourse . COURSE_ID FROM Student CROSS JOIN StudentCourse ; SELF JOIN A SELF JOIN is used to join a table to itself. This is useful for comparing rows within the same table. Syntax: SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 AS table1_alias JOIN table1 AS table2_alias ON table1_alias . matching_column = table2_alias . matching_column ; Example: SELECT Student . NAME , StudentCourse . COURSE_ID FROM Student AS s JOIN Student AS sc ON s . ROLL_NO = sc . ROLL_NO ; NATURAL JOIN A NATURAL JOIN is similar to an INNER JOIN but automatically matches columns with the same names. Syntax: SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 NATURAL JOIN table2 ; Example: SELECT Student . NAME , StudentCourse . COURSE_ID FROM Student NATURAL JOIN StudentCourse ;","title":"Livesession"},{"location":"Week%203/livesession/#examples","text":"Matching Strings That Start with a Specific Pattern : SELECT * FROM Customers WHERE CustomerName LIKE 'a%' ; This query will return all customers whose names start with the letter \"a\". 2. Matching Strings That Contain a Specific Pattern : SELECT * FROM Customers WHERE CustomerName LIKE '%L%' ; This query will return all customers whose names contain the letter \"L\" anywhere. 3. Matching Strings That End with a Specific Pattern : SELECT * FROM Customers WHERE CustomerName LIKE '%ia' ; This query will return all customers whose names end with the string \"ia\". 4. Matching Strings That Start with a Specific Pattern and Contain a Specific Character : SELECT * FROM Customers WHERE CustomerName LIKE 'U_' ; This query will return all customers whose names start with the letter \"U\" and have a single character after that.","title":"Examples"},{"location":"Week%203/livesession/#using-not-like","text":"The NOT LIKE operator is used to negate the results of a LIKE query. It returns all records that do not match the specified pattern.","title":"Using NOT LIKE"},{"location":"Week%203/livesession/#example","text":"SELECT * FROM Customers WHERE CustomerName NOT LIKE 'USA' ; This query will return all customers whose names do not contain the string \"USA\".","title":"Example"},{"location":"Week%203/livesession/#using-like-with-multiple-values","text":"The LIKE operator can be used with multiple string patterns by combining it with the OR operator.","title":"Using LIKE with Multiple Values"},{"location":"Week%203/livesession/#example_1","text":"SELECT * FROM Customers WHERE last_name LIKE 'R%t' OR last_name LIKE '%e' ; The \"in\" and \"not in\" operators in Python and SQL are used to check if a specified value is present or absent in a sequence or a table. Here are the key points about these operators:","title":"Example"},{"location":"Week%203/livesession/#python-in-and-not-in-operators","text":"\"in\" Operator: The \"in\" operator checks if a specified value is present in a sequence (like a list, tuple, or string). It returns True if the value is found and False if it is not[1].","title":"Python \"in\" and \"not in\" Operators"},{"location":"Week%203/livesession/#sql-in-and-not-in-operators","text":"\"in\" Operator: The \"in\" operator is used with the SELECT , UPDATE , and DELETE statements to filter records based on a condition. It selects records where the specified column's value is present in a list of values[2][3][4]. \"not in\" Operator: The \"not in\" operator is used to filter out records where the specified column's value is not present in a list of values. It is often used with the SELECT , UPDATE , and DELETE statements to exclude specific records[2][3][4].","title":"SQL \"in\" and \"not in\" Operators"},{"location":"Week%203/livesession/#key-points","text":"Both \"in\" and \"not in\" operators can be used in SQL with the WHERE clause to filter data. The \"in\" operator returns True if the value is found and False if it is not. The \"not in\" operator returns True if the value is not found and False if it is found. The \"in\" operator can be used with both numeric and non-numeric data types, such as strings. The \"not in\" operator can be used with both numeric and non-numeric data types, such as strings. The \"in\" operator can be used in combination with other operators like AND and OR . The \"not in\" operator can be used in combination with other operators like AND and OR .","title":"Key Points"},{"location":"Week%203/livesession/#examples_1","text":"Python: list1 = [ 1 , 2 , 3 , 4 , 5 ] string1 = \"My name is AskPython\" tuple1 = ( 11 , 22 , 33 , 44 ) print ( 5 in list1 ) # True print ( \"is\" in string1 ) # True print ( 88 in tuple1 ) # False print ( 5 not in list1 ) # False print ( \"is\" not in string1 ) # False print ( 88 not in tuple1 ) # True - SQL: SELECT Student_ID , Student_name , City , Age FROM student WHERE Age IN ( 25 , 29 ) AND City NOT IN ( 'Chennai' , 'Delhi' ); SELECT Name FROM Emp WHERE age NOT IN ( 23 , 22 , 21 ); SELECT * FROM Emp WHERE country NOT IN ( 'Australia' , 'Austria' ); SQL joins are used to combine rows from two or more tables based on a related column between them. This allows you to retrieve data from multiple tables by establishing logical relationships between them. Here are the different types of SQL joins:","title":"Examples"},{"location":"Week%203/livesession/#inner-join","text":"An INNER JOIN returns all records from both tables where the join condition is satisfied. It is the most commonly used type of join and assumes that there is a match between the tables.","title":"INNER JOIN"},{"location":"Week%203/livesession/#syntax","text":"SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 INNER JOIN table2 ON table1 . matching_column = table2 . matching_column ;","title":"Syntax:"},{"location":"Week%203/livesession/#example_2","text":"SELECT StudentCourse . COURSE_ID , Student . NAME , Student . AGE FROM Student INNER JOIN StudentCourse ON Student . ROLL_NO = StudentCourse . ROLL_NO ;","title":"Example:"},{"location":"Week%203/livesession/#left-join","text":"A LEFT JOIN returns all records from the left table and the matched records from the right table. If there is no match, the result-set will contain null values for the right table.","title":"LEFT JOIN"},{"location":"Week%203/livesession/#syntax_1","text":"SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 LEFT JOIN table2 ON table1 . matching_column = table2 . matching_column ;","title":"Syntax:"},{"location":"Week%203/livesession/#example_3","text":"SELECT Student . NAME , StudentCourse . COURSE_ID FROM Student LEFT JOIN StudentCourse ON StudentCourse . ROLL_NO = Student . ROLL_NO ;","title":"Example:"},{"location":"Week%203/livesession/#right-join","text":"A RIGHT JOIN returns all records from the right table and the matched records from the left table. If there is no match, the result-set will contain null values for the left table.","title":"RIGHT JOIN"},{"location":"Week%203/livesession/#syntax_2","text":"SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 RIGHT JOIN table2 ON table1 . matching_column = table2 . matching_column ;","title":"Syntax:"},{"location":"Week%203/livesession/#example_4","text":"SELECT StudentCourse . COURSE_ID , Student . NAME FROM StudentCourse RIGHT JOIN Student ON StudentCourse . ROLL_NO = Student . ROLL_NO ;","title":"Example:"},{"location":"Week%203/livesession/#full-join","text":"A FULL JOIN returns all records when there is a match in either the left or right table.","title":"FULL JOIN"},{"location":"Week%203/livesession/#syntax_3","text":"SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 FULL JOIN table2 ON table1 . matching_column = table2 . matching_column ;","title":"Syntax:"},{"location":"Week%203/livesession/#example_5","text":"SELECT Student . NAME , StudentCourse . COURSE_ID FROM Student FULL JOIN StudentCourse ON StudentCourse . ROLL_NO = Student . ROLL_NO ;","title":"Example:"},{"location":"Week%203/livesession/#cross-join","text":"A CROSS JOIN returns all possible combinations of rows from both tables.","title":"CROSS JOIN"},{"location":"Week%203/livesession/#syntax_4","text":"SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 CROSS JOIN table2 ;","title":"Syntax:"},{"location":"Week%203/livesession/#example_6","text":"SELECT Student . NAME , StudentCourse . COURSE_ID FROM Student CROSS JOIN StudentCourse ;","title":"Example:"},{"location":"Week%203/livesession/#self-join","text":"A SELF JOIN is used to join a table to itself. This is useful for comparing rows within the same table.","title":"SELF JOIN"},{"location":"Week%203/livesession/#syntax_5","text":"SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 AS table1_alias JOIN table1 AS table2_alias ON table1_alias . matching_column = table2_alias . matching_column ;","title":"Syntax:"},{"location":"Week%203/livesession/#example_7","text":"SELECT Student . NAME , StudentCourse . COURSE_ID FROM Student AS s JOIN Student AS sc ON s . ROLL_NO = sc . ROLL_NO ;","title":"Example:"},{"location":"Week%203/livesession/#natural-join","text":"A NATURAL JOIN is similar to an INNER JOIN but automatically matches columns with the same names.","title":"NATURAL JOIN"},{"location":"Week%203/livesession/#syntax_6","text":"SELECT table1 . column1 , table1 . column2 , table2 . column1 , ... FROM table1 NATURAL JOIN table2 ;","title":"Syntax:"},{"location":"Week%203/livesession/#example_8","text":"SELECT Student . NAME , StudentCourse . COURSE_ID FROM Student NATURAL JOIN StudentCourse ;","title":"Example:"},{"location":"Week%203/image/Lecture%203.3%20-%20Intermediate%20SQL2/","text":"Lecture 3.3 - Intermediate SQL2.pdf (PDF file) Summary Module 13: Intermediate SQL/2 Objectives To learn SQL expressions for Join To learn SQL expressions for Views Outline Join Expressions Views Join Expressions Definition: Join operations take two relations and return as a result another relation. Purpose: Join operations are used to combine rows from two relations based on a specified condition. Types of Join between Relations Cross join: Returns the Cartesian product of rows from tables in the join. Inner join: Returns rows that match on the specified condition. Outer join: Returns all rows from one relation and rows from the other relation that match on the specified condition. Left outer join: Returns all rows from the left relation and rows from the right relation that match on the specified condition. Right outer join: Returns all rows from the right relation and rows from the left relation that match on the specified condition. Full outer join: Returns all rows from both relations, regardless of whether they match on the specified condition. Cross JOIN Cross join: Returns the Cartesian product of rows from tables in the join. select * from employee cross join department; is same select * from employee , department; INNER JOIN Inner join is the intersection of the two table if we add natural then the result will ignore the the common attribute in the second table Outer Join Left Outer join Right outer Join Natural Full outer Join Views Definition: A view is a virtual relation that is defined by a query expression. Purpose: Views provide a mechanism to hide certain data from users or to create a new relation that is derived from existing relations. View Definition A view is defined using the create view statement, which has the form: create view v as < query expression > The view name is represented by v . The query expression specifies the rows and columns that are included in the view. View Expansion Definition: View expansion is the process of replacing a view reference in a query or another view with the query expression that defines the view. Purpose: View expansion is used to ensure that queries that use views always return the correct results, even if the underlying relations change. Update of a View Updates to views are typically not allowed. In some cases, updates to simple views (views that meet certain criteria) may be allowed. Materialized Views Definition: A materialized view is a physical table that contains the results of a query. Purpose: Materialized views can improve query performance by avoiding the need to re-execute the query every time it is used. Module Summary Join expressions are used to combine rows from two relations based on a specified condition. Views provide a mechanism to hide certain data from users or to create a new relation that is derived from existing relations. View expansion ensures that queries that use views always return the correct results, even if the underlying relations change. Updates to views are typically not allowed, but may be allowed in some cases for simple views. Materialized views can improve query performance by avoiding the need to re-execute the query every time it is used.","title":"Lecture 3.3 - Intermediate SQL2.pdf (PDF file)"},{"location":"Week%203/image/Lecture%203.3%20-%20Intermediate%20SQL2/#lecture-33-intermediate-sql2pdf-pdf-file","text":"Summary Module 13: Intermediate SQL/2 Objectives To learn SQL expressions for Join To learn SQL expressions for Views Outline Join Expressions Views Join Expressions Definition: Join operations take two relations and return as a result another relation. Purpose: Join operations are used to combine rows from two relations based on a specified condition. Types of Join between Relations Cross join: Returns the Cartesian product of rows from tables in the join. Inner join: Returns rows that match on the specified condition. Outer join: Returns all rows from one relation and rows from the other relation that match on the specified condition. Left outer join: Returns all rows from the left relation and rows from the right relation that match on the specified condition. Right outer join: Returns all rows from the right relation and rows from the left relation that match on the specified condition. Full outer join: Returns all rows from both relations, regardless of whether they match on the specified condition.","title":"Lecture 3.3 - Intermediate SQL2.pdf (PDF file)"},{"location":"Week%203/image/Lecture%203.3%20-%20Intermediate%20SQL2/#cross-join","text":"Cross join: Returns the Cartesian product of rows from tables in the join. select * from employee cross join department; is same select * from employee , department;","title":"Cross JOIN"},{"location":"Week%203/image/Lecture%203.3%20-%20Intermediate%20SQL2/#inner-join","text":"Inner join is the intersection of the two table if we add natural then the result will ignore the the common attribute in the second table","title":"INNER JOIN"},{"location":"Week%203/image/Lecture%203.3%20-%20Intermediate%20SQL2/#outer-join","text":"","title":"Outer Join"},{"location":"Week%203/image/Lecture%203.3%20-%20Intermediate%20SQL2/#left-outer-join","text":"","title":"Left Outer join"},{"location":"Week%203/image/Lecture%203.3%20-%20Intermediate%20SQL2/#right-outer-join","text":"","title":"Right outer Join"},{"location":"Week%203/image/Lecture%203.3%20-%20Intermediate%20SQL2/#natural-full-outer-join","text":"","title":"Natural Full outer Join"},{"location":"Week%203/image/Lecture%203.3%20-%20Intermediate%20SQL2/#views","text":"Definition: A view is a virtual relation that is defined by a query expression. Purpose: Views provide a mechanism to hide certain data from users or to create a new relation that is derived from existing relations. View Definition A view is defined using the create view statement, which has the form: create view v as < query expression > The view name is represented by v . The query expression specifies the rows and columns that are included in the view. View Expansion Definition: View expansion is the process of replacing a view reference in a query or another view with the query expression that defines the view. Purpose: View expansion is used to ensure that queries that use views always return the correct results, even if the underlying relations change. Update of a View Updates to views are typically not allowed. In some cases, updates to simple views (views that meet certain criteria) may be allowed. Materialized Views Definition: A materialized view is a physical table that contains the results of a query. Purpose: Materialized views can improve query performance by avoiding the need to re-execute the query every time it is used. Module Summary Join expressions are used to combine rows from two relations based on a specified condition. Views provide a mechanism to hide certain data from users or to create a new relation that is derived from existing relations. View expansion ensures that queries that use views always return the correct results, even if the underlying relations change. Updates to views are typically not allowed, but may be allowed in some cases for simple views. Materialized views can improve query performance by avoiding the need to re-execute the query every time it is used.","title":"Views"},{"location":"Week%204/Lecture%204.1%20-%20Formal%20Relational%20Query%20Languages1_annotated/","text":"Formal Relational Query Languages1_annotated Summary Module 16: Formal Relational Query Languages/1 Week Recap This lecture provides an overview of formal query languages in the context of database management systems, with a focus on relational algebra. Objectives Understand the concepts of formal query language and relational algebra. Outline Formal Relational Query Language Relational Algebra Select Project Union Difference Intersection Cartesian Product Rename Division Relational Algebra A procedural language used to manipulate relations (tables). Introduced by Edgar F. Codd in 1970. Consists of six basic operators: Select (\u03c3) Project (\u03a0) Union (\u222a) Set Difference (-) Cartesian Product (x) Rename (\u03c1) Select Operation (\u03c3) Selects tuples from a relation based on a predicate condition. Notation: \\(\u03c3_p(r)\\) , where p is the selection predicate. Example: \\(\u03c3_{dept_name='Physics'}(instructor)\\) Project Operation (\u03a0) Projects a relation onto a subset of its attributes. Notation: \\(\u03a0_{A1,A2,...,Ak}(r)\\) , where A1, A2,..., Ak are the selected attributes. Example: \\(\u03a0_{ID,name,salary}(instructor)\\) \" Union Operation (\u222a) Combines two relations with the same schema. Notation: \\(r \u222a s\\) Requires compatible arities (number of attributes) and attribute domains. Example: Find all courses taught in Fall 2009 or Spring 2010: \\(\u03a0_{course_id}(\u03c3_{semester='Fall'\u2227year=2009}(section)) \u222a \u03a0_{course_id}(\u03c3_{semester='Spring'\u2227year=2010}(section))\\) Difference Operation (-) Removes tuples from a relation that are also in another relation. Notation: r - s. Requires compatible relations (arities and attribute domains). Example: Find all courses taught in Fall 2009 but not in Spring 2010: \u03a0course_id(\u03c3semester='Fall'\u2227year=2009(section)) - \u03a0course_id(\u03c3semester='Spring'\u2227year=2010(section)) Intersection Operation (\u2229) Retains tuples that are common to both relations. Notation: \\(r \u2229 s\\) . Requires compatible relations (arities and attribute domains). Note: \\(r \u2229 s = r - (r - s)\\) Cartesian Product Operation (x) Produces all possible combinations of tuples from two relations. Notation: \\(\\mathbf{r} \\times \\mathbf{s}\\) Attributes of r and s must be disjoint (non-overlapping). If attributes are not disjoint, renaming must be used. Rename Operation (\u03c1) Allows for the naming of the results of relational algebra expressions. Allows for references to relations using different names. Notation: \\(\u03c1_{x}(E)\\) renames the expression E as X. Division Operation (\u00f7) Derived operation expressed in terms of other operations. Notation: r \u00f7 s \u2261 \u03a0R-S(r) - \u03a0R-S(r)((\u03a0R-S(r) x s) - \u03a0R-S,S(r)) Returns tuples from r that appear with every tuple in s. Module Summary Introduced the concept of formal relational query languages. Focused primarily on relational algebra and its six basic operators. Highlighted the practical applications of relational algebra operations.","title":"Formal Relational Query Languages1_annotated"},{"location":"Week%204/Lecture%204.1%20-%20Formal%20Relational%20Query%20Languages1_annotated/#formal-relational-query-languages1_annotated","text":"Summary Module 16: Formal Relational Query Languages/1 Week Recap This lecture provides an overview of formal query languages in the context of database management systems, with a focus on relational algebra. Objectives Understand the concepts of formal query language and relational algebra. Outline Formal Relational Query Language Relational Algebra Select Project Union Difference Intersection Cartesian Product Rename Division Relational Algebra A procedural language used to manipulate relations (tables). Introduced by Edgar F. Codd in 1970. Consists of six basic operators: Select (\u03c3) Project (\u03a0) Union (\u222a) Set Difference (-) Cartesian Product (x) Rename (\u03c1) Select Operation (\u03c3) Selects tuples from a relation based on a predicate condition. Notation: \\(\u03c3_p(r)\\) , where p is the selection predicate. Example: \\(\u03c3_{dept_name='Physics'}(instructor)\\) Project Operation (\u03a0) Projects a relation onto a subset of its attributes. Notation: \\(\u03a0_{A1,A2,...,Ak}(r)\\) , where A1, A2,..., Ak are the selected attributes. Example: \\(\u03a0_{ID,name,salary}(instructor)\\) \" Union Operation (\u222a) Combines two relations with the same schema. Notation: \\(r \u222a s\\) Requires compatible arities (number of attributes) and attribute domains. Example: Find all courses taught in Fall 2009 or Spring 2010: \\(\u03a0_{course_id}(\u03c3_{semester='Fall'\u2227year=2009}(section)) \u222a \u03a0_{course_id}(\u03c3_{semester='Spring'\u2227year=2010}(section))\\) Difference Operation (-) Removes tuples from a relation that are also in another relation. Notation: r - s. Requires compatible relations (arities and attribute domains). Example: Find all courses taught in Fall 2009 but not in Spring 2010: \u03a0course_id(\u03c3semester='Fall'\u2227year=2009(section)) - \u03a0course_id(\u03c3semester='Spring'\u2227year=2010(section)) Intersection Operation (\u2229) Retains tuples that are common to both relations. Notation: \\(r \u2229 s\\) . Requires compatible relations (arities and attribute domains). Note: \\(r \u2229 s = r - (r - s)\\) Cartesian Product Operation (x) Produces all possible combinations of tuples from two relations. Notation: \\(\\mathbf{r} \\times \\mathbf{s}\\) Attributes of r and s must be disjoint (non-overlapping). If attributes are not disjoint, renaming must be used. Rename Operation (\u03c1) Allows for the naming of the results of relational algebra expressions. Allows for references to relations using different names. Notation: \\(\u03c1_{x}(E)\\) renames the expression E as X. Division Operation (\u00f7) Derived operation expressed in terms of other operations. Notation: r \u00f7 s \u2261 \u03a0R-S(r) - \u03a0R-S(r)((\u03a0R-S(r) x s) - \u03a0R-S,S(r)) Returns tuples from r that appear with every tuple in s. Module Summary Introduced the concept of formal relational query languages. Focused primarily on relational algebra and its six basic operators. Highlighted the practical applications of relational algebra operations.","title":"Formal Relational Query Languages1_annotated"},{"location":"Week%204/Lecture%204.2%20-%20Formal%20Relational%20Query%20Languages2_annotated/","text":"Formal Relational Query Languages Summary Module 17: Formal Relational Query Languages/2 Objectives and Outline Objectives To understand formal calculus-based query language through relational algebra Outline Overview of Tuple Relational Calculus Overview of Domain Relational Calculus Equivalence of Algebra and Calculus Predicate Logic Predicate Logic is an extension of Propositional Logic or Boolean Algebra. It adds the concept of predicates and quantifiers to better capture the meaning of statements that cannot be adequately expressed by propositional logic. Tuple Relational Calculus and Domain Relational Calculus are based on Predicate Calculus. Predicate A predicate is a property that the subject of a statement can have. A statement involving n variables x1, x2, x3, \u00b7 \u00b7 \u00b7 , xn can be denoted by P(x1, x2, x3, \u00b7 \u00b7 \u00b7 , xn). Quantifiers Universal Quantifier: Asserts that a property is true for all the values of a variable in a particular domain. Existential Quantifier: Asserts that there is an element with a certain property. Tuple Relational Calculus A non-procedural query language, where each query is of the form: t = resulting tuples P(t) = predicate Domain Relational Calculus A non-procedural query language equivalent in power to the tuple relational calculus Each query is an expression of the form: x1, x2, . . . , xn represent domain variables P represents a formula similar to that of the predicate calculus Equivalence of Algebra and Calculus Relational Algebra, Tuple Relational Calculus, and Domain Relational Calculus are equivalent in expressive power. This equivalence means that any query that can be expressed in one language can also be expressed in the other two languages.","title":"Formal Relational Query Languages"},{"location":"Week%204/Lecture%204.2%20-%20Formal%20Relational%20Query%20Languages2_annotated/#formal-relational-query-languages","text":"Summary Module 17: Formal Relational Query Languages/2 Objectives and Outline Objectives To understand formal calculus-based query language through relational algebra Outline Overview of Tuple Relational Calculus Overview of Domain Relational Calculus Equivalence of Algebra and Calculus Predicate Logic Predicate Logic is an extension of Propositional Logic or Boolean Algebra. It adds the concept of predicates and quantifiers to better capture the meaning of statements that cannot be adequately expressed by propositional logic. Tuple Relational Calculus and Domain Relational Calculus are based on Predicate Calculus. Predicate A predicate is a property that the subject of a statement can have. A statement involving n variables x1, x2, x3, \u00b7 \u00b7 \u00b7 , xn can be denoted by P(x1, x2, x3, \u00b7 \u00b7 \u00b7 , xn). Quantifiers Universal Quantifier: Asserts that a property is true for all the values of a variable in a particular domain. Existential Quantifier: Asserts that there is an element with a certain property. Tuple Relational Calculus A non-procedural query language, where each query is of the form: t = resulting tuples P(t) = predicate Domain Relational Calculus A non-procedural query language equivalent in power to the tuple relational calculus Each query is an expression of the form: x1, x2, . . . , xn represent domain variables P represents a formula similar to that of the predicate calculus Equivalence of Algebra and Calculus Relational Algebra, Tuple Relational Calculus, and Domain Relational Calculus are equivalent in expressive power. This equivalence means that any query that can be expressed in one language can also be expressed in the other two languages.","title":"Formal Relational Query Languages"},{"location":"Week%204/Lecture%204.3%20-%20Entity-Relationship%20Model1_annotated/","text":"Entity-Relationship Model Summary Module 18: Entity-Relationship (ER) Model Introduction ER model is a data modeling technique used to represent the logical structure of a database. It provides a graphical representation of the entities, attributes, and relationships present in a database. Design Process Requirement analysis: Analyze data needs of users. Database design: Create an ER diagram and normalize the database. Implementation: Convert ER diagram to tables, load data, and test. ER Model Concepts 1. Attributes Properties associated with entities. Can be simple or composite, single-valued or multivalued, and derived . Each attribute has a domain , which is the set of possible values for that attribute. 2. Entity Sets Collections of entities that share the same properties and attributes. Each entity is uniquely identified by a primary key. 3. Relationships Associations between entities. Can be binary (between two entities) or ternary (between three entities). Cardinality defines the number of entities that can be associated with each other through a relationship. 4. Cardinality Specifies the mapping between entities in a relationship. Types of cardinality include one-to-one, one-to-many, many-to-one, and many-to-many. partial 5. Constraints Rules that restrict the data in a database. Can be used to enforce referential integrity, data types, and other requirements. 6. Weak Entity Sets Entity sets that do not have sufficient attributes to uniquely identify their entities. Must have a total participation in an identifying relationship with a strong entity set. Discrimator ER Diagram A graphical representation of an ER model. Uses rectangles to represent entities, ovals to represent attributes, and diamonds to represent relationships. Cardinality is indicated using lines and notation. Example ER Diagram Benefits of ER Modeling Facilitates database design by mapping real-world concepts to a logical structure. Helps identify and understand data relationships. Improves communication between stakeholders involved in database development. Conclusion The ER model is a fundamental concept in database management systems. It provides a structured and intuitive way to represent the data and relationships in a database. By understanding the principles of ER modeling, database designers can create efficient and effective database systems.","title":"Entity-Relationship Model"},{"location":"Week%204/Lecture%204.3%20-%20Entity-Relationship%20Model1_annotated/#entity-relationship-model","text":"Summary Module 18: Entity-Relationship (ER) Model Introduction ER model is a data modeling technique used to represent the logical structure of a database. It provides a graphical representation of the entities, attributes, and relationships present in a database. Design Process Requirement analysis: Analyze data needs of users. Database design: Create an ER diagram and normalize the database. Implementation: Convert ER diagram to tables, load data, and test. ER Model Concepts 1. Attributes Properties associated with entities. Can be simple or composite, single-valued or multivalued, and derived . Each attribute has a domain , which is the set of possible values for that attribute. 2. Entity Sets Collections of entities that share the same properties and attributes. Each entity is uniquely identified by a primary key. 3. Relationships Associations between entities. Can be binary (between two entities) or ternary (between three entities). Cardinality defines the number of entities that can be associated with each other through a relationship. 4. Cardinality Specifies the mapping between entities in a relationship. Types of cardinality include one-to-one, one-to-many, many-to-one, and many-to-many. partial 5. Constraints Rules that restrict the data in a database. Can be used to enforce referential integrity, data types, and other requirements. 6. Weak Entity Sets Entity sets that do not have sufficient attributes to uniquely identify their entities. Must have a total participation in an identifying relationship with a strong entity set. Discrimator ER Diagram A graphical representation of an ER model. Uses rectangles to represent entities, ovals to represent attributes, and diamonds to represent relationships. Cardinality is indicated using lines and notation. Example ER Diagram Benefits of ER Modeling Facilitates database design by mapping real-world concepts to a logical structure. Helps identify and understand data relationships. Improves communication between stakeholders involved in database development. Conclusion The ER model is a fundamental concept in database management systems. It provides a structured and intuitive way to represent the data and relationships in a database. By understanding the principles of ER modeling, database designers can create efficient and effective database systems.","title":"Entity-Relationship Model"},{"location":"Week%204/Lecture%204.4%20-%20Entity-Relationship%20Model2_annotated/","text":"Entity-Relationship (ER) Model/2 Overview An Entity-Relationship (ER) model is a graphical representation of a database schema. It uses entity sets, relationship sets, and attributes to describe the structure of a database. ER Diagram Notation Entity Sets: Represented as rectangles. Attributes are listed inside the rectangle. Primary key attributes are underlined. Relationship Sets: Represented as diamonds. _ _ _ means a attribute of the relationship Cardinality Constraints Express the number of entities in one set that can relate to a single entity in another set. Represented using directed lines (\u2192) for \"one\" and undirected lines (\u2014) for \"many.\" Examples: A student can have at most one advisor (one-to-one relationship). A course can have many students (one-to-many relationship). A student can take many courses (many-to-many relationship). Constraints Participation: Total: Every entity in an entity set participates in at least one relationship in the relationship set. Partial: Some entities may not participate in any relationship. * Bounds: Minimum and maximum number of relationships an entity can participate in. Represented using the notation l..h, where l is the minimum and h is the maximum. {} -> multivalue attribute () -> function Weak Entity set ER Model to Relational Schema Entity Sets: Each entity set becomes a table with columns for each attribute. Relationship Sets: Many-to-many relationships become separate tables with columns for the primary keys of the participating entities. One-to-many and many-to-one relationships can be represented by adding an extra attribute to the \"many\" side with the primary key of the \"one\" side. Complex Attributes Composite Attributes: Attributes that consist of multiple components. Components are flattened out into separate attributes in the relational schema. Multivalued Attributes: Attributes that can have multiple values for a single entity. Represented by a separate table with columns for the primary key of the entity and an attribute for each value of the multivalued attribute. Redundancy Schemas derived from relationship sets that are total on the \"many\" side may contain redundant data. This redundancy can be reduced by adding an extra attribute to the \"many\" side. Schemas for weak entity sets are redundant and can be eliminated. Module Summary Explored the use of ER diagrams to model database structures. Discussed the translation of ER models into relational schemas. Analyzed the various constraints and attributes used in ER models. Explained how to handle complex attributes and redundancy in relational schemas derived from ER models.","title":"Entity-Relationship (ER) Model/2"},{"location":"Week%204/Lecture%204.4%20-%20Entity-Relationship%20Model2_annotated/#entity-relationship-er-model2","text":"","title":"Entity-Relationship (ER) Model/2"},{"location":"Week%204/Lecture%204.4%20-%20Entity-Relationship%20Model2_annotated/#overview","text":"An Entity-Relationship (ER) model is a graphical representation of a database schema. It uses entity sets, relationship sets, and attributes to describe the structure of a database.","title":"Overview"},{"location":"Week%204/Lecture%204.4%20-%20Entity-Relationship%20Model2_annotated/#er-diagram-notation","text":"Entity Sets: Represented as rectangles. Attributes are listed inside the rectangle. Primary key attributes are underlined. Relationship Sets: Represented as diamonds. _ _ _ means a attribute of the relationship","title":"ER Diagram Notation"},{"location":"Week%204/Lecture%204.4%20-%20Entity-Relationship%20Model2_annotated/#cardinality-constraints","text":"Express the number of entities in one set that can relate to a single entity in another set. Represented using directed lines (\u2192) for \"one\" and undirected lines (\u2014) for \"many.\" Examples: A student can have at most one advisor (one-to-one relationship). A course can have many students (one-to-many relationship). A student can take many courses (many-to-many relationship).","title":"Cardinality Constraints"},{"location":"Week%204/Lecture%204.4%20-%20Entity-Relationship%20Model2_annotated/#constraints","text":"Participation: Total: Every entity in an entity set participates in at least one relationship in the relationship set. Partial: Some entities may not participate in any relationship. * Bounds: Minimum and maximum number of relationships an entity can participate in. Represented using the notation l..h, where l is the minimum and h is the maximum. {} -> multivalue attribute () -> function","title":"Constraints"},{"location":"Week%204/Lecture%204.4%20-%20Entity-Relationship%20Model2_annotated/#weak-entity-set","text":"","title":"Weak Entity set"},{"location":"Week%204/Lecture%204.4%20-%20Entity-Relationship%20Model2_annotated/#er-model-to-relational-schema","text":"Entity Sets: Each entity set becomes a table with columns for each attribute. Relationship Sets: Many-to-many relationships become separate tables with columns for the primary keys of the participating entities. One-to-many and many-to-one relationships can be represented by adding an extra attribute to the \"many\" side with the primary key of the \"one\" side.","title":"ER Model to Relational Schema"},{"location":"Week%204/Lecture%204.4%20-%20Entity-Relationship%20Model2_annotated/#complex-attributes","text":"Composite Attributes: Attributes that consist of multiple components. Components are flattened out into separate attributes in the relational schema. Multivalued Attributes: Attributes that can have multiple values for a single entity. Represented by a separate table with columns for the primary key of the entity and an attribute for each value of the multivalued attribute.","title":"Complex Attributes"},{"location":"Week%204/Lecture%204.4%20-%20Entity-Relationship%20Model2_annotated/#redundancy","text":"Schemas derived from relationship sets that are total on the \"many\" side may contain redundant data. This redundancy can be reduced by adding an extra attribute to the \"many\" side. Schemas for weak entity sets are redundant and can be eliminated.","title":"Redundancy"},{"location":"Week%204/Lecture%204.4%20-%20Entity-Relationship%20Model2_annotated/#module-summary","text":"Explored the use of ER diagrams to model database structures. Discussed the translation of ER models into relational schemas. Analyzed the various constraints and attributes used in ER models. Explained how to handle complex attributes and redundancy in relational schemas derived from ER models.","title":"Module Summary"},{"location":"Week%204/Lecture%204.5%20-%20Entity-Relationship%20Model3_annotated/","text":"Entity-Relationship Model Summary Entity-Relationship Model (ER Model): Extended Features and Design Issues Introduction The Entity-Relationship Model (ER Model) is a data modeling technique used to represent the structure and relationships of data in a database. ER Model provides a conceptual representation of data, making it easier to understand and manipulate. In this module, we will explore the extended features of the ER Model that enhance its representational capabilities and discuss various design issues involved in designing ER diagrams. Extended ER Features Non-Binary Relationships: Most ER relationships are binary, involving two entity sets. Non-binary relationships involve three or more entity sets. They can be represented with a ternary relationship construct, where three entity sets are connected by a diamond-shaped box. Specialization and Generalization: Specialization (ISA): Subdivide an entity set into smaller, more specific sub-entity sets (lower-level entity sets). Generalization: Combine multiple entity sets into a higher-level entity set that captures their common characteristics. Represented by a triangle component labeled ISA, indicating that a lower-level entity set inherits attributes and relationships from the higher-level entity set. Aggregation: Allows entities to have complex relationships by treating a relationship as an abstract entity. Associates attributes with the relationship itself, rather than with individual entities. Design Issues Entities vs. Attributes: Entities represent real-world objects or concepts. Attributes are characteristics of entities. Consider whether a real-world concept is best represented as an entity or an attribute. Entities vs. Relationship Sets: Entities represent objects, while relationship sets represent associations between objects. Use relationship sets to describe actions or interactions between entities. Binary vs. Non-Binary Relationships: Non-binary relationships represent complex associations involving multiple entities. Binary relationships are simpler and easier to implement. Consider whether a relationship is naturally binary or non-binary based on the underlying real-world scenario. Design Decisions: The use of an attribute or entity set to represent an object. Whether a real-world concept should be represented by an entity set or a relationship set. The use of ternary relationships versus pairs of binary relationships. The use of strong or weak entity sets. The use of specialization/generalization for modularity in design. The use of aggregation to treat complex relationships as single units. ER Notation Symbols Used in ER Notation: Symbol Meaning Rectangle Entity set Diamond Relationship set Line Connection between entity sets and relationship sets Ellipsis Weak entity set Double line Total specialization Dashed line Partial specialization Alternate Notations: Chen notation IDE1FX notation Crow's feet notation Module Summary In this module, we explored the extended features of the ER Model, including non-binary relationships, specialization/generalization, and aggregation. We also discussed various design issues that arise when creating ER diagrams and the symbols used in ER notation. Understanding these concepts and techniques is essential for effectively designing and implementing databases.","title":"Entity-Relationship Model"},{"location":"Week%204/Lecture%204.5%20-%20Entity-Relationship%20Model3_annotated/#entity-relationship-model","text":"Summary Entity-Relationship Model (ER Model): Extended Features and Design Issues Introduction The Entity-Relationship Model (ER Model) is a data modeling technique used to represent the structure and relationships of data in a database. ER Model provides a conceptual representation of data, making it easier to understand and manipulate. In this module, we will explore the extended features of the ER Model that enhance its representational capabilities and discuss various design issues involved in designing ER diagrams. Extended ER Features Non-Binary Relationships: Most ER relationships are binary, involving two entity sets. Non-binary relationships involve three or more entity sets. They can be represented with a ternary relationship construct, where three entity sets are connected by a diamond-shaped box. Specialization and Generalization: Specialization (ISA): Subdivide an entity set into smaller, more specific sub-entity sets (lower-level entity sets). Generalization: Combine multiple entity sets into a higher-level entity set that captures their common characteristics. Represented by a triangle component labeled ISA, indicating that a lower-level entity set inherits attributes and relationships from the higher-level entity set. Aggregation: Allows entities to have complex relationships by treating a relationship as an abstract entity. Associates attributes with the relationship itself, rather than with individual entities. Design Issues Entities vs. Attributes: Entities represent real-world objects or concepts. Attributes are characteristics of entities. Consider whether a real-world concept is best represented as an entity or an attribute. Entities vs. Relationship Sets: Entities represent objects, while relationship sets represent associations between objects. Use relationship sets to describe actions or interactions between entities. Binary vs. Non-Binary Relationships: Non-binary relationships represent complex associations involving multiple entities. Binary relationships are simpler and easier to implement. Consider whether a relationship is naturally binary or non-binary based on the underlying real-world scenario. Design Decisions: The use of an attribute or entity set to represent an object. Whether a real-world concept should be represented by an entity set or a relationship set. The use of ternary relationships versus pairs of binary relationships. The use of strong or weak entity sets. The use of specialization/generalization for modularity in design. The use of aggregation to treat complex relationships as single units. ER Notation Symbols Used in ER Notation: Symbol Meaning Rectangle Entity set Diamond Relationship set Line Connection between entity sets and relationship sets Ellipsis Weak entity set Double line Total specialization Dashed line Partial specialization Alternate Notations: Chen notation IDE1FX notation Crow's feet notation Module Summary In this module, we explored the extended features of the ER Model, including non-binary relationships, specialization/generalization, and aggregation. We also discussed various design issues that arise when creating ER diagrams and the symbols used in ER notation. Understanding these concepts and techniques is essential for effectively designing and implementing databases.","title":"Entity-Relationship Model"},{"location":"Week%204/Tutorial%204.2/","text":"Summary of Tutorial 4.2.pdf Summary Tuple Relational Calculus (TRC) TRC is a non-procedural query language used to retrieve data from relational databases. It defines queries in a declarative manner, specifying the conditions that the resulting tuples must satisfy. The syntax of a TRC query is: {t | P(t)} where: t is the variable representing the resulting tuples. P(t) is the predicate, a logical expression that specifies the conditions that the tuples must satisfy to be included in the result set. Predicates Predicates can be constructed using the following logical operators: \u2227 (AND): Conjunction (true if both operands are true) \u2228 (OR): Disjunction (true if either operand is true) \u00ac (NOT): Negation (true if the operand is false) Quantifiers TRC also supports quantifiers, which express conditions that apply to all or some tuples in a relation: \u2203t \u2208 r (Q(t)) : There exists a tuple t in relation r such that predicate Q(t) is true. \u2200t \u2208 r (Q(t)) : For all tuples t in relation r , predicate Q(t) is true. Examples Example 1: Querying Students Table {t.Fname | Student(t) \u2227 t.age > 21} This query retrieves the first names ( Fname ) of students who are over 21 years old from a table named Student . Example 2: Querying Student and Course Tables {t | \u2203s \u2208 student \u2203c \u2208 course(s.courseId = c.courseId \u2227 c.cname = \u2018DBMS\u2019 \u2227t.name = s.name)} This query retrieves the names ( t.name ) of students who have taken a course named \u2018DBMS`. Example 3: Querying Flights, Aircraft, Certified, and Employees Tables {F.flno | F \u2208 Flights \u2227 \u2203A \u2208 Aircraft\u2203C \u2208 Certified\u2203E \u2208 Employees(A.cruisingrange > F.distance \u2227 A.aid =C.aid \u2227 E.salary > 100, 000 \u2227 E.eid = C.eid)} This query identifies the flight numbers ( F.flno ) of flights that can be piloted by every pilot whose salary is over $100,000. Applications of TRC TRC is used for querying relational databases in a variety of applications, including: Retrieving data from multiple tables Filtering data based on complex conditions Aggregating data (e.g., finding average salaries) Identifying relationships between data Modifying data (e.g., updating or deleting rows) Advantages of TRC Declarative: TRC queries express what data to retrieve without specifying how to retrieve it. Expressive: TRC allows for complex queries involving multiple tables and conditions. Portable: TRC queries can be used across different database management systems. Disadvantages of TRC Efficiency: TRC queries can be inefficient for large databases, especially when they involve complex joins. Complexity: Writing optimized TRC queries can be challenging, especially for complex queries. Summary TRC is a powerful non-procedural query language specifically designed for querying relational databases. It allows for expressive and portable queries that can retrieve data based on complex conditions. While TRC has certain advantages, optimizing its efficiency for large databases can be a challenge.","title":"Summary of Tutorial 4.2.pdf"},{"location":"Week%204/Tutorial%204.2/#summary-of-tutorial-42pdf","text":"Summary Tuple Relational Calculus (TRC) TRC is a non-procedural query language used to retrieve data from relational databases. It defines queries in a declarative manner, specifying the conditions that the resulting tuples must satisfy. The syntax of a TRC query is: {t | P(t)} where: t is the variable representing the resulting tuples. P(t) is the predicate, a logical expression that specifies the conditions that the tuples must satisfy to be included in the result set. Predicates Predicates can be constructed using the following logical operators: \u2227 (AND): Conjunction (true if both operands are true) \u2228 (OR): Disjunction (true if either operand is true) \u00ac (NOT): Negation (true if the operand is false) Quantifiers TRC also supports quantifiers, which express conditions that apply to all or some tuples in a relation: \u2203t \u2208 r (Q(t)) : There exists a tuple t in relation r such that predicate Q(t) is true. \u2200t \u2208 r (Q(t)) : For all tuples t in relation r , predicate Q(t) is true. Examples Example 1: Querying Students Table {t.Fname | Student(t) \u2227 t.age > 21} This query retrieves the first names ( Fname ) of students who are over 21 years old from a table named Student . Example 2: Querying Student and Course Tables {t | \u2203s \u2208 student \u2203c \u2208 course(s.courseId = c.courseId \u2227 c.cname = \u2018DBMS\u2019 \u2227t.name = s.name)} This query retrieves the names ( t.name ) of students who have taken a course named \u2018DBMS`. Example 3: Querying Flights, Aircraft, Certified, and Employees Tables {F.flno | F \u2208 Flights \u2227 \u2203A \u2208 Aircraft\u2203C \u2208 Certified\u2203E \u2208 Employees(A.cruisingrange > F.distance \u2227 A.aid =C.aid \u2227 E.salary > 100, 000 \u2227 E.eid = C.eid)} This query identifies the flight numbers ( F.flno ) of flights that can be piloted by every pilot whose salary is over $100,000. Applications of TRC TRC is used for querying relational databases in a variety of applications, including: Retrieving data from multiple tables Filtering data based on complex conditions Aggregating data (e.g., finding average salaries) Identifying relationships between data Modifying data (e.g., updating or deleting rows) Advantages of TRC Declarative: TRC queries express what data to retrieve without specifying how to retrieve it. Expressive: TRC allows for complex queries involving multiple tables and conditions. Portable: TRC queries can be used across different database management systems. Disadvantages of TRC Efficiency: TRC queries can be inefficient for large databases, especially when they involve complex joins. Complexity: Writing optimized TRC queries can be challenging, especially for complex queries. Summary TRC is a powerful non-procedural query language specifically designed for querying relational databases. It allows for expressive and portable queries that can retrieve data based on complex conditions. While TRC has certain advantages, optimizing its efficiency for large databases can be a challenge.","title":"Summary of Tutorial 4.2.pdf"},{"location":"Week%204/Tutorial%204.3/","text":"Summary of Tutorial 4.3.pdf Summary Tutorial: Translation of E-R Diagram into Relational Schema Introduction An Entity-Relationship (E-R) diagram is a graphical representation of an entity set, relationships between entity sets, and various constraints that the relationships may have. A relational schema is a formal representation of a database that is used to create and manage the data in a database management system. This tutorial provides a step-by-step guide on how to translate an E-R diagram into a relational schema. Strong Entity Set A strong entity set is an entity set that can exist independently of any other entity set. It has a unique identifier that is used to distinguish it from other entities in the set. Simple Attributes A simple attribute is an attribute that can be represented by a single value, such as a name, address, or phone number. Composite Key A composite key is a set of two or more attributes that uniquely identify an entity in an entity set. Multivalued Attribute A multivalued attribute is an attribute that can have multiple values for a single entity. Derived Attribute A derived attribute is an attribute that is calculated from other attributes in the entity set. Relationship: Cardinality Constraint A cardinality constraint defines the number of entities in one entity set that can be related to a single entity in another entity set. Common cardinality constraints include many-to-many, many-to-one, and one-to-one. Descriptive Attributes Descriptive attributes provide additional information about a relationship. They are not used to define the cardinality constraint. Participation Constraint A participation constraint specifies whether an entity in one entity set must participate in a relationship with an entity in another entity set. Weak Entity Set A weak entity set is an entity set that cannot exist independently of another entity set. It has a foreign key that references the primary key of the other entity set. Ternary Relationship A ternary relationship is a relationship between three entity sets. Aggregation Aggregation is a relationship that represents a \"has-a\" relationship between two entity sets. Specialization Specialization is a relationship that represents a subclass-superclass relationship between two entity sets. Steps for Translating an E-R Diagram into a Relational Schema Identify the entity sets and their attributes. Identify the relationships between the entity sets and their cardinality constraints. Identify any descriptive attributes or participation constraints. Create a table for each entity set. Add the attributes to the tables. Add foreign keys to represent relationships. Create tables to represent weak entity sets. Create tables to represent ternary relationships. Create tables to represent aggregation. Create tables to represent specialization. Conclusion Translating an E-R diagram into a relational schema is a critical step in database design. By following the steps outlined in this tutorial, you can ensure that your relational schema accurately represents the data and relationships in your E-R diagram.","title":"Summary of Tutorial 4.3.pdf"},{"location":"Week%204/Tutorial%204.3/#summary-of-tutorial-43pdf","text":"Summary Tutorial: Translation of E-R Diagram into Relational Schema Introduction An Entity-Relationship (E-R) diagram is a graphical representation of an entity set, relationships between entity sets, and various constraints that the relationships may have. A relational schema is a formal representation of a database that is used to create and manage the data in a database management system. This tutorial provides a step-by-step guide on how to translate an E-R diagram into a relational schema. Strong Entity Set A strong entity set is an entity set that can exist independently of any other entity set. It has a unique identifier that is used to distinguish it from other entities in the set. Simple Attributes A simple attribute is an attribute that can be represented by a single value, such as a name, address, or phone number. Composite Key A composite key is a set of two or more attributes that uniquely identify an entity in an entity set. Multivalued Attribute A multivalued attribute is an attribute that can have multiple values for a single entity. Derived Attribute A derived attribute is an attribute that is calculated from other attributes in the entity set. Relationship: Cardinality Constraint A cardinality constraint defines the number of entities in one entity set that can be related to a single entity in another entity set. Common cardinality constraints include many-to-many, many-to-one, and one-to-one. Descriptive Attributes Descriptive attributes provide additional information about a relationship. They are not used to define the cardinality constraint. Participation Constraint A participation constraint specifies whether an entity in one entity set must participate in a relationship with an entity in another entity set. Weak Entity Set A weak entity set is an entity set that cannot exist independently of another entity set. It has a foreign key that references the primary key of the other entity set. Ternary Relationship A ternary relationship is a relationship between three entity sets. Aggregation Aggregation is a relationship that represents a \"has-a\" relationship between two entity sets. Specialization Specialization is a relationship that represents a subclass-superclass relationship between two entity sets. Steps for Translating an E-R Diagram into a Relational Schema Identify the entity sets and their attributes. Identify the relationships between the entity sets and their cardinality constraints. Identify any descriptive attributes or participation constraints. Create a table for each entity set. Add the attributes to the tables. Add foreign keys to represent relationships. Create tables to represent weak entity sets. Create tables to represent ternary relationships. Create tables to represent aggregation. Create tables to represent specialization. Conclusion Translating an E-R diagram into a relational schema is a critical step in database design. By following the steps outlined in this tutorial, you can ensure that your relational schema accurately represents the data and relationships in your E-R diagram.","title":"Summary of Tutorial 4.3.pdf"},{"location":"Week%204/Tutorial%204.4/","text":"Summary of Tutorial 4.4.pdf Summary Entities and Attributes In an entity-relationship (E-R) diagram, an entity represents a real-world object or concept. Entities are composed of a set of attributes, which are specific characteristics or properties that describe the entity. An entity set is a collection of entities of the same type, sharing similar attributes. Relationships Relationships establish associations between entities. A relationship set is a collection of relationships of a particular type. Cardinality describes the number of entities involved in a relationship. Cardinality constraints define the maximum and minimum number of entities that can participate in a relationship. Mapping Constraints Mapping constraints specify the relationships between entities and specify the cardinality constraints for each relationship. The most common mapping constraints are: Many-to-many: Both entities can have multiple relationships with each other. One-to-one: Each entity can have only one relationship with the other entity. One-to-many: Each entity in one set can have multiple relationships with entities in the other set, but each entity in the other set can have only one relationship with an entity in the first set. Many-to-one: Each entity in one set can have multiple relationships with an entity in the other set, but each entity in the other set can have only one relationship with an entity in the first set. Participation Constraints Participation constraints specify whether an entity must participate in a relationship. Total participation indicates that every entity in the entity set must participate in at least one relationship. Partial participation indicates that not all entities in the entity set must participate in a relationship. E-R Diagram Symbols E-R diagrams use specific symbols to represent entities, attributes, relationships, and constraints: Entity: A rectangle Attribute: An oval inside an entity Relationship: A diamond connected to entities Cardinality: Numbers or symbols on relationship lines Participation: Lines connecting entities to relationships Steps to Draw an E-R Diagram To create an E-R diagram, follow these steps: Identify Entities : Determine the real-world objects or concepts involved and represent each as an entity. Add Attributes : List the relevant characteristics or properties for each entity. Identify Relationships : Determine the associations between entities and represent each as a relationship. Add Cardinality : Specify the maximum and minimum number of entities that can participate in each relationship. Case Study Scenario: The Prescriptions-R-X pharmacy chain has requested a database design to manage its operations. Requirements are as follows: Patients: Identified by SSN Attribute: Name, Address, Age Doctors: Identified by SSN Attribute: Name, Specialty, Years of Experience Relationship: Every patient has a primary physician (one-to-many) Every doctor has at least one patient (one-to-many) Prescriptions: Date, Quantity Doctor prescribes one or more drugs for several patients (many-to-many) Patient obtains prescriptions from several doctors (many-to-many) Pharmaceutical Companies: Attribute: Name, Phone Number Drugs: Attribute: Trade Name, Formula Each drug sold by a specific pharmaceutical company (one-to-many) Pharmacies: Attribute: Name, Address, Phone Number Contracts: Relationship between pharmaceutical companies and pharmacies (many-to-many) Attribute: Start Date, End Date, Contract Text Contract Supervisors: Appointed by pharmacies for each contract (one-to-one) Drug Prices: Relationship between drugs and pharmacies (many-to-many) Attribute: Price E-R Diagram Using the provided requirements, an E-R diagram can be constructed as follows: Entities: Patient, Doctor, Prescription, Pharmaceutical Company, Drug, Pharmacy, Contract, Contract Supervisor Attributes: As per the requirements Relationships: Patient - Prescribes - Prescription (many-to-many) Doctor - Prescribes - Prescription (many-to-many) Pharmaceutical Company - Sells - Drug (one-to-many) Pharmacy - Contracts - Contract (many-to-many) Contract - Supervised By - Contract Supervisor (one-to-one) Pharmacy - Sells - Drug (many-to-many) Cardinality and Participation: Patient - Doctor (one-to-many, mandatory) Doctor - Patient (one-to-many, mandatory) Prescription - Doctor (many-to-many, optional) Prescription - Patient (many-to-many, optional) Drug - Pharmaceutical Company (one-to-many, mandatory) Contract - Pharmaceutical Company (many-to-many, optional) Contract - Pharmacy (many-to-many, optional) Contract - Contract Supervisor (one-to-one, mandatory) Drug - Pharmacy (many-to-many, optional)","title":"Summary of Tutorial 4.4.pdf"},{"location":"Week%204/Tutorial%204.4/#summary-of-tutorial-44pdf","text":"Summary Entities and Attributes In an entity-relationship (E-R) diagram, an entity represents a real-world object or concept. Entities are composed of a set of attributes, which are specific characteristics or properties that describe the entity. An entity set is a collection of entities of the same type, sharing similar attributes. Relationships Relationships establish associations between entities. A relationship set is a collection of relationships of a particular type. Cardinality describes the number of entities involved in a relationship. Cardinality constraints define the maximum and minimum number of entities that can participate in a relationship. Mapping Constraints Mapping constraints specify the relationships between entities and specify the cardinality constraints for each relationship. The most common mapping constraints are: Many-to-many: Both entities can have multiple relationships with each other. One-to-one: Each entity can have only one relationship with the other entity. One-to-many: Each entity in one set can have multiple relationships with entities in the other set, but each entity in the other set can have only one relationship with an entity in the first set. Many-to-one: Each entity in one set can have multiple relationships with an entity in the other set, but each entity in the other set can have only one relationship with an entity in the first set. Participation Constraints Participation constraints specify whether an entity must participate in a relationship. Total participation indicates that every entity in the entity set must participate in at least one relationship. Partial participation indicates that not all entities in the entity set must participate in a relationship. E-R Diagram Symbols E-R diagrams use specific symbols to represent entities, attributes, relationships, and constraints: Entity: A rectangle Attribute: An oval inside an entity Relationship: A diamond connected to entities Cardinality: Numbers or symbols on relationship lines Participation: Lines connecting entities to relationships Steps to Draw an E-R Diagram To create an E-R diagram, follow these steps: Identify Entities : Determine the real-world objects or concepts involved and represent each as an entity. Add Attributes : List the relevant characteristics or properties for each entity. Identify Relationships : Determine the associations between entities and represent each as a relationship. Add Cardinality : Specify the maximum and minimum number of entities that can participate in each relationship. Case Study Scenario: The Prescriptions-R-X pharmacy chain has requested a database design to manage its operations. Requirements are as follows: Patients: Identified by SSN Attribute: Name, Address, Age Doctors: Identified by SSN Attribute: Name, Specialty, Years of Experience Relationship: Every patient has a primary physician (one-to-many) Every doctor has at least one patient (one-to-many) Prescriptions: Date, Quantity Doctor prescribes one or more drugs for several patients (many-to-many) Patient obtains prescriptions from several doctors (many-to-many) Pharmaceutical Companies: Attribute: Name, Phone Number Drugs: Attribute: Trade Name, Formula Each drug sold by a specific pharmaceutical company (one-to-many) Pharmacies: Attribute: Name, Address, Phone Number Contracts: Relationship between pharmaceutical companies and pharmacies (many-to-many) Attribute: Start Date, End Date, Contract Text Contract Supervisors: Appointed by pharmacies for each contract (one-to-one) Drug Prices: Relationship between drugs and pharmacies (many-to-many) Attribute: Price E-R Diagram Using the provided requirements, an E-R diagram can be constructed as follows: Entities: Patient, Doctor, Prescription, Pharmaceutical Company, Drug, Pharmacy, Contract, Contract Supervisor Attributes: As per the requirements Relationships: Patient - Prescribes - Prescription (many-to-many) Doctor - Prescribes - Prescription (many-to-many) Pharmaceutical Company - Sells - Drug (one-to-many) Pharmacy - Contracts - Contract (many-to-many) Contract - Supervised By - Contract Supervisor (one-to-one) Pharmacy - Sells - Drug (many-to-many) Cardinality and Participation: Patient - Doctor (one-to-many, mandatory) Doctor - Patient (one-to-many, mandatory) Prescription - Doctor (many-to-many, optional) Prescription - Patient (many-to-many, optional) Drug - Pharmaceutical Company (one-to-many, mandatory) Contract - Pharmaceutical Company (many-to-many, optional) Contract - Pharmacy (many-to-many, optional) Contract - Contract Supervisor (one-to-one, mandatory) Drug - Pharmacy (many-to-many, optional)","title":"Summary of Tutorial 4.4.pdf"},{"location":"Week%204/er_model/","text":"description of the figures: Symbol/Shape Name Figure Description Description Rectangle Entity A rectangle with the entity name inside it. Represents an entity, which is a distinct object in the system with a unique identity. Ellipse Attribute An ellipse with the attribute name inside it. Represents an attribute, which is a property or characteristic of an entity. Double Ellipse Multivalued Attribute An ellipse with a double border and the attribute name inside it. Represents an attribute that can have multiple values for a single entity. Dashed Ellipse Derived Attribute An ellipse with a dashed border and the attribute name inside it. Represents an attribute whose value can be derived from other attributes. Diamond Relationship A diamond shape with the relationship name inside it. Represents a relationship, which is an association among two or more entities. Double Rectangle Weak Entity A rectangle with a double border and the entity name inside it. Represents a weak entity, which depends on another entity (its identifying owner) for its existence. Double Diamond Identifying Relationship A diamond shape with a double border and the relationship name inside it. Represents a relationship that links a weak entity to its identifying owner. Line Connector A simple line connecting shapes. Connects entities to relationships and attributes to entities or relationships. Double Line Total Participation A line connecting shapes with a double border. Indicates that every instance of the entity must participate in at least one instance of the relationship. Dashed Line Partial Participation A line with a dashed border connecting shapes. Indicates that not every instance of the entity must participate in the relationship. Primary Key (PK) Underlined Attribute An attribute name with an underline. Represents an attribute that uniquely identifies each entity instance. Foreign Key (FK) Dashed Underlined Attribute An attribute name with a dashed underline. Represents an attribute that is a primary key in another table, establishing a relationship between entities.","title":"Er model"},{"location":"Week%204/week4/","text":"[ ] lec class [ ] PPA [ ] GRPA [ ] GA [ ] instructor section 1 [ ] instructor section 2 [ ] TA session 1 [ ] TA session 2 [ ] Text book","title":"Week4"},{"location":"Week%205/Lecture%205.1%20-%20Relational%20Database%20Design1/","text":"Relational Database Design- 1 + 1NF Summary This module focuses on principles of relational database design. It emphasizes: Features of Good Design: Reflects real-world structure Accommodates future data additions Avoids redundancy Provides efficient data access Maintains data integrity Redundancy and Anomaly: Anomaly: inconsistencies that can arise due to data changes in a database with insertion, deletion, and update These problems occur in poorly planned, un-normalised databases where all the data is stored in one table (a flat-file database) Redundancy (duplicate data) leads to anomalies: Insertion: Can't add data if unknown data is required Deletion: Losing unrelated information when deleting records Update: Inaccurate changes due to multiple occurrences of data Decomposition: Decomposing relations into smaller ones removes redundancy and minimizes dependencies among attributes. Good decomposition ensures data preservation and integrity. functional dependency : dept name \u2192 building, budget In inst_dept, because dept_name is not a candidate key, the building and budget of a department may have to be repeated. \u25e6 This indicates the need to decompose inst dept Lossy Decomposition Lossless Join Decomposition is a decomposition of a relation R into relations R1 , R2 such that if we perform natural join of two smaller relations it will return the original relation Atomic Domains and First Normal Form (1NF): Atomic domains consist of indivisible elements. 1NF requires relations with atomic domains and each attribute holding a single value. Non-atomic values complicate storage and encourage redundancy. Atomic Domains An atomic domain refers to the indivisibility of data within a domain. In the context of databases, it means that the value in a particular field is indivisible and represents the smallest unit of data. Each attribute in a table should contain atomic (indivisible) values. Example of Atomic Values : 123 Main St , John Doe , 01/01/2020 Example of Non-Atomic Values : 123 Main St, Apt 4 (multiple pieces of information in one field) John and Jane Doe (multiple names in one field) First Normal Form (1NF) A relation (table) is said to be in the First Normal Form (1NF) if it satisfies the following conditions: Atomicity : All the values in the database are atomic (indivisible). Uniqueness of Rows : Each row in the table must be unique, meaning no two rows can be identical. Uniqueness of Column Names : Each column should have a unique name. No Repeating Groups : Each table should contain only one value per cell (intersection of a row and a column), and columns should not contain sets or lists of values. Examples of 1NF Non-1NF Table: StudentID Name Courses 1 John Doe Math, Science 2 Jane Smith English, History, Math In the above table: The Courses column contains multiple values, which violates the atomicity rule. 1NF Table: StudentID Name Course 1 John Doe Math 1 John Doe Science 2 Jane Smith English 2 Jane Smith History 2 Jane Smith Math In this 1NF table: Each cell contains only one value, adhering to the atomicity requirement. Each row is unique, and there are no repeating groups within any cell. Achieving 1NF To transform a table into 1NF: Remove Repeating Groups : Ensure that each column contains only a single value. Create Separate Tables for Multivalued Attributes : If necessary, split the multivalued attributes into separate rows or tables. Ensure Primary Keys : Define primary keys to uniquely identify each row in the table. By adhering to these principles, a database can be designed to comply with the First Normal Form, thus ensuring data integrity and facilitating easier querying and maintenance.","title":"Relational Database Design- 1 + 1NF"},{"location":"Week%205/Lecture%205.1%20-%20Relational%20Database%20Design1/#relational-database-design-1-1nf","text":"Summary This module focuses on principles of relational database design. It emphasizes: Features of Good Design: Reflects real-world structure Accommodates future data additions Avoids redundancy Provides efficient data access Maintains data integrity Redundancy and Anomaly: Anomaly: inconsistencies that can arise due to data changes in a database with insertion, deletion, and update These problems occur in poorly planned, un-normalised databases where all the data is stored in one table (a flat-file database) Redundancy (duplicate data) leads to anomalies: Insertion: Can't add data if unknown data is required Deletion: Losing unrelated information when deleting records Update: Inaccurate changes due to multiple occurrences of data Decomposition: Decomposing relations into smaller ones removes redundancy and minimizes dependencies among attributes. Good decomposition ensures data preservation and integrity. functional dependency : dept name \u2192 building, budget In inst_dept, because dept_name is not a candidate key, the building and budget of a department may have to be repeated. \u25e6 This indicates the need to decompose inst dept Lossy Decomposition Lossless Join Decomposition is a decomposition of a relation R into relations R1 , R2 such that if we perform natural join of two smaller relations it will return the original relation Atomic Domains and First Normal Form (1NF): Atomic domains consist of indivisible elements. 1NF requires relations with atomic domains and each attribute holding a single value. Non-atomic values complicate storage and encourage redundancy. Atomic Domains An atomic domain refers to the indivisibility of data within a domain. In the context of databases, it means that the value in a particular field is indivisible and represents the smallest unit of data. Each attribute in a table should contain atomic (indivisible) values. Example of Atomic Values : 123 Main St , John Doe , 01/01/2020 Example of Non-Atomic Values : 123 Main St, Apt 4 (multiple pieces of information in one field) John and Jane Doe (multiple names in one field)","title":"Relational Database Design- 1 + 1NF"},{"location":"Week%205/Lecture%205.1%20-%20Relational%20Database%20Design1/#first-normal-form-1nf","text":"A relation (table) is said to be in the First Normal Form (1NF) if it satisfies the following conditions: Atomicity : All the values in the database are atomic (indivisible). Uniqueness of Rows : Each row in the table must be unique, meaning no two rows can be identical. Uniqueness of Column Names : Each column should have a unique name. No Repeating Groups : Each table should contain only one value per cell (intersection of a row and a column), and columns should not contain sets or lists of values.","title":"First Normal Form (1NF)"},{"location":"Week%205/Lecture%205.1%20-%20Relational%20Database%20Design1/#examples-of-1nf","text":"Non-1NF Table: StudentID Name Courses 1 John Doe Math, Science 2 Jane Smith English, History, Math In the above table: The Courses column contains multiple values, which violates the atomicity rule. 1NF Table: StudentID Name Course 1 John Doe Math 1 John Doe Science 2 Jane Smith English 2 Jane Smith History 2 Jane Smith Math In this 1NF table: Each cell contains only one value, adhering to the atomicity requirement. Each row is unique, and there are no repeating groups within any cell.","title":"Examples of 1NF"},{"location":"Week%205/Lecture%205.1%20-%20Relational%20Database%20Design1/#achieving-1nf","text":"To transform a table into 1NF: Remove Repeating Groups : Ensure that each column contains only a single value. Create Separate Tables for Multivalued Attributes : If necessary, split the multivalued attributes into separate rows or tables. Ensure Primary Keys : Define primary keys to uniquely identify each row in the table. By adhering to these principles, a database can be designed to comply with the First Normal Form, thus ensuring data integrity and facilitating easier querying and maintenance.","title":"Achieving 1NF"},{"location":"Week%205/Lecture%205.2%20-%20Relational%20Database%20Design2/","text":"Relational Database Design-2 - Functional Dependencies Summary Objectives To introduce functional dependencies, a fundamental concept for designing relational databases. Outline Functional Dependencies Armstrong's Axioms Closure of Functional Dependencies Functional Dependencies Constraints on the set of legal relations. Require that the value for a certain set of attributes determines uniquely the value for another set of attributes. Formal definition: Let R be a relation schema, \u03b1 \u2286 R, and \u03b2 \u2286 R. The functional dependency (FD) \u03b1 \u2192 \u03b2 holds on R if and only if for any legal relations r(R), whenever any two tuples t1 and t2 of r agree on the attributes \u03b1, they also agree on the attributes \u03b2. primary key Armstrong's Axioms A set of rules that allow us to infer new FDs from a given set of FDs. Reflexivity: if \u03b2 \u2286 \u03b1, then \u03b1 \u2192 \u03b2 Augmentation: if \u03b1 \u2192 \u03b2, then \u03b3\u03b1 \u2192 \u03b3\u03b2 Transitivity: if \u03b1 \u2192 \u03b2 and \u03b2 \u2192 \u03b3, then \u03b1 \u2192 \u03b3 Closure of Functional Dependencies The set of all FDs that can be logically inferred from a given set of FDs F using Armstrong's Axioms. Denoted by F+. Applications of Functional Dependencies Testing relations to determine if they satisfy a given set of FDs. Specifying constraints on the set of legal relations. Expressing constraints that cannot be expressed using superkeys. Properties of Armstrong's Axioms Soundness: Generate only FDs that actually hold. Completeness: Eventually generate all FDs that hold. Properties of Functional Dependencies A functional dependency is trivial if it is satisfied by all instances of a relation. Example: ID, name \u2192 ID; name \u2192 name In general, \u03b1 \u2192 \u03b2 is trivial if \u03b2 \u2286 \u03b1. Examples of Functional Dependencies studentID \u2192 semester studentID, lecture \u2192 TA {studentID, lecture} \u2192 {TA, semester} employeeID \u2192 employeeName employeeID \u2192 departmentID departmentID \u2192 departmentName Conclusion Functional dependencies are a powerful tool for designing relational databases, allowing us to express data constraints and enforce referential integrity. Armstrong's Axioms provide a formal framework for manipulating and reasoning about FDs, while the closure of FDs helps us identify all possible constraints that can be derived from a given set of FDs.","title":"Relational Database Design-2 - Functional Dependencies"},{"location":"Week%205/Lecture%205.2%20-%20Relational%20Database%20Design2/#relational-database-design-2-functional-dependencies","text":"Summary Objectives To introduce functional dependencies, a fundamental concept for designing relational databases. Outline Functional Dependencies Armstrong's Axioms Closure of Functional Dependencies Functional Dependencies Constraints on the set of legal relations. Require that the value for a certain set of attributes determines uniquely the value for another set of attributes. Formal definition: Let R be a relation schema, \u03b1 \u2286 R, and \u03b2 \u2286 R. The functional dependency (FD) \u03b1 \u2192 \u03b2 holds on R if and only if for any legal relations r(R), whenever any two tuples t1 and t2 of r agree on the attributes \u03b1, they also agree on the attributes \u03b2. primary key Armstrong's Axioms A set of rules that allow us to infer new FDs from a given set of FDs. Reflexivity: if \u03b2 \u2286 \u03b1, then \u03b1 \u2192 \u03b2 Augmentation: if \u03b1 \u2192 \u03b2, then \u03b3\u03b1 \u2192 \u03b3\u03b2 Transitivity: if \u03b1 \u2192 \u03b2 and \u03b2 \u2192 \u03b3, then \u03b1 \u2192 \u03b3 Closure of Functional Dependencies The set of all FDs that can be logically inferred from a given set of FDs F using Armstrong's Axioms. Denoted by F+. Applications of Functional Dependencies Testing relations to determine if they satisfy a given set of FDs. Specifying constraints on the set of legal relations. Expressing constraints that cannot be expressed using superkeys. Properties of Armstrong's Axioms Soundness: Generate only FDs that actually hold. Completeness: Eventually generate all FDs that hold. Properties of Functional Dependencies A functional dependency is trivial if it is satisfied by all instances of a relation. Example: ID, name \u2192 ID; name \u2192 name In general, \u03b1 \u2192 \u03b2 is trivial if \u03b2 \u2286 \u03b1. Examples of Functional Dependencies studentID \u2192 semester studentID, lecture \u2192 TA {studentID, lecture} \u2192 {TA, semester} employeeID \u2192 employeeName employeeID \u2192 departmentID departmentID \u2192 departmentName Conclusion Functional dependencies are a powerful tool for designing relational databases, allowing us to express data constraints and enforce referential integrity. Armstrong's Axioms provide a formal framework for manipulating and reasoning about FDs, while the closure of FDs helps us identify all possible constraints that can be derived from a given set of FDs.","title":"Relational Database Design-2 - Functional Dependencies"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/","text":"Summary of Lecture 5.3 - Relational Database Design3.pdf Summary Functional Dependency Theory Armstrong's Axioms Armstrong's Axioms provide a set of rules for inferring new functional dependencies from existing ones. These axioms are: Reflexivity: If \u03b2 \u2286 \u03b1, then \u03b1 \u2192 \u03b2 Augmentation: If \u03b1 \u2192 \u03b2, then \u03b3\u03b1 \u2192 \u03b3\u03b2 Transitivity: If \u03b1 \u2192 \u03b2 and \u03b2 \u2192 \u03b3, then \u03b1 \u2192 \u03b3 These axioms allow us to generate new FDs by applying them repeatedly to existing FDs. The result is the closure of the set of FDs, denoted as F+. Closure of FDs The closure of a set of FDs F is the set of all FDs that can be logically implied by F. F+ is computed by repeatedly applying Armstrong's axioms to F until no new FDs can be generated. Closure of Attributes The closure of an attribute set \u03b1 under F, denoted as \u03b1+, is the set of all attributes that are functionally determined by \u03b1 under F. To compute \u03b1+, we start with \u03b1 and repeatedly apply the following rule until no new attributes can be added: for each \u03b2 \u2192 \u03b3 in F do if \u03b2 \u2286 result then result \u2190 result \u222a \u03b3 Decomposition Using Functional Dependencies Decomposition is the process of dividing a relation into smaller relations that are easier to maintain and query. To decompose a relation using FDs, we must ensure that: Lossless Join: The decomposition is lossless if we can reconstruct the original relation by joining the decomposed relations. Dependency Preservation: The decomposition is dependency preserving if all FDs hold on the decomposed relations. 2NF Second Normal Form Realation R is in 2NF R is 1NF means not multivariable dependency R contains no partial arttibutes Third Normal Form (3NF) A relation schema R is in 3NF if for every FD \u03b1 \u2192 \u03b2 \u2208 F+, either: \u03b1 \u2192 \u03b2 is trivial (i.e., \u03b2 \u2286 \u03b1) \u03b1 is a superkey for R Each attribute A in \u03b2 \u2212 \u03b1 is contained in a candidate key for R no prime attributes shouldnt determine no prime attribute means all funtional dependecy will have the PRIME attribute in LHS or if LHS is no prime arrtibute then RHS has to be PRIME attribute Boyce-Codd Normal Form (BCNF) A relation schema R is in BCNF if for every FD \u03b1 \u2192 \u03b2 \u2208 F+, either: \u03b1 \u2192 \u03b2 is trivial (i.e., \u03b2 \u2286 \u03b1) \u03b1 is a superkey for R Normalization Normalization is the process of converting a relation schema into a \"good\" form, such as BCNF or 3NF. The goals of normalization are to: Eliminate data redundancy Ensure data integrity Improve query performance Problems with Decomposition Decomposition may introduce some problems, such as: Lossiness: It may be impossible to reconstruct the original relation from the decomposed relations. Dependency Checking: Checking dependencies may require joins, which can be expensive. Query Performance: Some queries may become more expensive after decomposition. How Good is BCNF? BCNF is a strong normal form, but it is not always sufficient to eliminate all anomalies. For example, inst info can be decomposed into inst child and inst phone, but this decomposition does not eliminate insertion anomalies. Module Summary In this module, we introduced the theory of functional dependencies and discussed issues in \"good\" design in the context of functional dependencies. We also discussed different normal forms, such as BCNF and 3NF, and the goals of normalization.","title":"Summary of Lecture 5.3 - Relational Database Design3.pdf"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/#summary-of-lecture-53-relational-database-design3pdf","text":"Summary","title":"Summary of Lecture 5.3 - Relational Database Design3.pdf"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/#functional-dependency-theory","text":"","title":"Functional Dependency Theory"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/#armstrongs-axioms","text":"Armstrong's Axioms provide a set of rules for inferring new functional dependencies from existing ones. These axioms are: Reflexivity: If \u03b2 \u2286 \u03b1, then \u03b1 \u2192 \u03b2 Augmentation: If \u03b1 \u2192 \u03b2, then \u03b3\u03b1 \u2192 \u03b3\u03b2 Transitivity: If \u03b1 \u2192 \u03b2 and \u03b2 \u2192 \u03b3, then \u03b1 \u2192 \u03b3 These axioms allow us to generate new FDs by applying them repeatedly to existing FDs. The result is the closure of the set of FDs, denoted as F+.","title":"Armstrong's Axioms"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/#closure-of-fds","text":"The closure of a set of FDs F is the set of all FDs that can be logically implied by F. F+ is computed by repeatedly applying Armstrong's axioms to F until no new FDs can be generated.","title":"Closure of FDs"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/#closure-of-attributes","text":"The closure of an attribute set \u03b1 under F, denoted as \u03b1+, is the set of all attributes that are functionally determined by \u03b1 under F. To compute \u03b1+, we start with \u03b1 and repeatedly apply the following rule until no new attributes can be added: for each \u03b2 \u2192 \u03b3 in F do if \u03b2 \u2286 result then result \u2190 result \u222a \u03b3","title":"Closure of Attributes"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/#decomposition-using-functional-dependencies","text":"Decomposition is the process of dividing a relation into smaller relations that are easier to maintain and query. To decompose a relation using FDs, we must ensure that: Lossless Join: The decomposition is lossless if we can reconstruct the original relation by joining the decomposed relations. Dependency Preservation: The decomposition is dependency preserving if all FDs hold on the decomposed relations.","title":"Decomposition Using Functional Dependencies"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/#2nf-second-normal-form","text":"Realation R is in 2NF R is 1NF means not multivariable dependency R contains no partial arttibutes","title":"2NF Second Normal Form"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/#third-normal-form-3nf","text":"A relation schema R is in 3NF if for every FD \u03b1 \u2192 \u03b2 \u2208 F+, either: \u03b1 \u2192 \u03b2 is trivial (i.e., \u03b2 \u2286 \u03b1) \u03b1 is a superkey for R Each attribute A in \u03b2 \u2212 \u03b1 is contained in a candidate key for R no prime attributes shouldnt determine no prime attribute means all funtional dependecy will have the PRIME attribute in LHS or if LHS is no prime arrtibute then RHS has to be PRIME attribute","title":"Third Normal Form (3NF)"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/#boyce-codd-normal-form-bcnf","text":"A relation schema R is in BCNF if for every FD \u03b1 \u2192 \u03b2 \u2208 F+, either: \u03b1 \u2192 \u03b2 is trivial (i.e., \u03b2 \u2286 \u03b1) \u03b1 is a superkey for R","title":"Boyce-Codd Normal Form (BCNF)"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/#normalization","text":"Normalization is the process of converting a relation schema into a \"good\" form, such as BCNF or 3NF. The goals of normalization are to: Eliminate data redundancy Ensure data integrity Improve query performance","title":"Normalization"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/#problems-with-decomposition","text":"Decomposition may introduce some problems, such as: Lossiness: It may be impossible to reconstruct the original relation from the decomposed relations. Dependency Checking: Checking dependencies may require joins, which can be expensive. Query Performance: Some queries may become more expensive after decomposition.","title":"Problems with Decomposition"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/#how-good-is-bcnf","text":"BCNF is a strong normal form, but it is not always sufficient to eliminate all anomalies. For example, inst info can be decomposed into inst child and inst phone, but this decomposition does not eliminate insertion anomalies.","title":"How Good is BCNF?"},{"location":"Week%205/Lecture%205.3%20-%20Relational%20Database%20Design3/#module-summary","text":"In this module, we introduced the theory of functional dependencies and discussed issues in \"good\" design in the context of functional dependencies. We also discussed different normal forms, such as BCNF and 3NF, and the goals of normalization.","title":"Module Summary"},{"location":"Week%205/Lecture%205.4%20-%20Relational%20Database%20Design4/","text":"Summary of Lecture 5.4 - Relational Database Design4.pdf Summary Module 24: Relational Database Design/4 Objectives Learn Algorithms for Properties of Functional Dependencies Outline Algorithms for Functional Dependencies Attribute Set Closure Extraneous Attributes Equivalence of FD Sets Canonical Cover of FDs Practice Problems Algorithms for Functional Dependencies Functional dependencies (FDs) are used to represent relationships between attributes in a database table. The following algorithms can be used to test for various properties of FDs: Attribute Set Closure : Calculates the closure of an attribute set under a set of FDs, which is the set of all attributes that are functionally dependent on the given attribute set. Extraneous Attributes : Determines if an attribute in a functional dependency is extraneous, meaning it can be removed without affecting the set of FDs. Equivalence of FD Sets : Checks if two sets of FDs are equivalent, meaning they imply the same relationships between attributes. Attribute Set Closure To calculate the attribute set closure, we start with a set of attributes and repeatedly apply the following rules: If an attribute A is in the set, then add all attributes that are functionally dependent on A. If an attribute B is in the set and B \u2192 C is a functional dependency, then add C. Example: Given F = {A \u2192 B, A \u2192 C, CG \u2192 H, CG \u2192 I, B \u2192 H} (AG)+ (AG) = AG (AG) = ABCG (A \u2192 C and A \u2192 B) (AG) = ABCGH (CG \u2192 H and CG \u2286 AGBC) (AG) = ABCGHI (CG \u2192 I and CG \u2286 AGBCH) Extraneous Attributes An attribute A is extraneous in a functional dependency \u03b1 \u2192 \u03b2 if: A \u2208 \u03b1 and F logically implies (F \u2212 {\u03b1 \u2192 \u03b2}) \u222a {(\u03b1 \u2212 A) \u2192 \u03b2} A \u2208 \u03b2 and the set of FDs (F \u2212 {\u03b1 \u2192 \u03b2}) \u222a {\u03b1 \u2192 (\u03b2 \u2212 A)} logically implies F Example: Given F = {A \u2192 C, AB \u2192 C} B is extraneous in AB \u2192 C because {A \u2192 C, AB \u2192 C} logically implies A \u2192 C. A is extraneous in AB \u2192 C because A \u2192 C can be inferred from {AB \u2192 C} and {A \u2192 B}. Equivalence of FD Sets Two sets of functional dependencies F and G are equivalent if F+ = G+. That is: (F+ = G+) \u21d4 (F+ \u21d2 G and G+ \u21d2 F) Equivalence means that every functional dependency in F can be inferred from G, and every functional dependency in G can be inferred from F. Canonical Cover A canonical cover for a set of FDs is a set of dependencies Fc such that: F+ = Fc+ No functional dependency in Fc contains an extraneous attribute. Each left side of functional dependency in Fc is unique. Intuitively, a canonical cover is a minimal set of FDs that is logically equivalent to the original set of FDs. Practice Problems Find if the given functional dependency is implied from the set of Functional Dependencies: a) A \u2192 BC, CD \u2192 E, E \u2192 C, D \u2192 AEH, ABH \u2192 BD, DH \u2192 BC i) Check: BCD \u2192 H ii) Check: AED \u2192 C Find Super Key using Functional Dependencies: a) Relational Schema R(ABCDE). Functional dependencies: AB \u2192 C, DE \u2192 B, CD \u2192 E Find Candidate Key using Functional Dependencies: a) Relational Schema R(ABCDE). Functional dependencies: AB \u2192 C, DE \u2192 B, CD \u2192 E Find Prime and Non Prime Attributes using Functional Dependencies: a) R(ABCDEF) having FDs {AB \u2192 C, C \u2192 D, D \u2192 E, F \u2192 B, E \u2192 F} Check the Equivalence of a Pair of Sets of Functional Dependencies: a) Consider the two sets F and G with their FDs as below : i) F : A \u2192 C, AC \u2192 D, E \u2192 AD, E \u2192 H ii) G : A \u2192 CD, E \u2192 AH Find the Minimal Cover or Irreducible Sets or Canonical Cover of a Set of Functional Dependencies: a) AB \u2192 CD, BC \u2192 D b) ABCD \u2192 E, E \u2192 D, AC \u2192 D, A \u2192 B","title":"Summary of Lecture 5.4 - Relational Database Design4.pdf"},{"location":"Week%205/Lecture%205.4%20-%20Relational%20Database%20Design4/#summary-of-lecture-54-relational-database-design4pdf","text":"Summary Module 24: Relational Database Design/4 Objectives Learn Algorithms for Properties of Functional Dependencies Outline Algorithms for Functional Dependencies Attribute Set Closure Extraneous Attributes Equivalence of FD Sets Canonical Cover of FDs Practice Problems Algorithms for Functional Dependencies Functional dependencies (FDs) are used to represent relationships between attributes in a database table. The following algorithms can be used to test for various properties of FDs: Attribute Set Closure : Calculates the closure of an attribute set under a set of FDs, which is the set of all attributes that are functionally dependent on the given attribute set. Extraneous Attributes : Determines if an attribute in a functional dependency is extraneous, meaning it can be removed without affecting the set of FDs. Equivalence of FD Sets : Checks if two sets of FDs are equivalent, meaning they imply the same relationships between attributes. Attribute Set Closure To calculate the attribute set closure, we start with a set of attributes and repeatedly apply the following rules: If an attribute A is in the set, then add all attributes that are functionally dependent on A. If an attribute B is in the set and B \u2192 C is a functional dependency, then add C. Example: Given F = {A \u2192 B, A \u2192 C, CG \u2192 H, CG \u2192 I, B \u2192 H} (AG)+ (AG) = AG (AG) = ABCG (A \u2192 C and A \u2192 B) (AG) = ABCGH (CG \u2192 H and CG \u2286 AGBC) (AG) = ABCGHI (CG \u2192 I and CG \u2286 AGBCH) Extraneous Attributes An attribute A is extraneous in a functional dependency \u03b1 \u2192 \u03b2 if: A \u2208 \u03b1 and F logically implies (F \u2212 {\u03b1 \u2192 \u03b2}) \u222a {(\u03b1 \u2212 A) \u2192 \u03b2} A \u2208 \u03b2 and the set of FDs (F \u2212 {\u03b1 \u2192 \u03b2}) \u222a {\u03b1 \u2192 (\u03b2 \u2212 A)} logically implies F Example: Given F = {A \u2192 C, AB \u2192 C} B is extraneous in AB \u2192 C because {A \u2192 C, AB \u2192 C} logically implies A \u2192 C. A is extraneous in AB \u2192 C because A \u2192 C can be inferred from {AB \u2192 C} and {A \u2192 B}. Equivalence of FD Sets Two sets of functional dependencies F and G are equivalent if F+ = G+. That is: (F+ = G+) \u21d4 (F+ \u21d2 G and G+ \u21d2 F) Equivalence means that every functional dependency in F can be inferred from G, and every functional dependency in G can be inferred from F. Canonical Cover A canonical cover for a set of FDs is a set of dependencies Fc such that: F+ = Fc+ No functional dependency in Fc contains an extraneous attribute. Each left side of functional dependency in Fc is unique. Intuitively, a canonical cover is a minimal set of FDs that is logically equivalent to the original set of FDs. Practice Problems Find if the given functional dependency is implied from the set of Functional Dependencies: a) A \u2192 BC, CD \u2192 E, E \u2192 C, D \u2192 AEH, ABH \u2192 BD, DH \u2192 BC i) Check: BCD \u2192 H ii) Check: AED \u2192 C Find Super Key using Functional Dependencies: a) Relational Schema R(ABCDE). Functional dependencies: AB \u2192 C, DE \u2192 B, CD \u2192 E Find Candidate Key using Functional Dependencies: a) Relational Schema R(ABCDE). Functional dependencies: AB \u2192 C, DE \u2192 B, CD \u2192 E Find Prime and Non Prime Attributes using Functional Dependencies: a) R(ABCDEF) having FDs {AB \u2192 C, C \u2192 D, D \u2192 E, F \u2192 B, E \u2192 F} Check the Equivalence of a Pair of Sets of Functional Dependencies: a) Consider the two sets F and G with their FDs as below : i) F : A \u2192 C, AC \u2192 D, E \u2192 AD, E \u2192 H ii) G : A \u2192 CD, E \u2192 AH Find the Minimal Cover or Irreducible Sets or Canonical Cover of a Set of Functional Dependencies: a) AB \u2192 CD, BC \u2192 D b) ABCD \u2192 E, E \u2192 D, AC \u2192 D, A \u2192 B","title":"Summary of Lecture 5.4 - Relational Database Design4.pdf"},{"location":"Week%205/Lecture%205.5%20-%20Relational%20Database%20Design5/","text":"Summary of Lecture 5.5 - Relational Database Design5.pdf Summary Lossless Join Decomposition Definition: A decomposition of a relation R into two relations R1 and R2 is lossless join if the natural join of R1 and R2 is equal to R. Conditions for Lossless Join Decomposition: R1 \u222a R2 = R R1 \u2229 R2 \u2260 \u2205 R1 \u2229 R2 \u2192 R1 or R1 \u2229 R2 \u2192 R2 Dependency Preservation Definition: A decomposition of a relation R into two relations R1 and R2 preserves dependencies if every dependency in F (the set of functional dependencies on R) also holds in (F1 \u222a F2), where F1 and F2 are the sets of functional dependencies on R1 and R2, respectively. Conditions for Dependency Preservation: (F1 \u222a F2 \u222a ... \u222a Fn)\u207a = F\u207a If (F1 \u222a F2 \u222a ... \u222a Fn)\u207a \u2282 F\u207a, then the decomposition is not dependency preserving. If (F1 \u222a F2 \u222a ... \u222a Fn)\u207a \u2283 F\u207a, then this is not possible. Testing for Lossless Join Decomposition To determine whether a decomposition is lossless, check the following conditions: R1 \u222a R2 = R R1 \u2229 R2 \u2260 \u2205 R1 \u2229 R2 \u2192 R1 or R1 \u2229 R2 \u2192 R2 Testing for Dependency Preservation Using Attribute Closure To test for dependency preservation, follow these steps: Compute F\u207a, F1\u207a, F2\u207a, and so on. If (F1\u207a \u222a F2\u207a \u222a ... \u222a Fn\u207a) = F\u207a, then the decomposition preserves dependencies. If (F1\u207a \u222a F2\u207a \u222a ... \u222a Fn\u207a) \u2282 F\u207a, then the decomposition does not preserve dependencies. If (F1\u207a \u222a F2\u207a \u222a ... \u222a Fn\u207a) \u2283 F\u207a, then this is not possible. Testing for Dependency Preservation Using a Polynomial-Time Algorithm A more efficient method for testing dependency preservation is to use the following steps: Initialize result = \u03b1 (the dependency to be tested) While changes occur to result, do the following: For each Ri in the decomposition t = (result \u2229 Ri)\u207a \u2229 Ri result = result \u222a t If result contains all attributes in \u03b2 (the right-hand side of the dependency), then the decomposition preserves the dependency. Practice Problems Lossless Join Decomposition: Check if the following decomposition of R is lossless: R(ABC): F = {A \u2192 B, B \u2192 C} D = {AB, BC} Dependency Preservation: Check if the following decomposition of R preserves dependencies: R(ABCDEF): F = {A \u2192 BC, B \u2192 C, C \u2192 D, D \u2192 E, E \u2192 F} D = {AB, CDE, EF}","title":"Summary of Lecture 5.5 - Relational Database Design5.pdf"},{"location":"Week%205/Lecture%205.5%20-%20Relational%20Database%20Design5/#summary-of-lecture-55-relational-database-design5pdf","text":"Summary Lossless Join Decomposition Definition: A decomposition of a relation R into two relations R1 and R2 is lossless join if the natural join of R1 and R2 is equal to R. Conditions for Lossless Join Decomposition: R1 \u222a R2 = R R1 \u2229 R2 \u2260 \u2205 R1 \u2229 R2 \u2192 R1 or R1 \u2229 R2 \u2192 R2 Dependency Preservation Definition: A decomposition of a relation R into two relations R1 and R2 preserves dependencies if every dependency in F (the set of functional dependencies on R) also holds in (F1 \u222a F2), where F1 and F2 are the sets of functional dependencies on R1 and R2, respectively. Conditions for Dependency Preservation: (F1 \u222a F2 \u222a ... \u222a Fn)\u207a = F\u207a If (F1 \u222a F2 \u222a ... \u222a Fn)\u207a \u2282 F\u207a, then the decomposition is not dependency preserving. If (F1 \u222a F2 \u222a ... \u222a Fn)\u207a \u2283 F\u207a, then this is not possible. Testing for Lossless Join Decomposition To determine whether a decomposition is lossless, check the following conditions: R1 \u222a R2 = R R1 \u2229 R2 \u2260 \u2205 R1 \u2229 R2 \u2192 R1 or R1 \u2229 R2 \u2192 R2 Testing for Dependency Preservation Using Attribute Closure To test for dependency preservation, follow these steps: Compute F\u207a, F1\u207a, F2\u207a, and so on. If (F1\u207a \u222a F2\u207a \u222a ... \u222a Fn\u207a) = F\u207a, then the decomposition preserves dependencies. If (F1\u207a \u222a F2\u207a \u222a ... \u222a Fn\u207a) \u2282 F\u207a, then the decomposition does not preserve dependencies. If (F1\u207a \u222a F2\u207a \u222a ... \u222a Fn\u207a) \u2283 F\u207a, then this is not possible. Testing for Dependency Preservation Using a Polynomial-Time Algorithm A more efficient method for testing dependency preservation is to use the following steps: Initialize result = \u03b1 (the dependency to be tested) While changes occur to result, do the following: For each Ri in the decomposition t = (result \u2229 Ri)\u207a \u2229 Ri result = result \u222a t If result contains all attributes in \u03b2 (the right-hand side of the dependency), then the decomposition preserves the dependency. Practice Problems Lossless Join Decomposition: Check if the following decomposition of R is lossless: R(ABC): F = {A \u2192 B, B \u2192 C} D = {AB, BC} Dependency Preservation: Check if the following decomposition of R preserves dependencies: R(ABCDEF): F = {A \u2192 BC, B \u2192 C, C \u2192 D, D \u2192 E, E \u2192 F} D = {AB, CDE, EF}","title":"Summary of Lecture 5.5 - Relational Database Design5.pdf"},{"location":"Week%205/closureofartibut/","text":"(AC)^plus = as A-> B (AC)^+ = ACB as BC -> DE (AC)^+ = ACBDE AEF -> G and F not in (AC)^+ (AC)^+ = \"ACBDE\" closure if ACF as A -> B ACF + = ACFB BC -> DE ACF + = ACFBDE AEF -> G ACF + = ACFBDEG","title":"Closureofartibut"},{"location":"Week%205/ex_can/","text":"","title":"Ex can"},{"location":"Week%205/hw/","text":"","title":"Hw"},{"location":"Week%205/week5/","text":"[ ] lec class [ ] PPA [ ] GRPA [ ] GA [ ] instructor section 1 [ ] instructor section 2 [ ] TA session 1 [ ] TA session 2 [ ] Text book","title":"Week5"},{"location":"gatesmashers/Minimalcover/","text":"Step 1: Decompose the Functional Dependencies The first step is to decompose the functional dependencies into single attribute dependencies if they are not already in that form. Given functional dependencies: \\( A $\\rightarrow $ B \\) \\( C $\\rightarrow $ B \\) \\( D $\\rightarrow $ ABC \\) ( AC $\\rightarrow $ D ) Step 2: Remove Extraneous Attributes Next, we need to check for and remove any extraneous attributes from the left-hand side of the dependencies. \\( A $\\rightarrow $ B \\) (No extraneous attributes) \\( C $\\rightarrow $ B \\) (No extraneous attributes) \\( D $\\rightarrow $ A \\) , \\( D $\\rightarrow $ B \\) , \\( D $\\rightarrow $ C \\) (Decomposed from \\( D $\\rightarrow $ ABC \\) ) \\( AC $\\rightarrow $ D \\) (Check if \\( A \\) or \\( C \\) is extraneous) To check if \\( A \\) or \\( C \\) is extraneous in \\( AC $\\rightarrow $ D \\) : Compute the closure of \\( A \\) and \\( C \\) individually and see if they can derive \\( D \\) . Step 3: Remove Redundant Dependencies Finally, we need to check for and remove any redundant dependencies. \\( A $\\rightarrow $ B \\) (No redundancy) \\( C $\\rightarrow $ B \\) (No redundancy) \\( D $\\rightarrow $ A \\) , \\( D $\\rightarrow $ B \\) , \\( D $\\rightarrow $ C \\) (Check if any of these are redundant) \\( AC $\\rightarrow $ D \\) (Check if this is redundant) To check for redundancy: Remove one dependency at a time and compute the closure of the remaining dependencies to see if the removed dependency can still be derived. Final Minimal Cover After removing all extraneous attributes and redundant dependencies, the minimal cover is obtained. From the image, the minimal cover is: \\( A $\\rightarrow $ B \\) \\( C $\\rightarrow $ B \\) \\( D $\\rightarrow $ A \\) \\( D $\\rightarrow $ C \\) \\( AC $\\rightarrow $ D \\) The image shows the process of checking each dependency and marking them as redundant or necessary. The final minimal cover is derived by ensuring all dependencies are essential and minimal.","title":"Minimalcover"},{"location":"gatesmashers/Minimalcover/#step-2-remove-extraneous-attributes","text":"Next, we need to check for and remove any extraneous attributes from the left-hand side of the dependencies. \\( A $\\rightarrow $ B \\) (No extraneous attributes) \\( C $\\rightarrow $ B \\) (No extraneous attributes) \\( D $\\rightarrow $ A \\) , \\( D $\\rightarrow $ B \\) , \\( D $\\rightarrow $ C \\) (Decomposed from \\( D $\\rightarrow $ ABC \\) ) \\( AC $\\rightarrow $ D \\) (Check if \\( A \\) or \\( C \\) is extraneous) To check if \\( A \\) or \\( C \\) is extraneous in \\( AC $\\rightarrow $ D \\) : Compute the closure of \\( A \\) and \\( C \\) individually and see if they can derive \\( D \\) .","title":"Step 2: Remove Extraneous Attributes"},{"location":"gatesmashers/Minimalcover/#step-3-remove-redundant-dependencies","text":"Finally, we need to check for and remove any redundant dependencies. \\( A $\\rightarrow $ B \\) (No redundancy) \\( C $\\rightarrow $ B \\) (No redundancy) \\( D $\\rightarrow $ A \\) , \\( D $\\rightarrow $ B \\) , \\( D $\\rightarrow $ C \\) (Check if any of these are redundant) \\( AC $\\rightarrow $ D \\) (Check if this is redundant) To check for redundancy: Remove one dependency at a time and compute the closure of the remaining dependencies to see if the removed dependency can still be derived.","title":"Step 3: Remove Redundant Dependencies"},{"location":"gatesmashers/Minimalcover/#final-minimal-cover","text":"After removing all extraneous attributes and redundant dependencies, the minimal cover is obtained. From the image, the minimal cover is: \\( A $\\rightarrow $ B \\) \\( C $\\rightarrow $ B \\) \\( D $\\rightarrow $ A \\) \\( D $\\rightarrow $ C \\) \\( AC $\\rightarrow $ D \\) The image shows the process of checking each dependency and marking them as redundant or necessary. The final minimal cover is derived by ensuring all dependencies are essential and minimal.","title":"Final Minimal Cover"},{"location":"gatesmashers/Normalization/","text":"Normalization Anomaly Insertion Anomaly Deletion Anomaly Updation Anomaly First Normal Form (1NF) - EF Codd Definition: A table should not contain any multivalued attribute. This ensures that each column contains atomic (indivisible) values, and each entry in a column has a single value. Example Table Rollno Name Course 1 Sai C, Java 2 Aish C, DBMS Explanation of the Example: The table has three columns: Rollno, Name, and Course. The Course column contains multiple values for some entries, which means it is not in 1NF. For instance, for Rollno 1, the courses are \"C\" and \"Java,\" and for Rollno 2, the courses are \"C\" and \"DBMS.\" Not in 1NF The table violates 1NF because it contains multivalued attributes in the Course column. To convert this table to 1NF, each value in the Course column should be atomic. This would typically be done by creating separate rows for each course. Conversion to 1NF: Rollno Name Course 1 Sai C 1 Sai Java 2 Aish C 2 Aish DBMS In this converted table: Each entry in the Course column is now atomic. There are no multivalued attributes. Primary key {Rollno , Course} -> Composite Key By following this process, the table adheres to the rules of the First Normal Form (1NF), ensuring that each column contains only indivisible values and each row represents a unique entity. list of properties related to functional dependencies in the context of database theory. Reflexivity : If \\( Y \\) is a subset of \\( X \\) , then \\( X $\\rightarrow$ Y \\) . This means that if \\( Y \\) is contained within \\( X \\) , then \\( X \\) functionally determines \\( Y \\) . Augmentation : If \\( X $\\rightarrow$ Y \\) , then \\( XZ $\\rightarrow$ YZ \\) . This means that if \\( X \\) functionally determines \\( Y \\) , then adding the same set \\( Z \\) to both sides preserves the functional dependency. Transitivity : If \\( X $\\rightarrow$ Y \\) and \\( Y $\\rightarrow$ Z \\) , then \\( X $\\rightarrow$ Z \\) . This means that if \\( X \\) functionally determines \\( Y \\) and \\( Y \\) functionally determines \\( Z \\) , then \\( X \\) functionally determines \\( Z \\) . Union : If \\( X $\\rightarrow$ Y \\) and \\( X $\\rightarrow$ Z \\) , then \\( X $\\rightarrow$ YZ \\) . This means that if \\( X \\) functionally determines \\( Y \\) and \\( X \\) also functionally determines \\( Z \\) , then \\( X \\) functionally determines the combination of \\( Y \\) and \\( Z \\) . Decomposition : If \\( X $\\rightarrow$ YZ \\) , then \\( X $\\rightarrow$ Y \\) and \\( X $\\rightarrow$ Z \\) . This means that if \\( X \\) functionally determines the combination of \\( Y \\) and \\( Z \\) , then \\( X \\) functionally determines \\( Y \\) and \\( X \\) functionally determines \\( Z \\) separately. Pseudotransitivity : If \\( X $\\rightarrow$ Y \\) and \\( WY $\\rightarrow$ Z \\) , then \\( WX $\\rightarrow$ Z \\) . This means that if \\( X \\) functionally determines \\( Y \\) and \\( WY \\) functionally determines \\( Z \\) , then \\( WX \\) functionally determines \\( Z \\) . Composition : If \\( X $\\rightarrow$ Y \\) and \\( Z $\\rightarrow$ W \\) , then \\( XZ $\\rightarrow$ YW \\) . This means that if \\( X \\) functionally determines \\( Y \\) and \\( Z \\) functionally determines \\( W \\) , then the combination \\( XZ \\) functionally determines the combination \\( YW \\) . These properties are fundamental in understanding and working with functional dependencies in relational database design, particularly in the normalization process. Explanation of Second Normal Form (2NF) with Example Table Second Normal Form (2NF) To achieve Second Normal Form, a table must satisfy the following conditions: First Normal Form (1NF) : The table must already be in First Normal Form. This means that the table should have no repeating groups or arrays. Each column should contain atomic (indivisible) values, and each record should be unique. Full Functional Dependency : All non-prime attributes (attributes that are not part of any candidate key) must be fully functionally dependent on the candidate key. This means that each non-prime attribute should depend on the whole candidate key, not just a part of it. Example Table Consider the following table: Customer ID Store ID Location 1 1 Delhi 1 3 Mumbai 2 1 Delhi 3 2 Bangalore 4 3 Mumbai Customer ID and Store ID together form a composite key. Location is a non-prime attribute. Analysis of the Example Table First Normal Form (1NF) : The table is in 1NF as all values are atomic and each record is unique. Full Functional Dependency : The non-prime attribute Location should be fully functionally dependent on the entire composite key (Customer ID, Store ID). However, in this table, Location depends only on Store ID and not on the combination of Customer ID and Store ID . This indicates a partial dependency, which violates the 2NF rule. Converting to Second Normal Form (2NF) To convert this table to 2NF, we need to remove the partial dependency by creating separate tables. For example: Stores Table : Columns: Store ID , Location This table will store the location of each store. Store ID Location 1 Delhi 2 Bangalore 3 Mumbai Customer_Store Table : Columns: Customer ID , Store ID This table will store the relationship between customers and stores. Customer ID Store ID 1 1 1 3 2 1 3 2 4 3 By splitting the original table into these two tables, we ensure that the non-prime attribute (Location) is fully functionally dependent on the candidate key in its respective table, satisfying the 2NF requirements. To determine the 2nd Normal Form (2NF) for a given relation, we need to ensure that the relation is in 1st Normal Form (1NF) and that all non-prime attributes are fully functionally dependent on the primary key. From the image, we have the following information: Relation \\( R(ABCDEF) \\) Functional Dependencies (FDs): \\( FD: C $\\rightarrow $F \\) \\( E $\\rightarrow $A \\) \\( EC $\\rightarrow $D \\) \\( A $\\rightarrow $B \\) Candidate Key (CK): \\( EC \\) Prime Attributes: \\( E, C \\) Non-Prime Attributes: \\( A, B, D, F \\) To be in 2NF, the relation must be in 1NF and all non-prime attributes must be fully functionally dependent on the entire candidate key, not just part of it. Steps to achieve 2NF: Identify Partial Dependencies : \\( C $\\rightarrow $F \\) (Partial dependency since \\( C \\) is part of the candidate key \\( EC \\) ) \\( E $\\rightarrow $A \\) (Partial dependency since \\( E \\) is part of the candidate key \\( EC \\) ) \\( EC $\\rightarrow $D \\) (Full dependency since \\( D \\) depends on the entire candidate key \\( EC \\) ) \\( A $\\rightarrow $B \\) (Not a partial dependency since \\( A \\) is not part of the candidate key) Remove Partial Dependencies : Create new relations to remove partial dependencies: \\( R1(CE, D) \\) with \\( CE $\\rightarrow $D \\) \\( R2(C, F) \\) with \\( C $\\rightarrow $F \\) \\( R3(E, A) \\) with \\( E $\\rightarrow $A \\) \\( R4(A, B) \\) with \\( A $\\rightarrow $B \\) Resulting Relations in 2NF: \\( R1(CE, D) \\) \\( R2(C, F) \\) \\( R3(E, A , B) \\) These relations are now in 2NF because all non-prime attributes are fully functionally dependent on the entire candidate key of their respective relations. Explanation of Third Normal Form (3NF) Prerequisite: Second Normal Form (2NF) A table must already be in the Second Normal Form (2NF) before it can be considered for 3NF. 2NF requires that the table is in the First Normal Form (1NF) and that all non-key attributes are fully functionally dependent on the primary key. No Transitive Dependency For a table to be in 3NF, it should not have any transitive dependencies. A transitive dependency occurs when a non-key attribute depends on another non-key attribute rather than depending directly on the primary key. Example Consider the table shown in the image: Rollno State City 1 Punjab Mohali 2 Haryana Ambala 3 Punjab Mohali 4 Haryana Ambala 5 Bihar Patna In this table: Rollno is the primary key. State and City are non-key attributes. Step-by-Step Normalization to 3NF First Normal Form (1NF) The table is already in 1NF because all columns contain atomic values, and each column contains values of a single type. Second Normal Form (2NF) The table is in 2NF because there are no partial dependencies; all non-key attributes ( State and City ) depend on the whole primary key ( Rollno ). Third Normal Form (3NF) To check for 3NF, we need to ensure there are no transitive dependencies. In the given table, City depends on State , and State depends on Rollno . This is a transitive dependency because City indirectly depends on Rollno through State . To remove the transitive dependency, we can decompose the table into two tables: Table 1: Rollno_State Rollno State 1 Punjab 2 Haryana 3 Punjab 4 Haryana 5 Bihar Table 2: State_City State City Punjab Mohali Haryana Ambala Bihar Patna Now, both tables are in 3NF: In Rollno_State , State depends directly on Rollno . In State_City , City depends directly on State . By decomposing the original table, we have removed the transitive dependency, thus achieving the Third Normal Form (3NF). concept of Boyce-Codd Normal Form (BCNF) using a table named \"Student.\" Here is a detailed explanation: Table Structure The table \"Student\" has the following columns: Rollno : Unique identifier for each student. Name : Name of the student. Voteid : Voter ID of the student. Age : Age of the student. Data in the Table Rollno Name Voteid Age 1 Ravi K0123 20 2 Varun M034 21 3 Ravi K786 23 4 Rahul D286 21 Candidate Keys (CK) The candidate keys for this table are: Rollno Voteid Functional Dependencies (FD) The functional dependencies in the table are: Rollno \u2192 Name Rollno \u2192 Voteid Voteid \u2192 Age Voteid \u2192 Rollno Explanation of BCNF BCNF is a higher version of the Third Normal Form (3NF). A table is in BCNF if, for every one of its non-trivial functional dependencies X \u2192 Y, X is a super key. In other words, for every functional dependency, the left-hand side should be a candidate key. Analysis of the Table Rollno \u2192 Name : Rollno is a candidate key, so this dependency is fine. Rollno \u2192 Voteid : Rollno is a candidate key, so this dependency is fine. Voteid \u2192 Age : Voteid is a candidate key, so this dependency is fine. Voteid \u2192 Rollno : Voteid is a candidate key, so this dependency is fine. Since all the functional dependencies have candidate keys on the left-hand side, the table is already in BCNF. Conclusion The table \"Student\" is in Boyce-Codd Normal Form (BCNF) because all the functional dependencies have candidate keys on the left-hand side. This ensures that the table is free from redundancy and update anomalies. Dependency Preserving Decomposition : Third Normal Form (3NF) : Always ensures that the decomposition of a database schema is dependency preserving. This means that after decomposing a relation into smaller relations, all functional dependencies are preserved in the resulting relations. This is crucial for maintaining the integrity and consistency of the database. Boyce-Codd Normal Form (BCNF) : Does not always ensure dependency preserving decomposition. While BCNF is a stricter form of normalization compared to 3NF, it may sometimes result in a loss of some functional dependencies when decomposing a relation. Lossless Decomposition : Both 3NF and BCNF ensure lossless decomposition. This means that when a relation is decomposed into two or more relations, it is possible to reconstruct the original relation without any loss of information. Lossless decomposition is essential to ensure that no data is lost during the normalization process. Example : The relation \\( R(ABCD) \\) is given with the functional dependencies \\( \\{A \\rightarrow B, C \\rightarrow D, D \\rightarrow A\\} \\) . This example illustrates a scenario where the concepts of dependency preservation and lossless decomposition are applied. In summary, while both 3NF and BCNF ensure that the decomposition of a relation is lossless, only 3NF guarantees that all functional dependencies are preserved in the decomposed relations. BCNF, being a stricter form of normalization, may sometimes sacrifice dependency preservation to achieve a higher level of normalization. 1st Normal Form 2nd Normal Form 3rd Normal Form BCNF 4th Normal Form 5th Normal Form * No Multivalued attribute * In 1st NF + * In 2nd NF + * In 3rd NF + * In BCNF + * In 4th NF + * Only Single valued * No Partial Dependency * No Transitive Dependency * L.H.S must be CK or SK * No Multivalued Dependency * Lossless Decomposition Rollno * Only Full Dependency * No Non-prime should determine non-prime X -> Y X -> -> Y 1 C, C++ if AB is the primary key AB -> C if x is the prime and y and z are no prime X -> Y -> Z 2 C++, Java B -> C not possible PK -> Non-prime A -> C not possible CK - > Prime This condition vilote Non-prime -> Non- prime","title":"Normalization"},{"location":"gatesmashers/Normalization/#normalization","text":"","title":"Normalization"},{"location":"gatesmashers/Normalization/#anomaly","text":"Insertion Anomaly Deletion Anomaly Updation Anomaly","title":"Anomaly"},{"location":"gatesmashers/Normalization/#first-normal-form-1nf-ef-codd","text":"Definition: A table should not contain any multivalued attribute. This ensures that each column contains atomic (indivisible) values, and each entry in a column has a single value.","title":"First Normal Form (1NF) - EF Codd"},{"location":"gatesmashers/Normalization/#example-table","text":"Rollno Name Course 1 Sai C, Java 2 Aish C, DBMS Explanation of the Example: The table has three columns: Rollno, Name, and Course. The Course column contains multiple values for some entries, which means it is not in 1NF. For instance, for Rollno 1, the courses are \"C\" and \"Java,\" and for Rollno 2, the courses are \"C\" and \"DBMS.\"","title":"Example Table"},{"location":"gatesmashers/Normalization/#not-in-1nf","text":"The table violates 1NF because it contains multivalued attributes in the Course column. To convert this table to 1NF, each value in the Course column should be atomic. This would typically be done by creating separate rows for each course.","title":"Not in 1NF"},{"location":"gatesmashers/Normalization/#conversion-to-1nf","text":"Rollno Name Course 1 Sai C 1 Sai Java 2 Aish C 2 Aish DBMS In this converted table: Each entry in the Course column is now atomic. There are no multivalued attributes. Primary key {Rollno , Course} -> Composite Key By following this process, the table adheres to the rules of the First Normal Form (1NF), ensuring that each column contains only indivisible values and each row represents a unique entity. list of properties related to functional dependencies in the context of database theory. Reflexivity : If \\( Y \\) is a subset of \\( X \\) , then \\( X $\\rightarrow$ Y \\) . This means that if \\( Y \\) is contained within \\( X \\) , then \\( X \\) functionally determines \\( Y \\) . Augmentation : If \\( X $\\rightarrow$ Y \\) , then \\( XZ $\\rightarrow$ YZ \\) . This means that if \\( X \\) functionally determines \\( Y \\) , then adding the same set \\( Z \\) to both sides preserves the functional dependency. Transitivity : If \\( X $\\rightarrow$ Y \\) and \\( Y $\\rightarrow$ Z \\) , then \\( X $\\rightarrow$ Z \\) . This means that if \\( X \\) functionally determines \\( Y \\) and \\( Y \\) functionally determines \\( Z \\) , then \\( X \\) functionally determines \\( Z \\) . Union : If \\( X $\\rightarrow$ Y \\) and \\( X $\\rightarrow$ Z \\) , then \\( X $\\rightarrow$ YZ \\) . This means that if \\( X \\) functionally determines \\( Y \\) and \\( X \\) also functionally determines \\( Z \\) , then \\( X \\) functionally determines the combination of \\( Y \\) and \\( Z \\) . Decomposition : If \\( X $\\rightarrow$ YZ \\) , then \\( X $\\rightarrow$ Y \\) and \\( X $\\rightarrow$ Z \\) . This means that if \\( X \\) functionally determines the combination of \\( Y \\) and \\( Z \\) , then \\( X \\) functionally determines \\( Y \\) and \\( X \\) functionally determines \\( Z \\) separately. Pseudotransitivity : If \\( X $\\rightarrow$ Y \\) and \\( WY $\\rightarrow$ Z \\) , then \\( WX $\\rightarrow$ Z \\) . This means that if \\( X \\) functionally determines \\( Y \\) and \\( WY \\) functionally determines \\( Z \\) , then \\( WX \\) functionally determines \\( Z \\) . Composition : If \\( X $\\rightarrow$ Y \\) and \\( Z $\\rightarrow$ W \\) , then \\( XZ $\\rightarrow$ YW \\) . This means that if \\( X \\) functionally determines \\( Y \\) and \\( Z \\) functionally determines \\( W \\) , then the combination \\( XZ \\) functionally determines the combination \\( YW \\) . These properties are fundamental in understanding and working with functional dependencies in relational database design, particularly in the normalization process.","title":"Conversion to 1NF:"},{"location":"gatesmashers/Normalization/#explanation-of-second-normal-form-2nf-with-example-table","text":"","title":"Explanation of Second Normal Form (2NF) with Example Table"},{"location":"gatesmashers/Normalization/#second-normal-form-2nf","text":"To achieve Second Normal Form, a table must satisfy the following conditions: First Normal Form (1NF) : The table must already be in First Normal Form. This means that the table should have no repeating groups or arrays. Each column should contain atomic (indivisible) values, and each record should be unique. Full Functional Dependency : All non-prime attributes (attributes that are not part of any candidate key) must be fully functionally dependent on the candidate key. This means that each non-prime attribute should depend on the whole candidate key, not just a part of it.","title":"Second Normal Form (2NF)"},{"location":"gatesmashers/Normalization/#example-table_1","text":"Consider the following table: Customer ID Store ID Location 1 1 Delhi 1 3 Mumbai 2 1 Delhi 3 2 Bangalore 4 3 Mumbai Customer ID and Store ID together form a composite key. Location is a non-prime attribute.","title":"Example Table"},{"location":"gatesmashers/Normalization/#analysis-of-the-example-table","text":"First Normal Form (1NF) : The table is in 1NF as all values are atomic and each record is unique. Full Functional Dependency : The non-prime attribute Location should be fully functionally dependent on the entire composite key (Customer ID, Store ID). However, in this table, Location depends only on Store ID and not on the combination of Customer ID and Store ID . This indicates a partial dependency, which violates the 2NF rule.","title":"Analysis of the Example Table"},{"location":"gatesmashers/Normalization/#converting-to-second-normal-form-2nf","text":"To convert this table to 2NF, we need to remove the partial dependency by creating separate tables. For example: Stores Table : Columns: Store ID , Location This table will store the location of each store. Store ID Location 1 Delhi 2 Bangalore 3 Mumbai Customer_Store Table : Columns: Customer ID , Store ID This table will store the relationship between customers and stores. Customer ID Store ID 1 1 1 3 2 1 3 2 4 3 By splitting the original table into these two tables, we ensure that the non-prime attribute (Location) is fully functionally dependent on the candidate key in its respective table, satisfying the 2NF requirements. To determine the 2nd Normal Form (2NF) for a given relation, we need to ensure that the relation is in 1st Normal Form (1NF) and that all non-prime attributes are fully functionally dependent on the primary key. From the image, we have the following information: Relation \\( R(ABCDEF) \\) Functional Dependencies (FDs): \\( FD: C $\\rightarrow $F \\) \\( E $\\rightarrow $A \\) \\( EC $\\rightarrow $D \\) \\( A $\\rightarrow $B \\) Candidate Key (CK): \\( EC \\) Prime Attributes: \\( E, C \\) Non-Prime Attributes: \\( A, B, D, F \\) To be in 2NF, the relation must be in 1NF and all non-prime attributes must be fully functionally dependent on the entire candidate key, not just part of it.","title":"Converting to Second Normal Form (2NF)"},{"location":"gatesmashers/Normalization/#steps-to-achieve-2nf","text":"Identify Partial Dependencies : \\( C $\\rightarrow $F \\) (Partial dependency since \\( C \\) is part of the candidate key \\( EC \\) ) \\( E $\\rightarrow $A \\) (Partial dependency since \\( E \\) is part of the candidate key \\( EC \\) ) \\( EC $\\rightarrow $D \\) (Full dependency since \\( D \\) depends on the entire candidate key \\( EC \\) ) \\( A $\\rightarrow $B \\) (Not a partial dependency since \\( A \\) is not part of the candidate key) Remove Partial Dependencies : Create new relations to remove partial dependencies: \\( R1(CE, D) \\) with \\( CE $\\rightarrow $D \\) \\( R2(C, F) \\) with \\( C $\\rightarrow $F \\) \\( R3(E, A) \\) with \\( E $\\rightarrow $A \\) \\( R4(A, B) \\) with \\( A $\\rightarrow $B \\)","title":"Steps to achieve 2NF:"},{"location":"gatesmashers/Normalization/#resulting-relations-in-2nf","text":"\\( R1(CE, D) \\) \\( R2(C, F) \\) \\( R3(E, A , B) \\) These relations are now in 2NF because all non-prime attributes are fully functionally dependent on the entire candidate key of their respective relations.","title":"Resulting Relations in 2NF:"},{"location":"gatesmashers/Normalization/#explanation-of-third-normal-form-3nf","text":"Prerequisite: Second Normal Form (2NF) A table must already be in the Second Normal Form (2NF) before it can be considered for 3NF. 2NF requires that the table is in the First Normal Form (1NF) and that all non-key attributes are fully functionally dependent on the primary key. No Transitive Dependency For a table to be in 3NF, it should not have any transitive dependencies. A transitive dependency occurs when a non-key attribute depends on another non-key attribute rather than depending directly on the primary key.","title":"Explanation of Third Normal Form (3NF)"},{"location":"gatesmashers/Normalization/#example","text":"Consider the table shown in the image: Rollno State City 1 Punjab Mohali 2 Haryana Ambala 3 Punjab Mohali 4 Haryana Ambala 5 Bihar Patna In this table: Rollno is the primary key. State and City are non-key attributes.","title":"Example"},{"location":"gatesmashers/Normalization/#step-by-step-normalization-to-3nf","text":"First Normal Form (1NF) The table is already in 1NF because all columns contain atomic values, and each column contains values of a single type. Second Normal Form (2NF) The table is in 2NF because there are no partial dependencies; all non-key attributes ( State and City ) depend on the whole primary key ( Rollno ). Third Normal Form (3NF) To check for 3NF, we need to ensure there are no transitive dependencies. In the given table, City depends on State , and State depends on Rollno . This is a transitive dependency because City indirectly depends on Rollno through State . To remove the transitive dependency, we can decompose the table into two tables: Table 1: Rollno_State Rollno State 1 Punjab 2 Haryana 3 Punjab 4 Haryana 5 Bihar Table 2: State_City State City Punjab Mohali Haryana Ambala Bihar Patna Now, both tables are in 3NF: In Rollno_State , State depends directly on Rollno . In State_City , City depends directly on State . By decomposing the original table, we have removed the transitive dependency, thus achieving the Third Normal Form (3NF).","title":"Step-by-Step Normalization to 3NF"},{"location":"gatesmashers/Normalization/#concept-of-boyce-codd-normal-form-bcnf-using-a-table-named-student-here-is-a-detailed-explanation","text":"","title":"concept of Boyce-Codd Normal Form (BCNF) using a table named \"Student.\" Here is a detailed explanation:"},{"location":"gatesmashers/Normalization/#table-structure","text":"The table \"Student\" has the following columns: Rollno : Unique identifier for each student. Name : Name of the student. Voteid : Voter ID of the student. Age : Age of the student.","title":"Table Structure"},{"location":"gatesmashers/Normalization/#data-in-the-table","text":"Rollno Name Voteid Age 1 Ravi K0123 20 2 Varun M034 21 3 Ravi K786 23 4 Rahul D286 21","title":"Data in the Table"},{"location":"gatesmashers/Normalization/#candidate-keys-ck","text":"The candidate keys for this table are: Rollno Voteid","title":"Candidate Keys (CK)"},{"location":"gatesmashers/Normalization/#functional-dependencies-fd","text":"The functional dependencies in the table are: Rollno \u2192 Name Rollno \u2192 Voteid Voteid \u2192 Age Voteid \u2192 Rollno","title":"Functional Dependencies (FD)"},{"location":"gatesmashers/Normalization/#explanation-of-bcnf","text":"BCNF is a higher version of the Third Normal Form (3NF). A table is in BCNF if, for every one of its non-trivial functional dependencies X \u2192 Y, X is a super key. In other words, for every functional dependency, the left-hand side should be a candidate key.","title":"Explanation of BCNF"},{"location":"gatesmashers/Normalization/#analysis-of-the-table","text":"Rollno \u2192 Name : Rollno is a candidate key, so this dependency is fine. Rollno \u2192 Voteid : Rollno is a candidate key, so this dependency is fine. Voteid \u2192 Age : Voteid is a candidate key, so this dependency is fine. Voteid \u2192 Rollno : Voteid is a candidate key, so this dependency is fine. Since all the functional dependencies have candidate keys on the left-hand side, the table is already in BCNF.","title":"Analysis of the Table"},{"location":"gatesmashers/Normalization/#conclusion","text":"The table \"Student\" is in Boyce-Codd Normal Form (BCNF) because all the functional dependencies have candidate keys on the left-hand side. This ensures that the table is free from redundancy and update anomalies. Dependency Preserving Decomposition : Third Normal Form (3NF) : Always ensures that the decomposition of a database schema is dependency preserving. This means that after decomposing a relation into smaller relations, all functional dependencies are preserved in the resulting relations. This is crucial for maintaining the integrity and consistency of the database. Boyce-Codd Normal Form (BCNF) : Does not always ensure dependency preserving decomposition. While BCNF is a stricter form of normalization compared to 3NF, it may sometimes result in a loss of some functional dependencies when decomposing a relation. Lossless Decomposition : Both 3NF and BCNF ensure lossless decomposition. This means that when a relation is decomposed into two or more relations, it is possible to reconstruct the original relation without any loss of information. Lossless decomposition is essential to ensure that no data is lost during the normalization process. Example : The relation \\( R(ABCD) \\) is given with the functional dependencies \\( \\{A \\rightarrow B, C \\rightarrow D, D \\rightarrow A\\} \\) . This example illustrates a scenario where the concepts of dependency preservation and lossless decomposition are applied. In summary, while both 3NF and BCNF ensure that the decomposition of a relation is lossless, only 3NF guarantees that all functional dependencies are preserved in the decomposed relations. BCNF, being a stricter form of normalization, may sometimes sacrifice dependency preservation to achieve a higher level of normalization. 1st Normal Form 2nd Normal Form 3rd Normal Form BCNF 4th Normal Form 5th Normal Form * No Multivalued attribute * In 1st NF + * In 2nd NF + * In 3rd NF + * In BCNF + * In 4th NF + * Only Single valued * No Partial Dependency * No Transitive Dependency * L.H.S must be CK or SK * No Multivalued Dependency * Lossless Decomposition Rollno * Only Full Dependency * No Non-prime should determine non-prime X -> Y X -> -> Y 1 C, C++ if AB is the primary key AB -> C if x is the prime and y and z are no prime X -> Y -> Z 2 C++, Java B -> C not possible PK -> Non-prime A -> C not possible CK - > Prime This condition vilote Non-prime -> Non- prime","title":"Conclusion"},{"location":"gatesmashers/checklist/","text":"[X] Lec-1: DBMS Syllabus for GATE, UGCNET, NIELIT, DSSSB etc.| Full DBMS for College/University Students [X] Lec-2: Introduction to DBMS (Database Management System) With Real life examples | What is DBMS [X] Lec-3: File System vs DBMS | Disadvantages of File System | DBMS Advantages [X] Lec-4: 2 tier and 3 tier Architecture with real life examples | Database Management System [X] Lec-5: What is Schema | How to define Schema | Database management system in Hindi [X] Lec-6: Three Schema Architecture | Three Level of Abstraction | Database Management System [X] Lec-7: What is Data Independence | Logical vs. Physical Independence | DBMS [X] Lec-8.0: Integrity Constraints in Database with Examples [X] Lec-8: What is CANDIDATE KEY and PRIMARY key | Full Concept | Most suitable examples | DBMS [X] Lec-9: What is Primary Key in DBMS | Primary Key with Examples in Hindi [X] Lec-10: Foreign Key in DBMS | Full Concept with examples | DBMS in Hindi [X] Lec-11: Insert, Update & Delete from Foreign Key table | Referential Integrity [X] Lec-12: Question on Foreign Key | \u092f\u0947 Question Competition Exams \u092e\u0947\u0902 \u0905\u0915\u094d\u0938\u0930 \u092a\u0942\u091b\u093e \u0917\u092f\u093e \u0939\u0948 [X] Lec-13: Super key in DBMS in HINDI | \u092f\u0947 Question Competition Exams \u092e\u0947\u0902 \u0905\u0915\u094d\u0938\u0930 \u092a\u0942\u091b\u093e \u0917\u092f\u093e \u0939\u0948 [ ] Lec-14: Introduction to ER model | ER Model \u0915\u094d\u092f\u093e \u0939\u0948 [ ] Lec-15: Types of Attributes in ER Model | Full Concept | DBMS in Hindi [ ] Lec-16: One to One relationship in DBMS in Hindi [ ] Lec-17: One to Many Relationship in DBMS in Hindi | 1-M Relationship [ ] Lec-18: Many to Many Relationship in DBMS | M-N Relationship [ ] Lec-19:Question on Minimize tables in ER Model | \u092f\u0947 Question Competition Exams \u092e\u0947\u0902 \u0905\u0915\u094d\u0938\u0930 \u092a\u0942\u091b\u093e \u0917\u092f\u093e \u0939\u0948 [X] Lec-20: Introduction to Normalization | Insertion, Deletion & Updation Anomaly [X] Lec-21: First Normal form in DBMS in HINDI | 1st Normal form \u0915\u094d\u092f\u093e \u0939\u094b\u0924\u0940 \u0939\u0948 ? [X] Lec-22: Finding Closure of Functional dependency in DBMS | Easiest & Simplest way [X] Lec-23: Functional Dependency & its properties in DBMS in HINDI [X] Lec-24: Second Normal Form | 2NF | Database Management System [X] Lec-25: Third Normal Form in dbms with examples in Hindi | Normalization [X] Lec-26: Boyce Codd Normal Form #BCNF #DBMS #Normalization with best examples [X] Lec-27: BCNF Always Ensures Dependency Preserving Decomposition?? Normalization Examples [X] Lec-28: Lossless and Lossy Decomposition| Fifth (5th) Normal Form | Database Management System [X] Lec-29: All Normal Forms with Real life examples | 1NF 2NF 3NF BCNF 4NF 5NF | All in One [X] Lec-30: Minimal Cover in DBMS With Example | Canonical cover [ ] Lec-31: Practice Question on Normalization | Database Management System [ ] Lec-32: How to find out the Normal form of a Relation | DBMS [ ] Lec-33: How to Solve Normalization Questions | DBMS [ ] Lec-34: Important Question Explanation on Normalization [ ] Lec-35: Cover and Equivalence of Functional Dependencies | Database Management System [ ] Lec-36: Dependency Preserving Decomposition in DBMS with Examples in Hindi | DBMS [ ] Lec-37: Dependency Preserving Decomposition in DBMS | Example 2 in Hindi [X] Lec-38: Introduction to Joins and its types | Need of Joins with example | DBMS [X] Lec-39: Natural Join operation with Example | Database Management System [X] Lec-40: Self Join operation with Example | Database Management System [X] Lec-41: Equi Join operation with Example | Database Management System [X] Lec-42: Left Ou>>>?ter Join operation with Example | Database Management System [X] Lec-43: Right Outer Join operation with Example | Database Management System [X] Lec-44: Introduction to Relational Algebra | Database Management System [X] Lec-45: Projection in Relational Algebra | Database Management System [X] Lec-46: Selection in Relational Algebra | Database Management System [X] Lec-47: Cross/Cartesian Product in Relational Algebra | Database Management System [X] Lec-48: Set Difference in Relational Algebra | Database Management System [X] Lec-49: Union Operation in Relational Algebra | Database Management System [X] Lec-50: Division Operation in Relational Algebra | Database Management System [X] Lec-51: Tuple Calculus in DBMS with examples [X] Lec-52: Introduction to Structured Query Language | All Points regarding its Features and Syllabus [X] Lec-53: All Types of SQL Commands with Example | DDL, DML, DCL, TCL and CONSTRAINTS | DBMS [X] Lec-54: Create table in SQL with execution | SQL for Beginners | Oracle LIVE [X] Lec-55: ALTER Command (DDL) in SQL with Implementation on ORACLE [X] Lec-56: Difference between Alter and Update in SQL with examples in Hindi | DBMS [X] Lec-57 Difference between Delete, Drop & Truncate in SQL | DBMS [X] Lec-58: Constraints in SQL in Hindi | DBMS [X] Lec-59: SQL Queries and Subqueries (part-1) | Database Management System [X] Lec-60: SQL Queries and Subqueries (part-2) | 2nd Highest Salary | Nested Queries | DBMS [X] Lec-61: SQL Queries and Subqueries (part-3) | Group By clause | Database Management System [X] Lec-62: SQL Queries and Subqueries (part-4) | Having clause | Database Management System [X] Lec-63: SQL Queries and Subqueries (part-5) | Database Management System [X] Lec-64: SQL Queries and Subqueries (part-6)| use of IN and Not IN | Database Management System [X] Lec-65: SQL Queries and Subqueries (part-7)| use of IN and Not IN in Subquery | DBMS [X] Lec-66: EXIST and NOT EXIST Subqueries(part-8) | Database Management System [X] Lec-67: SQL Aggregate Functions - SUM, AVG(n), COUNT, MIN, MAX Functions | DBMS [X] Lec-68: Correlated Subquery in SQL with Example | Imp for Placements, GATE, NET & SQL certification [X] Lec-69: Difference between Joins, Nested Subquery and Correlated Subquery | Most Imp Concept of SQL [X] Lec-70: Find Nth(1st,2nd,3rd....N) Highest Salary in SQL | Imp for Competitive & Placement exam [X] Lec-71: 3 Imp Questions on SQL basic Concepts | DBMS [ ] Lec-72: Introduction to PL-SQL in DBMS [ ] Lec-73: Introduction to Transaction Concurrency in HINDI | Database Management System [ ] Lec-74: ACID Properties of a Transaction | Database Management System [ ] Lec-75: Transaction States | Database Management System [ ] Lec-76: What is Schedule | Serial Vs Parallel Schedule | Database Management System [ ] Lec-77: All Concurrency Problems | Dirty Read | Incorrect Summary | Lost Update | Phantom Read [ ] Lec-78: Write-Read Conflict or Dirty Read Problem | Database Management System [ ] Lec-79: Read-Write Conflict or Unrepeatable Read Problem | Database Management System [ ] Lec-80: Irrecoverable Vs Recoverable Schedules in Transactions | DBMS [ ] Lec-81: Cascading vs Cascadeless Schedule with Example | Recoverability | DBMS [ ] Lec-82: Introduction to Serializability | Transactions Concurrency and Control | DBMS [ ] Lec-83: Conflict Equivalent Schedules with Example | Transaction concurrency and Control | DBMS [ ] Lec-84: Conflict Serializability | Precedence Graph | Transaction | DBMS [ ] Lec-85: Why View Serializability is Used | Introduction to View Serializability | DBMS [ ] Lec-86:Shared Exclusive Locking Protocol with Example in Hindi | Concurrency Control | DBMS | Part-1 [ ] Lec-87: Drawbacks in Shared/Exclusive Locking Protocol with Example | Concurrency Control Part-2 [ ] Lec-88: 2 Phase Locking(2PL) Protocol in Transaction Concurrency Control | DBMS [ ] Lec-89: Drawbacks in 2 Phase Locking(2PL) Protocol with examples | Concurrency Control | DBMS [ ] Lec-90: Strict 2PL, Rigorous 2PL and Conservative 2PLSchedule | 2 Phase Locking in DBMS [ ] Lec-91: Basic Timestamp Ordering Protocol with Example in Hindi | Concurrency Control | DBMS [ ] Lec-92: How to Solve Question on Timestamp Ordering Protocol | Concurrency Control | DBMS [ ] Lec-93: Why Indexing is used | Indexing Beginning | DBMS [ ] Lec-94: Numerical Example on I/O Cost in Indexing | Part-1 | DBMS [ ] Lec-95: Numerical Example on I/O Cost in Indexing | Part 2 | DBMS [ ] Lec-96: Types Of Indexes | Most Important Video on Indexing [ ] Lec-97: Primary Index With Example | GATE, PSU and UGC NET | DBMS [ ] Lec-98: Clustered Index in Database with Example [ ] Lec-99: Secondary Index in Database with Example | Multilevel Indexing [ ] Lec-100: Introduction to B-Tree and its Structure | Block Pointer, Record Pointer, Key [ ] Lec-101: Insertion in B-Tree with example in Hindi [ ] Lec-102: How to find Order of B-Tree | Imp Question on B-Tree [ ] Lec-103: Difference b/w B-Tree & B+Tree in Hindi with examples [ ] Lec-104: Order of B+ Tree | Order of Leaf Node & Non Leaf Node in B+Tree [ ] Lec-105: Immediate Database Modification in DBMS | Log Based Recovery Methods [ ] Lec-106: Deferred Database Modification in DBMS | Log Based Recovery | Imp for UGC NET and KVS [ ] Lec-107: Like Command in SQL with example in Hindi | Learn SQL in Easiest Way| DBMS [ ] Lec-108: Basic PL-SQL Programming With Execution | Part-1 [ ] Lec-109: Basic PL-SQL Programming(While, For Loop) With Execution | Part-2 [ ] Lec-110: Single row and Multi row functions in SQL [ ] Lec-111: Character functions in SQL with execution | Oracle LIVE [ ] Lec-112: View in Database | Oracle, SQL Server Views | Types of Views [ ] Lec-113: How Aggregate Functions work on NULL Values | SQL | DBMS [ ] Lec-114: What is RAID? RAID 0, RAID 1, RAID 4, RAID 5, RAID 6, Nested RAID 10 Explained [ ] Lec-115: Various objects in Database | Oracle, SQL Server [ ] Lec-116: Important Question explanation on ER Model | DBMS [ ] Lec-117: Very Imp. Questions on DBMS basic concepts and Data Modelling | DBMS [ ] Lec-118: Question on Inner, Left, Right & Full Outer Joins Explanation | DBMS [ ] Lec-119: 4 Imp Questions on Advance DBMS | BIG Data and Data Warehouse | DBMS [ ] Lec-120: Important Question on Normalization (Schemas) Explanation | DBMS [ ] Lec-121: Important Question on Relational Algebra | DBMS [ ] Lec-122: Codd\u2019s 12 Rules of RDBMS with examples [ ] Lec-123: Top 15 SQL Interview Questions Answers | Most Important Questions for Job Interview [ ] Lec-124: CREATE Command (DDL) in SQL with Implementation on ORACLE [ ] Lec-125: SEQUENCE in SQL with Syntax & Examples [ ] Lec-126: How SQL Query executes?? Order of SQL Query Execution\u23f3\ud83d\udd04 [ ] Lec-127: Introduction to Hadoop\ud83d\udc18| What is Hadoop\ud83d\udc18| Hadoop Framework\ud83d\udda5 [ ] Lec-128: Introduction to BIG Data in Hindi | Small Data Vs BIG Data | Real Life Examples [ ] Lec-129: Simple vs Complex vs Materialized Views with examples | DBMS [ ] Foreign Key\ud83d\udd11 with On Delete Cascade with Execution [ ] Procedures in PL-SQL | Local Procedure vs Stored Procedure [ ] How to Fetch Data from Database using Procedures | PL-SQL Procedure [ ] %TYPE & %ROWTYPE in PL-SQL with Examples [ ] What is Cursor in PL-SQL with example","title":"Checklist"},{"location":"gatesmashers/notes/","text":"","title":"Notes"},{"location":"gatesmashers/sql/","text":"SQL key Candidate Key -> Primary Key Candidate key -> keys which are not used as primary key are called alternative key Primary Key {Unique + Not Null } Foreign Key Refrencing Table insert course violation if FK is not in in refrenced table Refrenced Table on delete cascade on delete set null on delete no action on update cascade on update set null on update no action Summary of Relational Algebra Operators Operator Description \u03c3 Select \u03c0 Project \u222a Union \u2212 Difference \u2229 Intersection \u00d7 Cartesian Product $ \\rho$ Rename \\(\\bowtie\\) Natural Join SUM Computes the sum of a specified column AVG Computes the average of a specified column MAX Computes the maximum value of a specified column MIN Computes the minimum value of a specified column DDL- Data Definition Language Create Alter Drop Truncate Rename DML - Data Defination Language Select Insert update Delete DCL - Data Control Language Grant Revoke TCL - Transaction Control Language Commit Rollback save point Constraints primary key foreign key check unique default Not null Create Table - DDL create table <table name> ( col1name datattype , col2name datattype , col1name datattype); desc tablename; create table student ( ID varchar(5),name varchar(20) not null, dept name varchar(20),tot cred numeric(3, 0), primary key (ID), foreign key (dept name) references department); Alter - DDL add columns remove columns modify datatype modify datatype length add constraints remove constraints rename constraints Alter vs Update alter - DDL update - DML Delete Vs Drop Vs Truncate Delete - DML Drop - DDL - drop table Truncate - DDL - no back up even in roll back Constraints Condition for columns or attribute unique not null primary -> unique + not null check forgien key default Nested subquery Vs correlated sub query vs joins Instructor = e1= id salary 1 10000 2 20000 3 20000 4 30000 5 40000 6 5000 Instructor = e2= id salary 1 10000 2 20000 3 20000 4 30000 5 40000 6 5000 first row id e1.salary id e2.salary e2.salary > e1.salary count 1 10000 1 10000 False 0 1 10000 2 20000 True 1 1 10000 3 20000 True 2 1 10000 4 30000 True 2 1 10000 5 40000 True 3 1 10000 2 50000 True 4 Second row id e1.salary id e2.salary e2.salary > e1.salary count 2 20000 1 10000 False 0 2 20000 2 20000 False 0 2 20000 3 20000 False 0 2 20000 4 30000 True 1 2 20000 5 40000 True 2 2 20000 2 50000 True 3 Joins employee= PK Eno E-name Address 1 Ram delhi 2 varun chd 3 Ravi chd 4 Amrit Delhi 5 Nitin noida Department = PK FK Dep_no Name eno D1 HR 1 D2 IT 2 D3 MRKT 4 D4 FINANCE 5 Natural join cross poduct + condition = join employee natural join department = cross poduct + condition \\(empolyee.eno = department.eno\\) cross product = select * from employee , department employee.Eno E-name Address Dep_no Name departmment.eno 1 Ram delhi D1 HR 1 1 Ram delhi D2 IT 2 1 Ram delhi D3 MRKT 4 1 Ram delhi D4 FINANCE 5 2 varun chd D1 HR 1 2 varun chd D2 IT 2 2 varun chd D3 MRKT 4 2 varun chd D4 FINANCE 5 3 Ravi chd D1 HR 1 3 Ravi chd D2 IT 2 3 Ravi chd D3 MRKT 4 3 Ravi chd D4 FINANCE 5 4 Amrit Delhi D1 HR 1 4 Amrit Delhi D2 IT 2 4 Amrit Delhi D3 MRKT 4 4 Amrit Delhi D4 FINANCE 5 5 Nitin noida D1 HR 1 5 Nitin noida D2 IT 2 5 Nitin noida D3 MRKT 4 5 Nitin noida D4 FINANCE 5 select * from employee department empolyee.eno = department.eno Eno E-name Address Dep_no Name eno empolyee.eno = department.eno 1 Ram delhi D1 HR 1 True 1 Ram delhi D2 IT 2 False 1 Ram delhi D3 MRKT 4 False 1 Ram delhi D4 FINANCE 5 False 2 varun chd D1 HR 1 False 2 varun chd D2 IT 2 True 2 varun chd D3 MRKT 4 False 2 varun chd D4 FINANCE 5 False 3 Ravi chd D1 HR 1 False 3 Ravi chd D2 IT 2 False 3 Ravi chd D3 MRKT 4 False 3 Ravi chd D4 FINANCE 5 False 4 Amrit Delhi D1 HR 1 False 4 Amrit Delhi D2 IT 2 False 4 Amrit Delhi D3 MRKT 4 True 4 Amrit Delhi D4 FINANCE 5 False 5 Nitin noida D1 HR 1 False 5 Nitin noida D2 IT 2 False 5 Nitin noida D3 MRKT 4 False 5 Nitin noida D4 FINANCE 5 True result= Eno E-name Address Dep_no Name eno 1 Ram delhi D1 HR 1 2 varun chd D2 IT 2 4 Amrit Delhi D3 MRKT 4 5 Nitin noida D4 FINANCE 5 select E-name from employee natural join department E-name Ram varun Amrit Nitin self Join S_id course_id since s1 c1 2016 s2 c2 2017 s1 c2 2017 find student who is enrolled in two courses ? cross prduct S_id (Left) course_id (Left) since (Left) S_id (Right) course_id (Right) since (Right) s1 c1 2016 s1 c1 2016 s1 c1 2016 s2 c2 2017 s1 c1 2016 s1 c2 2017 s2 c2 2017 s1 c1 2016 s2 c2 2017 s2 c2 2017 s2 c2 2017 s1 c2 2017 s1 c2 2017 s1 c1 2016 s1 c2 2017 s2 c2 2017 s1 c2 2017 s1 c2 2017 select LEFT.s_id from study as left study as right where left.s_id = right.s_id and left.course_id < > right.course_id S_id (Left) course_id (Left) since (Left) S_id (Right) course_id (Right) since (Right) CONDF s1 c1 2016 s1 c1 2016 FALSE s1 c1 2016 s2 c2 2017 FALSE s1 c1 2016 s1 c2 2017 TRUE s2 c2 2017 s1 c1 2016 FALSE s2 c2 2017 s2 c2 2017 FALSE s2 c2 2017 s1 c2 2017 FALSE s1 c2 2017 s1 c1 2016 TRUE s1 c2 2017 s2 c2 2017 FALSE s1 c2 2017 s1 c2 2017 FALSE S_id (Left) s1 Equi Join select e.E_name from emp e dept d where e.address = d.location and e.en_no = d.eno; left outer join A LEFT OUTER JOIN in SQL is a type of join operation that combines rows from two or more tables based on a specified condition and includes unmatched rows from the left table. It allows you to retrieve data from multiple tables based on their related values. Here's a detailed explanation: What is a LEFT OUTER JOIN? A LEFT OUTER JOIN is a type of join operation that combines rows from two or more tables based on a specified condition. It includes all rows from the left table (the \"left\" or \"first\" table) and matching rows from the right table (the \"right\" or \"second\" table). If there is no match in the right table, the result includes NULL values for the columns of the right table[1][2][3]. Syntax The syntax for a LEFT OUTER JOIN in SQL is as follows: SELECT column_name ( s ) FROM table1 LEFT JOIN table2 ON table1 . column_name = table2 . column_name ; Example For example, consider a query that retrieves all employees along with their departments and salaries (if available). The Employees table is the left table, and we perform a LEFT OUTER JOIN with the Departments table using the join condition E.DepartmentID = D.DepartmentID . This ensures that all employees from the Employees table are included in the result set, regardless of whether they have a matching department. We then perform another LEFT OUTER JOIN with the Salaries table using the join condition E.EmployeeID = S.EmployeeID , allowing us to include salary information for employees if it exists[2]. SELECT E . EmployeeID , E . Name , D . DepartmentName , S . SalaryAmount FROM Employees E LEFT OUTER JOIN Departments D ON E . DepartmentID = D . DepartmentID LEFT OUTER JOIN Salaries S ON E . EmployeeID = S . EmployeeID ; Output The output of this query would include all employees, even if they do not have a matching department or salary. The unmatched rows in the Employees table would have NULL values in the DepartmentName and SalaryAmount columns[2]. Key Points Includes all rows from the left table : A LEFT OUTER JOIN includes all rows from the left table, even if there are no matching rows in the right table. Includes matching rows from the right table : If there is a match in the right table, the result includes the corresponding row from the right table. Includes NULL values for unmatched rows in the right table : If there is no match in the right table, the result includes NULL values for the columns of the right table[1][2][3]. Practical Examples Retrieving all customers, even if they haven't placed any orders : A LEFT OUTER JOIN can be used to retrieve all customers, even if they haven't placed any orders. The unmatched rows in the Customers table would have NULL values in the OrdersID column[2]. Right outer join","title":"SQL"},{"location":"gatesmashers/sql/#sql","text":"","title":"SQL"},{"location":"gatesmashers/sql/#key","text":"Candidate Key -> Primary Key Candidate key -> keys which are not used as primary key are called alternative key","title":"key"},{"location":"gatesmashers/sql/#primary-key","text":"{Unique + Not Null }","title":"Primary Key"},{"location":"gatesmashers/sql/#foreign-key","text":"Refrencing Table insert course violation if FK is not in in refrenced table Refrenced Table on delete cascade on delete set null on delete no action on update cascade on update set null on update no action Summary of Relational Algebra Operators Operator Description \u03c3 Select \u03c0 Project \u222a Union \u2212 Difference \u2229 Intersection \u00d7 Cartesian Product $ \\rho$ Rename \\(\\bowtie\\) Natural Join SUM Computes the sum of a specified column AVG Computes the average of a specified column MAX Computes the maximum value of a specified column MIN Computes the minimum value of a specified column","title":"Foreign Key"},{"location":"gatesmashers/sql/#ddl-data-definition-language","text":"Create Alter Drop Truncate Rename","title":"DDL- Data Definition Language"},{"location":"gatesmashers/sql/#dml-data-defination-language","text":"Select Insert update Delete","title":"DML - Data Defination Language"},{"location":"gatesmashers/sql/#dcl-data-control-language","text":"Grant Revoke","title":"DCL - Data Control Language"},{"location":"gatesmashers/sql/#tcl-transaction-control-language","text":"Commit Rollback save point","title":"TCL - Transaction Control Language"},{"location":"gatesmashers/sql/#constraints","text":"primary key foreign key check unique default Not null","title":"Constraints"},{"location":"gatesmashers/sql/#create-table-ddl","text":"create table <table name> ( col1name datattype , col2name datattype , col1name datattype); desc tablename; create table student ( ID varchar(5),name varchar(20) not null, dept name varchar(20),tot cred numeric(3, 0), primary key (ID), foreign key (dept name) references department);","title":"Create Table - DDL"},{"location":"gatesmashers/sql/#alter-ddl","text":"add columns remove columns modify datatype modify datatype length add constraints remove constraints rename constraints","title":"Alter - DDL"},{"location":"gatesmashers/sql/#alter-vs-update","text":"alter - DDL update - DML","title":"Alter vs Update"},{"location":"gatesmashers/sql/#delete-vs-drop-vs-truncate","text":"Delete - DML Drop - DDL - drop table Truncate - DDL - no back up even in roll back","title":"Delete Vs Drop Vs Truncate"},{"location":"gatesmashers/sql/#constraints_1","text":"Condition for columns or attribute unique not null primary -> unique + not null check forgien key default","title":"Constraints"},{"location":"gatesmashers/sql/#nested-subquery-vs-correlated-sub-query-vs-joins","text":"Instructor = e1= id salary 1 10000 2 20000 3 20000 4 30000 5 40000 6 5000 Instructor = e2= id salary 1 10000 2 20000 3 20000 4 30000 5 40000 6 5000 first row id e1.salary id e2.salary e2.salary > e1.salary count 1 10000 1 10000 False 0 1 10000 2 20000 True 1 1 10000 3 20000 True 2 1 10000 4 30000 True 2 1 10000 5 40000 True 3 1 10000 2 50000 True 4 Second row id e1.salary id e2.salary e2.salary > e1.salary count 2 20000 1 10000 False 0 2 20000 2 20000 False 0 2 20000 3 20000 False 0 2 20000 4 30000 True 1 2 20000 5 40000 True 2 2 20000 2 50000 True 3","title":"Nested subquery Vs correlated sub query vs joins"},{"location":"gatesmashers/sql/#joins","text":"employee= PK Eno E-name Address 1 Ram delhi 2 varun chd 3 Ravi chd 4 Amrit Delhi 5 Nitin noida Department = PK FK Dep_no Name eno D1 HR 1 D2 IT 2 D3 MRKT 4 D4 FINANCE 5","title":"Joins"},{"location":"gatesmashers/sql/#natural-join","text":"cross poduct + condition = join employee natural join department = cross poduct + condition \\(empolyee.eno = department.eno\\) cross product = select * from employee , department employee.Eno E-name Address Dep_no Name departmment.eno 1 Ram delhi D1 HR 1 1 Ram delhi D2 IT 2 1 Ram delhi D3 MRKT 4 1 Ram delhi D4 FINANCE 5 2 varun chd D1 HR 1 2 varun chd D2 IT 2 2 varun chd D3 MRKT 4 2 varun chd D4 FINANCE 5 3 Ravi chd D1 HR 1 3 Ravi chd D2 IT 2 3 Ravi chd D3 MRKT 4 3 Ravi chd D4 FINANCE 5 4 Amrit Delhi D1 HR 1 4 Amrit Delhi D2 IT 2 4 Amrit Delhi D3 MRKT 4 4 Amrit Delhi D4 FINANCE 5 5 Nitin noida D1 HR 1 5 Nitin noida D2 IT 2 5 Nitin noida D3 MRKT 4 5 Nitin noida D4 FINANCE 5 select * from employee department empolyee.eno = department.eno Eno E-name Address Dep_no Name eno empolyee.eno = department.eno 1 Ram delhi D1 HR 1 True 1 Ram delhi D2 IT 2 False 1 Ram delhi D3 MRKT 4 False 1 Ram delhi D4 FINANCE 5 False 2 varun chd D1 HR 1 False 2 varun chd D2 IT 2 True 2 varun chd D3 MRKT 4 False 2 varun chd D4 FINANCE 5 False 3 Ravi chd D1 HR 1 False 3 Ravi chd D2 IT 2 False 3 Ravi chd D3 MRKT 4 False 3 Ravi chd D4 FINANCE 5 False 4 Amrit Delhi D1 HR 1 False 4 Amrit Delhi D2 IT 2 False 4 Amrit Delhi D3 MRKT 4 True 4 Amrit Delhi D4 FINANCE 5 False 5 Nitin noida D1 HR 1 False 5 Nitin noida D2 IT 2 False 5 Nitin noida D3 MRKT 4 False 5 Nitin noida D4 FINANCE 5 True","title":"Natural join"},{"location":"gatesmashers/sql/#result","text":"Eno E-name Address Dep_no Name eno 1 Ram delhi D1 HR 1 2 varun chd D2 IT 2 4 Amrit Delhi D3 MRKT 4 5 Nitin noida D4 FINANCE 5 select E-name from employee natural join department E-name Ram varun Amrit Nitin","title":"result="},{"location":"gatesmashers/sql/#self-join","text":"S_id course_id since s1 c1 2016 s2 c2 2017 s1 c2 2017 find student who is enrolled in two courses ? cross prduct S_id (Left) course_id (Left) since (Left) S_id (Right) course_id (Right) since (Right) s1 c1 2016 s1 c1 2016 s1 c1 2016 s2 c2 2017 s1 c1 2016 s1 c2 2017 s2 c2 2017 s1 c1 2016 s2 c2 2017 s2 c2 2017 s2 c2 2017 s1 c2 2017 s1 c2 2017 s1 c1 2016 s1 c2 2017 s2 c2 2017 s1 c2 2017 s1 c2 2017 select LEFT.s_id from study as left study as right where left.s_id = right.s_id and left.course_id < > right.course_id S_id (Left) course_id (Left) since (Left) S_id (Right) course_id (Right) since (Right) CONDF s1 c1 2016 s1 c1 2016 FALSE s1 c1 2016 s2 c2 2017 FALSE s1 c1 2016 s1 c2 2017 TRUE s2 c2 2017 s1 c1 2016 FALSE s2 c2 2017 s2 c2 2017 FALSE s2 c2 2017 s1 c2 2017 FALSE s1 c2 2017 s1 c1 2016 TRUE s1 c2 2017 s2 c2 2017 FALSE s1 c2 2017 s1 c2 2017 FALSE S_id (Left) s1","title":"self Join"},{"location":"gatesmashers/sql/#equi-join","text":"select e.E_name from emp e dept d where e.address = d.location and e.en_no = d.eno;","title":"Equi Join"},{"location":"gatesmashers/sql/#left-outer-join","text":"A LEFT OUTER JOIN in SQL is a type of join operation that combines rows from two or more tables based on a specified condition and includes unmatched rows from the left table. It allows you to retrieve data from multiple tables based on their related values. Here's a detailed explanation:","title":"left outer join"},{"location":"gatesmashers/sql/#what-is-a-left-outer-join","text":"A LEFT OUTER JOIN is a type of join operation that combines rows from two or more tables based on a specified condition. It includes all rows from the left table (the \"left\" or \"first\" table) and matching rows from the right table (the \"right\" or \"second\" table). If there is no match in the right table, the result includes NULL values for the columns of the right table[1][2][3].","title":"What is a LEFT OUTER JOIN?"},{"location":"gatesmashers/sql/#syntax","text":"The syntax for a LEFT OUTER JOIN in SQL is as follows: SELECT column_name ( s ) FROM table1 LEFT JOIN table2 ON table1 . column_name = table2 . column_name ;","title":"Syntax"},{"location":"gatesmashers/sql/#example","text":"For example, consider a query that retrieves all employees along with their departments and salaries (if available). The Employees table is the left table, and we perform a LEFT OUTER JOIN with the Departments table using the join condition E.DepartmentID = D.DepartmentID . This ensures that all employees from the Employees table are included in the result set, regardless of whether they have a matching department. We then perform another LEFT OUTER JOIN with the Salaries table using the join condition E.EmployeeID = S.EmployeeID , allowing us to include salary information for employees if it exists[2]. SELECT E . EmployeeID , E . Name , D . DepartmentName , S . SalaryAmount FROM Employees E LEFT OUTER JOIN Departments D ON E . DepartmentID = D . DepartmentID LEFT OUTER JOIN Salaries S ON E . EmployeeID = S . EmployeeID ;","title":"Example"},{"location":"gatesmashers/sql/#output","text":"The output of this query would include all employees, even if they do not have a matching department or salary. The unmatched rows in the Employees table would have NULL values in the DepartmentName and SalaryAmount columns[2].","title":"Output"},{"location":"gatesmashers/sql/#key-points","text":"Includes all rows from the left table : A LEFT OUTER JOIN includes all rows from the left table, even if there are no matching rows in the right table. Includes matching rows from the right table : If there is a match in the right table, the result includes the corresponding row from the right table. Includes NULL values for unmatched rows in the right table : If there is no match in the right table, the result includes NULL values for the columns of the right table[1][2][3].","title":"Key Points"},{"location":"gatesmashers/sql/#practical-examples","text":"Retrieving all customers, even if they haven't placed any orders : A LEFT OUTER JOIN can be used to retrieve all customers, even if they haven't placed any orders. The unmatched rows in the Customers table would have NULL values in the OrdersID column[2].","title":"Practical Examples"},{"location":"gatesmashers/sql/#right-outer-join","text":"","title":"Right outer join"},{"location":"revision/quiz1_1/","text":"Week 1: Database Management Systems (DBMS) vs. File Handling via Python A comparison between file handling via Python and Database Management Systems (DBMS) across various parameters. Scalability: Python: Struggles with handling large amounts of data and structural changes. DBMS: Offers built-in features for high scalability and easy structural modifications. Performance: Python: Executes operations in seconds. DBMS: Operates in milliseconds. Data Persistence: Python: Requires manual updating of temporary data structures. DBMS: Ensures automatic data persistence. Robustness: Python: Requires manual effort to ensure data robustness. DBMS: Provides automated backup, recovery, and restore features. Security: Python: Difficult to implement (relies on OS-level security). DBMS: Offers user-specific access control at the database level. Programmer Productivity: Python: Requires extensive coding for basic operations. DBMS: Provides standard queries, increasing programmer efficiency. Arithmetic Operations: Python: Easy to perform computations. DBMS: Limited set of arithmetic operations available. Costs: Python: Lower costs for hardware, software, and human resources. DBMS: Higher costs across all resources. Overall, DBMS appears to offer more robust, efficient, and scalable data management solutions, while Python file handling is simpler and less costly but with significant limitations for large-scale data operations. The Levels of Abstraction in database systems. key points: Physical Level: Describes how a record is physically stored Examples include blocks of storage Logical Level: Describes the data stored in a database Includes information about the relationships among data fields Covers attributes and data types of attributes View Level: Refers to application programs that hide details of data types Also hides information for security purposes Typically involves user interfaces The image includes a diagram illustrating the hierarchy of these levels: At the top is the View Level, which contains multiple views (view 1, view 2, ..., view n) Below the View Level is the Logical Level This abstraction model allows for separation of concerns in database design and management: The Physical Level deals with the actual storage mechanics The Logical Level focuses on data structure and relationships The View Level provides a user-friendly interface while maintaining security This layered approach enables efficient data management, security, and user interaction in database systems. The levels of abstraction shown in the image and general database knowledge: Physical Level Independence: This refers to the ability to change the physical storage structure or device without affecting the logical schema or application programs. It allows database administrators to optimize physical storage for performance without impacting how programmers or users interact with the database. For example, changing from one type of storage device to another, or reorganizing the file structures, should not require changes to the logical schema or application programs. Logical Level Independence: This refers to the ability to change the logical schema without affecting the external schemas or application programs. It allows database designers to modify the conceptual schema (add a table, add or delete attributes, etc.) without having to change the external views or rewrite application programs. For example, adding a new field to a table should not require changes to applications that don't use that field. The levels of abstraction shown in the image support these independences: The Physical Level deals with how data is actually stored. Physical independence means changes at this level don't affect the Logical Level above it. The Logical Level describes the structure and relationships of the data. Logical independence means changes at this level don't affect the View Level above it. The View Level, which includes multiple views, represents how different applications or users see the data. This level remains stable even when logical or physical levels change, thanks to data independence. These independences are crucial for database flexibility, maintenance, and evolution. They allow different aspects of the database to be modified without causing a ripple effect of changes throughout the entire system, thus saving time and reducing the risk of errors when updates are needed. Two key components of a database management system: the Storage Manager and Query Processing. Storage Manager: Acts as an interface between low-level data stored in the database and application programs/queries submitted to the system. Interacts with the operating system's file manager. Responsible for efficient storing, retrieving, and updating of data. Query Processing: Involves several steps: Parsing and translation, Optimization, and Evaluation. The process flow is illustrated in the diagram: a. Query is input b. Parser and translator convert it to a relational-algebra expression c. Optimizer creates an execution plan d. Evaluation engine processes the query using the execution plan e. Query output is produced The diagram also shows that the evaluation engine interacts with the data storage and uses statistics for optimization. This structure allows for efficient handling of database operations, separating the concerns of data storage from query processing and optimization. It enables the system to manage data effectively while providing fast and accurate responses to user queries. The different components shown in the diagram: Query: This is the starting point of the process. It represents the user's input, typically in the form of a SQL statement or another query language. Parser and Translator: This component takes the input query and converts it into a relational-algebra expression. It checks the syntax of the query, verifies that the referenced relations and attributes exist, and translates the query into an internal representation that the database system can work with. Relational-Algebra Expression: This is the output of the parser and translator. It's an internal representation of the query using relational algebra operations. Optimizer: The optimizer takes the relational-algebra expression and generates an efficient execution plan. It considers various factors like the size of relations, available indexes, and statistics about the data to determine the most efficient way to execute the query. Execution Plan: This is the output of the optimizer. It's a detailed step-by-step plan for how the query should be executed. Evaluation Engine: This component carries out the execution plan. It interacts with the data storage to retrieve, process, and manipulate the data as specified in the execution plan. Query Output: This is the final result of the query, which is returned to the user or application that initiated the query. Data: These are the actual data stored in the database, represented by cylinder shapes in the diagram. The evaluation engine interacts with this data to process the query. Statistics about Data: This component, represented by a cylinder, contains statistical information about the data in the database. The optimizer uses these statistics to make informed decisions about the best execution plan. The diagram shows how these components interact: The query flows through the parser and translator, optimizer, and evaluation engine. The optimizer uses statistics about the data to create the execution plan. The evaluation engine interacts with the actual data storage to process the query. The process results in the query output. This structure allows for efficient query processing by separating the stages of parsing, optimization, and execution, enabling each component to specialize in its task and contribute to overall system performance. Analyzing DocumentsAnalyzing DocumentsCertainly! Here is a detailed summary of the key concepts covered in the \"Week 1 - 4 summary slides.pdf\" document to help you prepare for your exam: Week 2: Database Schema and Keys Attribute Types: Domain: Set of allowed values for each attribute (e.g., alphanumeric string, date, number). Schema and Instances: Schema: Defines the structure of a relation (e.g., R = (A1, A2, ..., An)). Instance: Collection of information stored in the database at a particular moment. Keys: Super Key: A set of attributes that uniquely identifies a tuple. Candidate Key: A minimal super key. Primary Key: A selected candidate key. Surrogate Key: A unique identifier for an entity or object. Secondary/Alternate Key: Candidate keys other than the primary key. Simple Key: Consists of a single attribute. Composite Key: Consists of multiple attributes. Foreign Key: An attribute in one relation that appears in another. Relational Query Languages: Procedural Programming: Specifies how to get the output. Declarative Programming: Describes relationships between entities. Week 3: SQL and Relational Algebra Basic SQL Operations: Select Operation: Selection of rows (e.g., \u03c3D>5(r)). Project Operation: Selection of columns (e.g., \u03c0A,C(r)). Union: Union of two relations (e.g., r \u222a s). Difference: Set difference of two relations (e.g., r \u2212 s). Intersection: Set intersection of two relations (e.g., r \u2229 s). Cartesian Product: Joining two relations (e.g., r \u00d7 s). Natural Join: Matches tuples with the same values on common attributes (e.g., r \u25b7\u25c1 s). SQL Structure: Select Clause: Lists desired attributes. Where Clause: Specifies conditions. From Clause: Lists involved relations. Additional SQL Features: String Operations: Using LIKE with % and _ . Order By: Specifies sorting order. Set Operations: Union, intersect, except. Aggregate Functions: avg, min, max, sum, count. Group By and Having: Grouping and filtering groups. Nested Subqueries: Used in where, from, and select clauses for set membership and comparisons. Join Expressions: Cross Join: Cartesian product. Inner Join: Joins based on a condition. Equi-Join: Condition with equality. Natural Join: Based on common attributes. Outer Join: Includes non-matching tuples with null values. Self-Join: A table joined with itself. Views: Virtual relations defined using CREATE VIEW . Can hide certain data and be materialized. Integrity Constraints: Not Null, Primary Key, Unique, Check: Constraints on single relations. Referential Integrity: Ensures consistency across relations. Cascading Actions: ON DELETE CASCADE , ON UPDATE CASCADE . Built-in Data Types and User-Defined Types: SQL supports various data types and allows creating custom types and domains. Authorization and Roles: Using GRANT and REVOKE statements to manage permissions. Creating roles for user groups. SQL Functions and Procedures: Creating functions and procedures with control structures like loops and conditionals. Triggers for actions in response to data modifications. Week 4: Tuple and Domain Relational Calculus Tuple Relational Calculus (TRC): Nonprocedural query language with queries of the form {t | P(t)} . Uses logical conditions combined with OR (\u2228), AND (\u2227), NOT(\u00ac), and quantifiers (\u2203, \u2200). Domain Relational Calculus (DRC): Queries of the form {< x1, x2, ..., xn > | P(x1, x2, ..., xn)} . Uses domain variables and logical conditions. Example Queries: RA, TRC, DRC for students with age > 25 or enrolled in Maths: RA: \u03a0Name(\u03c3age>25\u2228subject=\"Maths\"(Students)) TRC: {t | \u2203s \u2208students(s.age > 25 \u2228 s.subject = \"Maths\" \u2227 t.name = s.name)} DRC: {< a >| \u2203b, c, d(< a, b, c, d >\u2208students \u2227 b > 25 \u2228 d = \"Maths\")} RA, TRC, DRC for students < 25 years old with marks > 75: RA: \u03a0Name,Sports,(\u03c3age<25\u2227Marks>75(Students \u22ca\u22c9Activity)) TRC: {t | \u2203s \u2208students \u2203a \u2208activity(s.name = a.name \u2227 s.age < 25 \u2227 s.marks > 75 \u2227 t.name = s.name \u2227 t.sports = a.sports)} DRC: {< a, f >| \u2203b, c, d(< a, b, c, d >\u2208students \u2227 b < 25 \u2227 c > 75) \u2227 \u2203e, g, h(< e, f, g, h >\u2208activity \u2227 a = e)} RA, TRC, DRC for students < 25 years old with > 3 awards: RA: \u03a0Name,Sports,(\u03c3age<25\u2227awards>3(Students \u22ca\u22c9Activity)) TRC: {t | \u2203s \u2208students \u2203a \u2208activity(s.name = a.name \u2227 s.age < 25 \u2227 a.awards > 3 \u2227 t.name = s.name \u2227 t.sports = a.sports)} DRC: {< a, f >| \u2203b, c, d(< a, b, c, d >\u2208students \u2227 b < 25) \u2227 \u2203e, g, h(< e, f, g, h >\u2208activity \u2227 g > 3 \u2227 a = e)} E-R Diagrams: Mapping Constraints: One-to-many, many-to-many, many-to-one, one-to-one relationships. Participation: Total and partial participation. This comprehensive summary should help you cover the essential topics for your exam preparation. Good luck!","title":"Quiz1 1"},{"location":"revision/quiz1_1/#week-1-database-management-systems-dbms-vs-file-handling-via-python","text":"A comparison between file handling via Python and Database Management Systems (DBMS) across various parameters. Scalability: Python: Struggles with handling large amounts of data and structural changes. DBMS: Offers built-in features for high scalability and easy structural modifications. Performance: Python: Executes operations in seconds. DBMS: Operates in milliseconds. Data Persistence: Python: Requires manual updating of temporary data structures. DBMS: Ensures automatic data persistence. Robustness: Python: Requires manual effort to ensure data robustness. DBMS: Provides automated backup, recovery, and restore features. Security: Python: Difficult to implement (relies on OS-level security). DBMS: Offers user-specific access control at the database level. Programmer Productivity: Python: Requires extensive coding for basic operations. DBMS: Provides standard queries, increasing programmer efficiency. Arithmetic Operations: Python: Easy to perform computations. DBMS: Limited set of arithmetic operations available. Costs: Python: Lower costs for hardware, software, and human resources. DBMS: Higher costs across all resources. Overall, DBMS appears to offer more robust, efficient, and scalable data management solutions, while Python file handling is simpler and less costly but with significant limitations for large-scale data operations. The Levels of Abstraction in database systems. key points: Physical Level: Describes how a record is physically stored Examples include blocks of storage Logical Level: Describes the data stored in a database Includes information about the relationships among data fields Covers attributes and data types of attributes View Level: Refers to application programs that hide details of data types Also hides information for security purposes Typically involves user interfaces The image includes a diagram illustrating the hierarchy of these levels: At the top is the View Level, which contains multiple views (view 1, view 2, ..., view n) Below the View Level is the Logical Level This abstraction model allows for separation of concerns in database design and management: The Physical Level deals with the actual storage mechanics The Logical Level focuses on data structure and relationships The View Level provides a user-friendly interface while maintaining security This layered approach enables efficient data management, security, and user interaction in database systems. The levels of abstraction shown in the image and general database knowledge: Physical Level Independence: This refers to the ability to change the physical storage structure or device without affecting the logical schema or application programs. It allows database administrators to optimize physical storage for performance without impacting how programmers or users interact with the database. For example, changing from one type of storage device to another, or reorganizing the file structures, should not require changes to the logical schema or application programs. Logical Level Independence: This refers to the ability to change the logical schema without affecting the external schemas or application programs. It allows database designers to modify the conceptual schema (add a table, add or delete attributes, etc.) without having to change the external views or rewrite application programs. For example, adding a new field to a table should not require changes to applications that don't use that field. The levels of abstraction shown in the image support these independences: The Physical Level deals with how data is actually stored. Physical independence means changes at this level don't affect the Logical Level above it. The Logical Level describes the structure and relationships of the data. Logical independence means changes at this level don't affect the View Level above it. The View Level, which includes multiple views, represents how different applications or users see the data. This level remains stable even when logical or physical levels change, thanks to data independence. These independences are crucial for database flexibility, maintenance, and evolution. They allow different aspects of the database to be modified without causing a ripple effect of changes throughout the entire system, thus saving time and reducing the risk of errors when updates are needed. Two key components of a database management system: the Storage Manager and Query Processing. Storage Manager: Acts as an interface between low-level data stored in the database and application programs/queries submitted to the system. Interacts with the operating system's file manager. Responsible for efficient storing, retrieving, and updating of data. Query Processing: Involves several steps: Parsing and translation, Optimization, and Evaluation. The process flow is illustrated in the diagram: a. Query is input b. Parser and translator convert it to a relational-algebra expression c. Optimizer creates an execution plan d. Evaluation engine processes the query using the execution plan e. Query output is produced The diagram also shows that the evaluation engine interacts with the data storage and uses statistics for optimization. This structure allows for efficient handling of database operations, separating the concerns of data storage from query processing and optimization. It enables the system to manage data effectively while providing fast and accurate responses to user queries. The different components shown in the diagram: Query: This is the starting point of the process. It represents the user's input, typically in the form of a SQL statement or another query language. Parser and Translator: This component takes the input query and converts it into a relational-algebra expression. It checks the syntax of the query, verifies that the referenced relations and attributes exist, and translates the query into an internal representation that the database system can work with. Relational-Algebra Expression: This is the output of the parser and translator. It's an internal representation of the query using relational algebra operations. Optimizer: The optimizer takes the relational-algebra expression and generates an efficient execution plan. It considers various factors like the size of relations, available indexes, and statistics about the data to determine the most efficient way to execute the query. Execution Plan: This is the output of the optimizer. It's a detailed step-by-step plan for how the query should be executed. Evaluation Engine: This component carries out the execution plan. It interacts with the data storage to retrieve, process, and manipulate the data as specified in the execution plan. Query Output: This is the final result of the query, which is returned to the user or application that initiated the query. Data: These are the actual data stored in the database, represented by cylinder shapes in the diagram. The evaluation engine interacts with this data to process the query. Statistics about Data: This component, represented by a cylinder, contains statistical information about the data in the database. The optimizer uses these statistics to make informed decisions about the best execution plan. The diagram shows how these components interact: The query flows through the parser and translator, optimizer, and evaluation engine. The optimizer uses statistics about the data to create the execution plan. The evaluation engine interacts with the actual data storage to process the query. The process results in the query output. This structure allows for efficient query processing by separating the stages of parsing, optimization, and execution, enabling each component to specialize in its task and contribute to overall system performance. Analyzing DocumentsAnalyzing DocumentsCertainly! Here is a detailed summary of the key concepts covered in the \"Week 1 - 4 summary slides.pdf\" document to help you prepare for your exam:","title":"Week 1: Database Management Systems (DBMS) vs. File Handling via Python"},{"location":"revision/quiz1_1/#week-2-database-schema-and-keys","text":"","title":"Week 2: Database Schema and Keys"},{"location":"revision/quiz1_1/#attribute-types","text":"Domain: Set of allowed values for each attribute (e.g., alphanumeric string, date, number).","title":"Attribute Types:"},{"location":"revision/quiz1_1/#schema-and-instances","text":"Schema: Defines the structure of a relation (e.g., R = (A1, A2, ..., An)). Instance: Collection of information stored in the database at a particular moment.","title":"Schema and Instances:"},{"location":"revision/quiz1_1/#keys","text":"Super Key: A set of attributes that uniquely identifies a tuple. Candidate Key: A minimal super key. Primary Key: A selected candidate key. Surrogate Key: A unique identifier for an entity or object. Secondary/Alternate Key: Candidate keys other than the primary key. Simple Key: Consists of a single attribute. Composite Key: Consists of multiple attributes. Foreign Key: An attribute in one relation that appears in another.","title":"Keys:"},{"location":"revision/quiz1_1/#relational-query-languages","text":"Procedural Programming: Specifies how to get the output. Declarative Programming: Describes relationships between entities.","title":"Relational Query Languages:"},{"location":"revision/quiz1_1/#week-3-sql-and-relational-algebra","text":"","title":"Week 3: SQL and Relational Algebra"},{"location":"revision/quiz1_1/#basic-sql-operations","text":"Select Operation: Selection of rows (e.g., \u03c3D>5(r)). Project Operation: Selection of columns (e.g., \u03c0A,C(r)). Union: Union of two relations (e.g., r \u222a s). Difference: Set difference of two relations (e.g., r \u2212 s). Intersection: Set intersection of two relations (e.g., r \u2229 s). Cartesian Product: Joining two relations (e.g., r \u00d7 s). Natural Join: Matches tuples with the same values on common attributes (e.g., r \u25b7\u25c1 s).","title":"Basic SQL Operations:"},{"location":"revision/quiz1_1/#sql-structure","text":"Select Clause: Lists desired attributes. Where Clause: Specifies conditions. From Clause: Lists involved relations.","title":"SQL Structure:"},{"location":"revision/quiz1_1/#additional-sql-features","text":"String Operations: Using LIKE with % and _ . Order By: Specifies sorting order. Set Operations: Union, intersect, except. Aggregate Functions: avg, min, max, sum, count. Group By and Having: Grouping and filtering groups.","title":"Additional SQL Features:"},{"location":"revision/quiz1_1/#nested-subqueries","text":"Used in where, from, and select clauses for set membership and comparisons.","title":"Nested Subqueries:"},{"location":"revision/quiz1_1/#join-expressions","text":"Cross Join: Cartesian product. Inner Join: Joins based on a condition. Equi-Join: Condition with equality. Natural Join: Based on common attributes. Outer Join: Includes non-matching tuples with null values. Self-Join: A table joined with itself.","title":"Join Expressions:"},{"location":"revision/quiz1_1/#views","text":"Virtual relations defined using CREATE VIEW . Can hide certain data and be materialized.","title":"Views:"},{"location":"revision/quiz1_1/#integrity-constraints","text":"Not Null, Primary Key, Unique, Check: Constraints on single relations. Referential Integrity: Ensures consistency across relations. Cascading Actions: ON DELETE CASCADE , ON UPDATE CASCADE .","title":"Integrity Constraints:"},{"location":"revision/quiz1_1/#built-in-data-types-and-user-defined-types","text":"SQL supports various data types and allows creating custom types and domains.","title":"Built-in Data Types and User-Defined Types:"},{"location":"revision/quiz1_1/#authorization-and-roles","text":"Using GRANT and REVOKE statements to manage permissions. Creating roles for user groups.","title":"Authorization and Roles:"},{"location":"revision/quiz1_1/#sql-functions-and-procedures","text":"Creating functions and procedures with control structures like loops and conditionals. Triggers for actions in response to data modifications.","title":"SQL Functions and Procedures:"},{"location":"revision/quiz1_1/#week-4-tuple-and-domain-relational-calculus","text":"","title":"Week 4: Tuple and Domain Relational Calculus"},{"location":"revision/quiz1_1/#tuple-relational-calculus-trc","text":"Nonprocedural query language with queries of the form {t | P(t)} . Uses logical conditions combined with OR (\u2228), AND (\u2227), NOT(\u00ac), and quantifiers (\u2203, \u2200).","title":"Tuple Relational Calculus (TRC):"},{"location":"revision/quiz1_1/#domain-relational-calculus-drc","text":"Queries of the form {< x1, x2, ..., xn > | P(x1, x2, ..., xn)} . Uses domain variables and logical conditions.","title":"Domain Relational Calculus (DRC):"},{"location":"revision/quiz1_1/#example-queries","text":"RA, TRC, DRC for students with age > 25 or enrolled in Maths: RA: \u03a0Name(\u03c3age>25\u2228subject=\"Maths\"(Students)) TRC: {t | \u2203s \u2208students(s.age > 25 \u2228 s.subject = \"Maths\" \u2227 t.name = s.name)} DRC: {< a >| \u2203b, c, d(< a, b, c, d >\u2208students \u2227 b > 25 \u2228 d = \"Maths\")} RA, TRC, DRC for students < 25 years old with marks > 75: RA: \u03a0Name,Sports,(\u03c3age<25\u2227Marks>75(Students \u22ca\u22c9Activity)) TRC: {t | \u2203s \u2208students \u2203a \u2208activity(s.name = a.name \u2227 s.age < 25 \u2227 s.marks > 75 \u2227 t.name = s.name \u2227 t.sports = a.sports)} DRC: {< a, f >| \u2203b, c, d(< a, b, c, d >\u2208students \u2227 b < 25 \u2227 c > 75) \u2227 \u2203e, g, h(< e, f, g, h >\u2208activity \u2227 a = e)} RA, TRC, DRC for students < 25 years old with > 3 awards: RA: \u03a0Name,Sports,(\u03c3age<25\u2227awards>3(Students \u22ca\u22c9Activity)) TRC: {t | \u2203s \u2208students \u2203a \u2208activity(s.name = a.name \u2227 s.age < 25 \u2227 a.awards > 3 \u2227 t.name = s.name \u2227 t.sports = a.sports)} DRC: {< a, f >| \u2203b, c, d(< a, b, c, d >\u2208students \u2227 b < 25) \u2227 \u2203e, g, h(< e, f, g, h >\u2208activity \u2227 g > 3 \u2227 a = e)}","title":"Example Queries:"},{"location":"revision/quiz1_1/#e-r-diagrams","text":"Mapping Constraints: One-to-many, many-to-many, many-to-one, one-to-one relationships. Participation: Total and partial participation. This comprehensive summary should help you cover the essential topics for your exam preparation. Good luck!","title":"E-R Diagrams:"},{"location":"week%206/Lecture%206.1%20-%20Relational%20Database%20Design6/","text":"Summary of Lecture 6.1 - Relational Database Design6.pdf Summary Objectives and Outline Objectives: Understand the significance of normal forms in relational design. Outline: Overview of normalization Normal forms First Normal Form (1NF) Second Normal Form (2NF) Third Normal Form (3NF) Normalization: An Overview Normalization is a technique used to decompose tables into smaller, more efficient ones to eliminate redundancy and undesirable characteristics, such as: Insertion Anomaly: Unable to add new records due to missing required data. Update Anomaly: Updating multiple records to maintain data consistency. Deletion Anomaly: Losing related data when deleting records. Normal Forms Normal forms specify constraints on relational schemas to ensure data integrity and reduce redundancy. First Normal Form (1NF) A relation is in 1NF if all attributes contain atomic values (no multivalued attributes). Second Normal Form (2NF) A relation is in 2NF if it is in 1NF and contains no partial dependency, where a non-prime attribute is functionally dependent on a proper subset of the candidate key. Third Normal Form (3NF) A relation is in 3NF if it is in 2NF and contains no transitive dependencies, where a non-prime attribute is functionally dependent on a non-key attribute, which in turn is functionally dependent on the candidate key. Additional Normal Forms Beyond 3NF, there are additional normal forms that offer higher levels of data integrity and efficiency, including: Elementary Key Normal Form (EKNF) Boyce-Codd Normal Form (BCNF) Fourth Normal Form (4NF) Essential Tuple Normal Form (ETNF) Join Dependencies and Fifth Normal Form (5NF) Sixth Normal Form (6NF) Domain/Key Normal Form (DKNF) Importance of Normalization Normalization helps to: Eliminate redundant data Ensure logical data storage Prevent data anomalies Improve data consistency and integrity Enhance database performance by reducing unnecessary joins and updates","title":"Summary of Lecture 6.1 - Relational Database Design6.pdf"},{"location":"week%206/Lecture%206.1%20-%20Relational%20Database%20Design6/#summary-of-lecture-61-relational-database-design6pdf","text":"Summary Objectives and Outline Objectives: Understand the significance of normal forms in relational design. Outline: Overview of normalization Normal forms First Normal Form (1NF) Second Normal Form (2NF) Third Normal Form (3NF) Normalization: An Overview Normalization is a technique used to decompose tables into smaller, more efficient ones to eliminate redundancy and undesirable characteristics, such as: Insertion Anomaly: Unable to add new records due to missing required data. Update Anomaly: Updating multiple records to maintain data consistency. Deletion Anomaly: Losing related data when deleting records. Normal Forms Normal forms specify constraints on relational schemas to ensure data integrity and reduce redundancy. First Normal Form (1NF) A relation is in 1NF if all attributes contain atomic values (no multivalued attributes). Second Normal Form (2NF) A relation is in 2NF if it is in 1NF and contains no partial dependency, where a non-prime attribute is functionally dependent on a proper subset of the candidate key. Third Normal Form (3NF) A relation is in 3NF if it is in 2NF and contains no transitive dependencies, where a non-prime attribute is functionally dependent on a non-key attribute, which in turn is functionally dependent on the candidate key. Additional Normal Forms Beyond 3NF, there are additional normal forms that offer higher levels of data integrity and efficiency, including: Elementary Key Normal Form (EKNF) Boyce-Codd Normal Form (BCNF) Fourth Normal Form (4NF) Essential Tuple Normal Form (ETNF) Join Dependencies and Fifth Normal Form (5NF) Sixth Normal Form (6NF) Domain/Key Normal Form (DKNF) Importance of Normalization Normalization helps to: Eliminate redundant data Ensure logical data storage Prevent data anomalies Improve data consistency and integrity Enhance database performance by reducing unnecessary joins and updates","title":"Summary of Lecture 6.1 - Relational Database Design6.pdf"},{"location":"week%206/Lecture%206.2%20-%20Relational%20Database%20Design7/","text":"Summary of Lecture 6.2 - Relational Database Design7.pdf Summary Module 27: Relational Database Design/7: Normal Forms Objectives: Learn the Decomposition Algorithm for a Relation to 3NF Learn the Decomposition Algorithm for a Relation to BCNF Outline: Decomposition to 3NF Test Algorithm Practice Problem Decomposition to BCNF Test Algorithm Practice Problem Comparison Module Summary Decomposition to 3NF Motivation: In certain situations, BCNF is not dependency preserving, and Efficient checking for FD violation on updates is important. Solution: define a weaker normal form, called Third Normal Form (3NF) Allows some redundancy (with resultant problems; as seen above) But functional dependencies can be checked on individual relations without computing a join There is always a lossless-join, dependency-preserving decomposition into 3NF. 3NF Definition: A relational schema R is in 3NF if for every FD X \u2192 A associated with R either A \u2286 X (that is, the FD is trivial) or X is a superkey of R or A is part of some candidate key (not just superkey!). A relation in 3NF is naturally in 2NF. Testing for 3NF: Optimization: Need to check only FDs in F, need not check all FDs in F+. Use attribute closure to check for each dependency \u03b1 \u2192 \u03b2, if \u03b1 is a superkey. If \u03b1 is not a superkey, we have to verify if each attribute in \u03b2 is contained in a candidate key of R. This test is rather more expensive, since it involves finding candidate keys. Testing for 3NF has been shown to be NP-hard. Decomposition into 3NF can be done in polynomial time. Algorithm: Given: relation R, set F of functional dependencies Find: decomposition of R into a set of 3NF relation Ri Algorithm: a) Eliminate redundant FDs, resulting in a canonical cover Fc of F. b) Create a relation Ri = XY for each FD X \u2192 Y in Fc. c) If the key K of R does not occur in any relation Ri, create one more relation Ri = K. Example: Relation schema: cust banker branch = (customer id, employee id, branch name, type) Functional dependencies: a) customer id, employee id \u2192 branch name, type b) employee id \u2192 branch name c) customer id, branch name \u2192 employee id The for loop generates following 3NF schema: (customer id, employee id, type) (employee id, branch name) (customer id, branch name, employee id) Practice Problems: R = ABCDEFGH FDs = {A \u2192 B, ABCD \u2192 E, EF \u2192 GH, ACDF \u2192 EG} R = CSJDPQV FDs = {C \u2192 CSJDPQV, SD \u2192 P, JP \u2192 C, J \u2192 S} R = ABCDEFGH F = {A \u2192 CD, ACF \u2192 G, AD \u2192 BEF, BCG \u2192 D, CF \u2192 AH, CH \u2192 G, D \u2192 B, H \u2192 DEG} R = ABCDE F = {A \u2192 B, A \u2192 C, C \u2192 D, A \u2192 E} R = ABCDE F = {A \u2192 BC, CD \u2192 E, B \u2192 D, E \u2192 A} R = ABCD F = {A \u2192 D, AB \u2192 C, AD \u2192 C, B \u2192 C, D \u2192 AB} Decomposition to BCNF BCNF Definition: A relation schema R is in BCNF with respect to a set F of FDs if for all FDs in F+ of the form \u03b1 \u2192 \u03b2, where \u03b1 \u2286 R and \u03b2 \u2286 R at least one of the following holds: \u03b1 \u2192 \u03b2 is trivial (that is, \u03b2 \u2286 \u03b1). \u03b1 is a superkey for R. Testing for BCNF: To check if a non-trivial dependency \u03b1 \u2192 \u03b2 causes a violation of BCNF a) Compute \u03b1+ (the attribute closure of \u03b1), and b) Verify that it includes all attributes of R, that is, it is a superkey of R. Simplified test: To check if a relation schema R is in BCNF, it suffices to check only the dependencies in the given set F for violation of BCNF, rather than checking all dependencies in F+. However, simplified test using only F is incorrect when testing a relation in a decomposition of R. Testing for BCNF Decomposition: To check if a relation Ri in a decomposition of R is in BCNF, Either test Ri for BCNF with respect to the restriction of F to Ri (that is, all FDs in F+ that contain only attributes from Ri), Or use the original set of dependencies F that hold on R, but with the following test: For every set of attributes \u03b1 \u2286 Ri, check that \u03b1+ (the attribute closure of \u03b1) Either includes no attribute of Ri \u2212 \u03b1, or Includes all attributes of Ri. If the condition is violated by some \u03b1 \u2192 \u03b2 in F, the dependency \u03b1 \u2192 (\u03b1+ \u2212 \u03b1) \u2229 Ri can be shown to hold on Ri, and Ri violates BCNF. We use above dependency to decompose Ri. Testing Dependency Preservation: Using Closure Set of FD (Exp. Algo.): Consider the example given below, we will apply both the algorithms to check dependency preservation and will discuss the results. R (A, B, C, D) F = {A \u2192 B, B \u2192 C, C \u2192 D, D \u2192 A} Decomposition: R1(A, B) R2(B, C) R3(C, D) A \u2192 B is preserved on table R1 B \u2192 C is preserved on table R2 C \u2192 D is preserved on table R3 We have to check whether the one remaining FD: D\u2192A is preserved or not. R1 F1={A \u2192 AB, B \u2192 BA} R2 F2={B \u2192 BC, C \u2192 CB} R3 F3={C \u2192 CD, D \u2192 DC} F0 = F1 \u222a F2 \u222a F3. Checking for: D \u2192 A in F0+ . D \u2192 C (from R3), C \u2192 B (from R2), B \u2192 A (from R1) : D\u2192 A (By Transitivity) Hence all dependencies are preserved. Using Closure of Attributes (Poly. Algo.): R(ABCD) :. F = {A \u2192 B, B \u2192 C, C \u2192 D, D \u2192 A} Decomp = {AB, BC, CD} On projections: (D) + /F1 = D. (D) + /F2 = D. (D) + /F3 = D. So, D \u2192 A could not be preserved. In the previous method we saw the dependency was preserved. In reality also it is preserved. Therefore the polynomial time algorithm may not work in case of all examples. To prove preservation, Algo 2 is sufficient but not necessary whereas Algo 1 is both sufficient as well as necessary. Algorithm: a) For all dependencies A \u2192 B in F+, check if A is a superkey - By using attribute closure b) If not, then - Choose a dependency in F+ that breaks the BCNF rules, say A \u2192 B - Create R1 = AB - Create R2 = (R \u2212 (B \u2212 A)) - Note that: R1 \u2229 R2 = A and A \u2192 AB (= R1), so this is lossless decomposition c) Repeat for R1, and R2 - By defining F1+ to be all dependencies in F that contain only attributes in R1 - Similarly F2+ Example: R = (A, B, C) F = {A \u2192 B, B \u2192 C} Key = {A} R is not in BCNF (B \u2192 C but B is not superkey) Decomposition R1 = (B, C) R2 = (A, B) Practice Problems: R = ABCDE. F = {A \u2192 B, BC \u2192 D} R = ABCDEH. F = {A \u2192 BC, E \u2192 HA} R = CSJDPQV. F = {C \u2192 CSJDPQV, SD \u2192 P, JP \u2192 C, J \u2192 S} R = ABCD. F = {C \u2192 D, C \u2192 A, B \u2192 C} Comparison of BCNF and 3NF | Feature | 3NF | B","title":"Summary of Lecture 6.2 - Relational Database Design7.pdf"},{"location":"week%206/Lecture%206.2%20-%20Relational%20Database%20Design7/#summary-of-lecture-62-relational-database-design7pdf","text":"Summary Module 27: Relational Database Design/7: Normal Forms Objectives: Learn the Decomposition Algorithm for a Relation to 3NF Learn the Decomposition Algorithm for a Relation to BCNF Outline: Decomposition to 3NF Test Algorithm Practice Problem Decomposition to BCNF Test Algorithm Practice Problem Comparison Module Summary Decomposition to 3NF Motivation: In certain situations, BCNF is not dependency preserving, and Efficient checking for FD violation on updates is important. Solution: define a weaker normal form, called Third Normal Form (3NF) Allows some redundancy (with resultant problems; as seen above) But functional dependencies can be checked on individual relations without computing a join There is always a lossless-join, dependency-preserving decomposition into 3NF. 3NF Definition: A relational schema R is in 3NF if for every FD X \u2192 A associated with R either A \u2286 X (that is, the FD is trivial) or X is a superkey of R or A is part of some candidate key (not just superkey!). A relation in 3NF is naturally in 2NF. Testing for 3NF: Optimization: Need to check only FDs in F, need not check all FDs in F+. Use attribute closure to check for each dependency \u03b1 \u2192 \u03b2, if \u03b1 is a superkey. If \u03b1 is not a superkey, we have to verify if each attribute in \u03b2 is contained in a candidate key of R. This test is rather more expensive, since it involves finding candidate keys. Testing for 3NF has been shown to be NP-hard. Decomposition into 3NF can be done in polynomial time. Algorithm: Given: relation R, set F of functional dependencies Find: decomposition of R into a set of 3NF relation Ri Algorithm: a) Eliminate redundant FDs, resulting in a canonical cover Fc of F. b) Create a relation Ri = XY for each FD X \u2192 Y in Fc. c) If the key K of R does not occur in any relation Ri, create one more relation Ri = K. Example: Relation schema: cust banker branch = (customer id, employee id, branch name, type) Functional dependencies: a) customer id, employee id \u2192 branch name, type b) employee id \u2192 branch name c) customer id, branch name \u2192 employee id The for loop generates following 3NF schema: (customer id, employee id, type) (employee id, branch name) (customer id, branch name, employee id) Practice Problems: R = ABCDEFGH FDs = {A \u2192 B, ABCD \u2192 E, EF \u2192 GH, ACDF \u2192 EG} R = CSJDPQV FDs = {C \u2192 CSJDPQV, SD \u2192 P, JP \u2192 C, J \u2192 S} R = ABCDEFGH F = {A \u2192 CD, ACF \u2192 G, AD \u2192 BEF, BCG \u2192 D, CF \u2192 AH, CH \u2192 G, D \u2192 B, H \u2192 DEG} R = ABCDE F = {A \u2192 B, A \u2192 C, C \u2192 D, A \u2192 E} R = ABCDE F = {A \u2192 BC, CD \u2192 E, B \u2192 D, E \u2192 A} R = ABCD F = {A \u2192 D, AB \u2192 C, AD \u2192 C, B \u2192 C, D \u2192 AB} Decomposition to BCNF BCNF Definition: A relation schema R is in BCNF with respect to a set F of FDs if for all FDs in F+ of the form \u03b1 \u2192 \u03b2, where \u03b1 \u2286 R and \u03b2 \u2286 R at least one of the following holds: \u03b1 \u2192 \u03b2 is trivial (that is, \u03b2 \u2286 \u03b1). \u03b1 is a superkey for R. Testing for BCNF: To check if a non-trivial dependency \u03b1 \u2192 \u03b2 causes a violation of BCNF a) Compute \u03b1+ (the attribute closure of \u03b1), and b) Verify that it includes all attributes of R, that is, it is a superkey of R. Simplified test: To check if a relation schema R is in BCNF, it suffices to check only the dependencies in the given set F for violation of BCNF, rather than checking all dependencies in F+. However, simplified test using only F is incorrect when testing a relation in a decomposition of R. Testing for BCNF Decomposition: To check if a relation Ri in a decomposition of R is in BCNF, Either test Ri for BCNF with respect to the restriction of F to Ri (that is, all FDs in F+ that contain only attributes from Ri), Or use the original set of dependencies F that hold on R, but with the following test: For every set of attributes \u03b1 \u2286 Ri, check that \u03b1+ (the attribute closure of \u03b1) Either includes no attribute of Ri \u2212 \u03b1, or Includes all attributes of Ri. If the condition is violated by some \u03b1 \u2192 \u03b2 in F, the dependency \u03b1 \u2192 (\u03b1+ \u2212 \u03b1) \u2229 Ri can be shown to hold on Ri, and Ri violates BCNF. We use above dependency to decompose Ri. Testing Dependency Preservation: Using Closure Set of FD (Exp. Algo.): Consider the example given below, we will apply both the algorithms to check dependency preservation and will discuss the results. R (A, B, C, D) F = {A \u2192 B, B \u2192 C, C \u2192 D, D \u2192 A} Decomposition: R1(A, B) R2(B, C) R3(C, D) A \u2192 B is preserved on table R1 B \u2192 C is preserved on table R2 C \u2192 D is preserved on table R3 We have to check whether the one remaining FD: D\u2192A is preserved or not. R1 F1={A \u2192 AB, B \u2192 BA} R2 F2={B \u2192 BC, C \u2192 CB} R3 F3={C \u2192 CD, D \u2192 DC} F0 = F1 \u222a F2 \u222a F3. Checking for: D \u2192 A in F0+ . D \u2192 C (from R3), C \u2192 B (from R2), B \u2192 A (from R1) : D\u2192 A (By Transitivity) Hence all dependencies are preserved. Using Closure of Attributes (Poly. Algo.): R(ABCD) :. F = {A \u2192 B, B \u2192 C, C \u2192 D, D \u2192 A} Decomp = {AB, BC, CD} On projections: (D) + /F1 = D. (D) + /F2 = D. (D) + /F3 = D. So, D \u2192 A could not be preserved. In the previous method we saw the dependency was preserved. In reality also it is preserved. Therefore the polynomial time algorithm may not work in case of all examples. To prove preservation, Algo 2 is sufficient but not necessary whereas Algo 1 is both sufficient as well as necessary. Algorithm: a) For all dependencies A \u2192 B in F+, check if A is a superkey - By using attribute closure b) If not, then - Choose a dependency in F+ that breaks the BCNF rules, say A \u2192 B - Create R1 = AB - Create R2 = (R \u2212 (B \u2212 A)) - Note that: R1 \u2229 R2 = A and A \u2192 AB (= R1), so this is lossless decomposition c) Repeat for R1, and R2 - By defining F1+ to be all dependencies in F that contain only attributes in R1 - Similarly F2+ Example: R = (A, B, C) F = {A \u2192 B, B \u2192 C} Key = {A} R is not in BCNF (B \u2192 C but B is not superkey) Decomposition R1 = (B, C) R2 = (A, B) Practice Problems: R = ABCDE. F = {A \u2192 B, BC \u2192 D} R = ABCDEH. F = {A \u2192 BC, E \u2192 HA} R = CSJDPQV. F = {C \u2192 CSJDPQV, SD \u2192 P, JP \u2192 C, J \u2192 S} R = ABCD. F = {C \u2192 D, C \u2192 A, B \u2192 C} Comparison of BCNF and 3NF | Feature | 3NF | B","title":"Summary of Lecture 6.2 - Relational Database Design7.pdf"},{"location":"week%206/Lecture%206.3%20-%20Relational%20Database%20Design8/","text":"Summary of Lecture 6.3 - Relational Database Design8.pdf Summary Introduction The objective of this module is to design a relational database schema for a Library Information System (LIS) of an Institute. The LIS should manage the books, the members, and the issue-return process. Entity Sets books : Each book has title, author, publisher, year of publication, ISBN number, and accession number. students : Each student has name, roll number, department, gender, mobile number, date of birth, and degree. faculty : Each faculty has name, employee id, department, gender, mobile number, and date of joining. members : Each member has a membership number and a member type (student, faculty, research scholar, etc.). quota : Each member type has a maximum quota for the number of books they can issue and the maximum duration for which they can issue books. staff : Library staff have a name, id, gender, mobile number, and date of joining. Relationships book issue : A book can be issued by a member. members : A member can belong to a member type and has a quota. students : A student is a member and has a roll number. faculty : A faculty is a member and has an employee id. staff : A staff member is a member and has an id. Relational Schema book catalogue : (title, author fname, author lname, publisher, year, ISBN no) book copies : (ISBN no, accession no) book issue : (member no, accession no, doi) quota : (member type, max books, max duration) members : (member no, member class, member type, roll no, id) students : (student fname, student lname, roll no, department, gender, mobile no, dob, degree) faculty : (faculty fname, faculty lname, id, department, gender, mobile no, doj) staff : (staff fname, staff lname, id, gender, mobile no, doj) Schema Refinement The initial relational schema is refined to ensure that it is in BCNF. The following steps are taken: The book catalogue relation is decomposed into two relations: book catalogue and book copies. The members relation is decomposed into two relations: members and students/faculty. Final Schema The final relational schema is as follows: book catalogue : (title, author fname, author lname, publisher, year, ISBN no) book copies : (ISBN no, accession no) book issue : (member no, accession no, doi) quota : (member type, max books, max duration) members : (member no, member class, member type, roll no, id) students : (student fname, student lname, roll no, department, gender, mobile no, dob, degree) faculty : (faculty fname, faculty lname, id, department, gender, mobile no, doj) staff : (staff fname, staff lname, id, gender, mobile no, doj) Module Summary The module illustrates how a schema can be designed and refined for an LIS. The final schema is in BCNF and supports the required queries.","title":"Summary of Lecture 6.3 - Relational Database Design8.pdf"},{"location":"week%206/Lecture%206.3%20-%20Relational%20Database%20Design8/#summary-of-lecture-63-relational-database-design8pdf","text":"Summary Introduction The objective of this module is to design a relational database schema for a Library Information System (LIS) of an Institute. The LIS should manage the books, the members, and the issue-return process. Entity Sets books : Each book has title, author, publisher, year of publication, ISBN number, and accession number. students : Each student has name, roll number, department, gender, mobile number, date of birth, and degree. faculty : Each faculty has name, employee id, department, gender, mobile number, and date of joining. members : Each member has a membership number and a member type (student, faculty, research scholar, etc.). quota : Each member type has a maximum quota for the number of books they can issue and the maximum duration for which they can issue books. staff : Library staff have a name, id, gender, mobile number, and date of joining. Relationships book issue : A book can be issued by a member. members : A member can belong to a member type and has a quota. students : A student is a member and has a roll number. faculty : A faculty is a member and has an employee id. staff : A staff member is a member and has an id. Relational Schema book catalogue : (title, author fname, author lname, publisher, year, ISBN no) book copies : (ISBN no, accession no) book issue : (member no, accession no, doi) quota : (member type, max books, max duration) members : (member no, member class, member type, roll no, id) students : (student fname, student lname, roll no, department, gender, mobile no, dob, degree) faculty : (faculty fname, faculty lname, id, department, gender, mobile no, doj) staff : (staff fname, staff lname, id, gender, mobile no, doj) Schema Refinement The initial relational schema is refined to ensure that it is in BCNF. The following steps are taken: The book catalogue relation is decomposed into two relations: book catalogue and book copies. The members relation is decomposed into two relations: members and students/faculty. Final Schema The final relational schema is as follows: book catalogue : (title, author fname, author lname, publisher, year, ISBN no) book copies : (ISBN no, accession no) book issue : (member no, accession no, doi) quota : (member type, max books, max duration) members : (member no, member class, member type, roll no, id) students : (student fname, student lname, roll no, department, gender, mobile no, dob, degree) faculty : (faculty fname, faculty lname, id, department, gender, mobile no, doj) staff : (staff fname, staff lname, id, gender, mobile no, doj) Module Summary The module illustrates how a schema can be designed and refined for an LIS. The final schema is in BCNF and supports the required queries.","title":"Summary of Lecture 6.3 - Relational Database Design8.pdf"},{"location":"week%206/Lecture%206.4%20-%20Relational%20Database%20Design9/","text":"Summary of Lecture 6.4 - Relational Database Design9.pdf Summary Module 29: Multivalued Dependency (MVD) and Decomposition to 4NF Objectives Understand multi-valued dependencies arising from attributes that can have multiple values Define Fourth Normal Form and learn the decomposition algorithm to 4NF Multivalued Dependency Definition A multivalued dependency (MVD) \u03b1 \u27f6 \u03b2 holds on a relation schema R if, for any two tuples t1 and t2 in R such that t1[\u03b1] = t2[\u03b1], there exist tuples t3 and t4 in R such that: t1[\u03b1] = t2[\u03b1] = t3[\u03b1] = t4[\u03b1] t3[\u03b2] = t1[\u03b2] t3[R \u2013 \u03b2] = t2[R \u2013 \u03b2] t4[\u03b2] = t2[\u03b2] t4[R \u2013 \u03b2] = t1[R \u2013 \u03b2] Example Consider a relation schema for a university course with the following attributes: ID Course Book Lecturer The following MVDs hold on this relation: ID \u27f6 Book ID \u27f6 Lecturer Use MVDs are used to: Test relations to determine if they are legal under a given set of functional and multivalued dependencies Specify constraints on the set of legal relations Theory If \u03b1 \u2192 \u03b2, then \u03b1 \u27f6 \u03b2 C-Complementation : If X \u27f6 Y, then X \u27f6 (R \u2212 (X \u222a Y)). A-Augmentation : If X \u27f6 Y and W \u2287 Z, then WX \u27f6 YZ. T-Transitivity : If X \u27f6 Y and Y \u27f6 Z, then X \u27f6 (Z \u2212 Y). Replication : If X \u2192 Y, then X \u27f6 Y but the reverse is not true. Coalescence : If X \u27f6 Y and there is a W such that W \u2229 Y is empty, W \u2192 Z, and Y \u2287 Z, then X \u2192 Z. Decomposition to 4NF Fourth Normal Form A relation schema R is in 4NF with respect to a set D of functional and multivalued dependencies if, for all multivalued dependencies in D+ of the form \u03b1 \u27f6 \u03b2, where \u03b1 \u2286 R and \u03b2 \u2286 R, at least one of the following hold: \u03b1 \u27f6 \u03b2 is trivial (that is, \u03b2 \u2286 \u03b1 or \u03b1 \u222a \u03b2 = R) \u03b1 is a superkey for schema R Decomposition Algorithm To decompose a relation schema R to 4NF: For all dependencies A \u27f6 B in D+, check if A is a superkey. If not, choose a dependency in F+ that breaks the 4NF rules, say A \u27f6 B. Create R1 = A B. Create R2 = (R \u2013 (B \u2013 A)). Note that: R1 \u2229 R2 = A and A \u27f6 AB (= R1), so this is a lossless decomposition. Repeat for R1 and R2.","title":"Summary of Lecture 6.4 - Relational Database Design9.pdf"},{"location":"week%206/Lecture%206.4%20-%20Relational%20Database%20Design9/#summary-of-lecture-64-relational-database-design9pdf","text":"","title":"Summary of Lecture 6.4 - Relational Database Design9.pdf"},{"location":"week%206/Lecture%206.4%20-%20Relational%20Database%20Design9/#summary","text":"Module 29: Multivalued Dependency (MVD) and Decomposition to 4NF","title":"Summary"},{"location":"week%206/Lecture%206.4%20-%20Relational%20Database%20Design9/#objectives","text":"Understand multi-valued dependencies arising from attributes that can have multiple values Define Fourth Normal Form and learn the decomposition algorithm to 4NF","title":"Objectives"},{"location":"week%206/Lecture%206.4%20-%20Relational%20Database%20Design9/#multivalued-dependency","text":"","title":"Multivalued Dependency"},{"location":"week%206/Lecture%206.4%20-%20Relational%20Database%20Design9/#definition","text":"A multivalued dependency (MVD) \u03b1 \u27f6 \u03b2 holds on a relation schema R if, for any two tuples t1 and t2 in R such that t1[\u03b1] = t2[\u03b1], there exist tuples t3 and t4 in R such that: t1[\u03b1] = t2[\u03b1] = t3[\u03b1] = t4[\u03b1] t3[\u03b2] = t1[\u03b2] t3[R \u2013 \u03b2] = t2[R \u2013 \u03b2] t4[\u03b2] = t2[\u03b2] t4[R \u2013 \u03b2] = t1[R \u2013 \u03b2]","title":"Definition"},{"location":"week%206/Lecture%206.4%20-%20Relational%20Database%20Design9/#example","text":"Consider a relation schema for a university course with the following attributes: ID Course Book Lecturer The following MVDs hold on this relation: ID \u27f6 Book ID \u27f6 Lecturer","title":"Example"},{"location":"week%206/Lecture%206.4%20-%20Relational%20Database%20Design9/#use","text":"MVDs are used to: Test relations to determine if they are legal under a given set of functional and multivalued dependencies Specify constraints on the set of legal relations","title":"Use"},{"location":"week%206/Lecture%206.4%20-%20Relational%20Database%20Design9/#theory","text":"If \u03b1 \u2192 \u03b2, then \u03b1 \u27f6 \u03b2 C-Complementation : If X \u27f6 Y, then X \u27f6 (R \u2212 (X \u222a Y)). A-Augmentation : If X \u27f6 Y and W \u2287 Z, then WX \u27f6 YZ. T-Transitivity : If X \u27f6 Y and Y \u27f6 Z, then X \u27f6 (Z \u2212 Y). Replication : If X \u2192 Y, then X \u27f6 Y but the reverse is not true. Coalescence : If X \u27f6 Y and there is a W such that W \u2229 Y is empty, W \u2192 Z, and Y \u2287 Z, then X \u2192 Z.","title":"Theory"},{"location":"week%206/Lecture%206.4%20-%20Relational%20Database%20Design9/#decomposition-to-4nf","text":"","title":"Decomposition to 4NF"},{"location":"week%206/Lecture%206.4%20-%20Relational%20Database%20Design9/#fourth-normal-form","text":"A relation schema R is in 4NF with respect to a set D of functional and multivalued dependencies if, for all multivalued dependencies in D+ of the form \u03b1 \u27f6 \u03b2, where \u03b1 \u2286 R and \u03b2 \u2286 R, at least one of the following hold: \u03b1 \u27f6 \u03b2 is trivial (that is, \u03b2 \u2286 \u03b1 or \u03b1 \u222a \u03b2 = R) \u03b1 is a superkey for schema R","title":"Fourth Normal Form"},{"location":"week%206/Lecture%206.4%20-%20Relational%20Database%20Design9/#decomposition-algorithm","text":"To decompose a relation schema R to 4NF: For all dependencies A \u27f6 B in D+, check if A is a superkey. If not, choose a dependency in F+ that breaks the 4NF rules, say A \u27f6 B. Create R1 = A B. Create R2 = (R \u2013 (B \u2013 A)). Note that: R1 \u2229 R2 = A and A \u27f6 AB (= R1), so this is a lossless decomposition. Repeat for R1 and R2.","title":"Decomposition Algorithm"},{"location":"week%206/Lecture%206.5%20-%20Relational%20Database%20Design10/","text":"Database Design Process : The goal is to achieve BCNF or 4NF, ensuring lossless join and dependency preservation. If these goals are not met, compromises like lack of dependency preservation or redundancy due to 3NF are accepted. Normal Forms and Normalization : The document discusses various normal forms including BCNF, 4NF, 5NF, and 6NF. It explains the importance of decomposing relations to eliminate multivalued dependencies (MVDs) and achieve higher normal forms. Denormalization : Sometimes denormalization is used for performance reasons, despite the potential for redundancy and update anomalies. Temporal Databases : Temporal data includes time-dependent or time-varying data, such as medical records, share prices, and exchange rates. Temporal databases store not only the data but also the time period during which the data is valid. There are two types of temporal relations: uni-temporal (valid time or transaction time) and bi-temporal (both valid time and transaction time). Modeling Temporal Data : Examples illustrate how temporal data can be modeled, showing the difference between non-temporal, uni-temporal, and bi-temporal databases. The document highlights the advantages (historical and rollback information) and disadvantages (more storage, complex queries, and maintenance) of bi-temporal databases. Examples and Case Studies : The document provides examples, such as the book catalog and John\u2019s address history, to illustrate the application of normalization and temporal data modeling. Summary : The module concludes with a recap of the database design process and the challenges associated with temporal data. This comprehensive overview covers the theoretical and practical aspects of relational database design, emphasizing the importance of normalization and the complexities introduced by temporal data.","title":"Lecture 6.5   Relational Database Design10"},{"location":"week%207/Lecture%207.1%20-%20Application%20Design%20and%20Development1/","text":"Summary of Lecture 7.1 - Application Design and Development1.pdf Summary Module 31: Application Design and Development Objectives and Outline Understand the diversity and commonality of application programs Classify and understand the evolution of application architectures Examine the architecture of sample applications Application Programs and Architecture Application Programs Application programs encompass a wide range of domains and functionalities, including: Financial: netbanking, share market, insurance, payment gateways Travel and tourism: travel reservations, accommodation, transportation, navigation Communication: live interaction, intermittent interaction, mail, social media Knowledge discovery: static, Q&A Sports: cricket, tennis Software engineering: issue tracking, VCS, online IDE Library: digital library, archives Education: eLearning, MOOCs Document processing: editing, website/blog Health: telemedicine, national healthcare apps Organizational ERP: students, faculty, course, patient, doctor, inventory, customers Characteristic of Application Programs Despite their diversity, application programs share commonalities: Diversity: Domain, functionality, user base, response time, scale Unity: Use relational database management systems (RDBMS) like Oracle, DB2 MySQL, PostgreSQL Functionally split into frontend, middle, and backend layers Application Architecture Frontend/Presentation Layer/Tier Interacts with the user: display/view, input/output Interfaces can be browser-based, mobile app, or custom Middle/Application/Business Logic Layer/Tier Implements the functionality of the application Links front and backend layers Authentication, search/browse logic, pricing, cart management, payment handling, order management, delivery management Backend/Data Access Layer/Tier Manages persistent data User, cart, inventory, order, vendor databases Application Architectures: Layers Presentation Layer / Tier: Model-View-Controller (MVC) architecture Model: business logic View: presentation of data, depends on display device Controller: receives events, executes actions, and returns a view to the user Business Logic Layer / Tier: Provides high level view of data and actions on data Often using an object data model Data Access Layer / Tier: Interfaces between business logic layer and the underlying database Provides mapping from object model of business layer to relational model of database Application Architecture (2): MVC MVC architecture separates the application into three components: Model: Business logic View: Presentation of data Controller: Handles user interactions and updates the view Application Architecture (3): User Interface Web browsers have become the de-facto user interface to databases Mobile devices are becoming increasingly popular Application Architecture (4): Business Logic Layer Provides abstractions of entities Enforces business rules for carrying out actions Supports workflows for carrying out tasks involving multiple participants Application Architecture (5): Object-Relational Mapping Allows application code to be written on top of object-oriented data model while storing data in a relational database Schema designer provides a mapping between object data and relational schema Application Architecture (6): Data Access Layer Issues of data access and update will be discussed later Architecture Classification 1-Tier Architecture All components (interface, middleware, back-end data) are located on a single server or platform 2-Tier Architecture Client-server architecture Direct communication between client and server 3-Tier Architecture Separates presentation, logic, and data access tiers Most widely used architecture for DBMS design n-Tier Architecture Distributes components of the 3 tiers between different servers and adds interfaces tiers Sample Applications in Multiple Tiers Financial: 3-tier Travel and tourism: 3-tier Communication: 3-tier Knowledge discovery: 2-tier Sports: 2-tier Software engineering: 3-tier Library: 3-tier Education: 3-tier Document processing: 3-tier Health: 3-tier Organizational ERP: 3-tier","title":"Summary of Lecture 7.1 - Application Design and Development1.pdf"},{"location":"week%207/Lecture%207.1%20-%20Application%20Design%20and%20Development1/#summary-of-lecture-71-application-design-and-development1pdf","text":"Summary Module 31: Application Design and Development Objectives and Outline Understand the diversity and commonality of application programs Classify and understand the evolution of application architectures Examine the architecture of sample applications Application Programs and Architecture Application Programs Application programs encompass a wide range of domains and functionalities, including: Financial: netbanking, share market, insurance, payment gateways Travel and tourism: travel reservations, accommodation, transportation, navigation Communication: live interaction, intermittent interaction, mail, social media Knowledge discovery: static, Q&A Sports: cricket, tennis Software engineering: issue tracking, VCS, online IDE Library: digital library, archives Education: eLearning, MOOCs Document processing: editing, website/blog Health: telemedicine, national healthcare apps Organizational ERP: students, faculty, course, patient, doctor, inventory, customers Characteristic of Application Programs Despite their diversity, application programs share commonalities: Diversity: Domain, functionality, user base, response time, scale Unity: Use relational database management systems (RDBMS) like Oracle, DB2 MySQL, PostgreSQL Functionally split into frontend, middle, and backend layers Application Architecture Frontend/Presentation Layer/Tier Interacts with the user: display/view, input/output Interfaces can be browser-based, mobile app, or custom Middle/Application/Business Logic Layer/Tier Implements the functionality of the application Links front and backend layers Authentication, search/browse logic, pricing, cart management, payment handling, order management, delivery management Backend/Data Access Layer/Tier Manages persistent data User, cart, inventory, order, vendor databases Application Architectures: Layers Presentation Layer / Tier: Model-View-Controller (MVC) architecture Model: business logic View: presentation of data, depends on display device Controller: receives events, executes actions, and returns a view to the user Business Logic Layer / Tier: Provides high level view of data and actions on data Often using an object data model Data Access Layer / Tier: Interfaces between business logic layer and the underlying database Provides mapping from object model of business layer to relational model of database Application Architecture (2): MVC MVC architecture separates the application into three components: Model: Business logic View: Presentation of data Controller: Handles user interactions and updates the view Application Architecture (3): User Interface Web browsers have become the de-facto user interface to databases Mobile devices are becoming increasingly popular Application Architecture (4): Business Logic Layer Provides abstractions of entities Enforces business rules for carrying out actions Supports workflows for carrying out tasks involving multiple participants Application Architecture (5): Object-Relational Mapping Allows application code to be written on top of object-oriented data model while storing data in a relational database Schema designer provides a mapping between object data and relational schema Application Architecture (6): Data Access Layer Issues of data access and update will be discussed later Architecture Classification 1-Tier Architecture All components (interface, middleware, back-end data) are located on a single server or platform 2-Tier Architecture Client-server architecture Direct communication between client and server 3-Tier Architecture Separates presentation, logic, and data access tiers Most widely used architecture for DBMS design n-Tier Architecture Distributes components of the 3 tiers between different servers and adds interfaces tiers Sample Applications in Multiple Tiers Financial: 3-tier Travel and tourism: 3-tier Communication: 3-tier Knowledge discovery: 2-tier Sports: 2-tier Software engineering: 3-tier Library: 3-tier Education: 3-tier Document processing: 3-tier Health: 3-tier Organizational ERP: 3-tier","title":"Summary of Lecture 7.1 - Application Design and Development1.pdf"},{"location":"week%207/Lecture%207.2%20-%20Application%20Design%20and%20Development2/","text":"Web Fundamentals Summary Web Fundamentals The World Wide Web, a distributed information system built upon the foundation of hypertext, takes center stage. Hypertext documents, primarily formatted using the HyperText Markup Language (HTML), serve as the building blocks of the web. HTML empowers developers to structure content, specify font styles and formatting instructions, embed hypertext links connecting to other documents, and design forms enabling user input, which can be transmitted back to the web server for processing. Uniform Resource Locators (URLs) provide a standardized mechanism for locating resources on the web. A URL comprises several components: the protocol used to access the document (e.g., HTTP), the domain name of the web server hosting the document, and the path to the specific resource on the server. URLs can be further categorized as Uniform Resource Identifiers (URIs) or Uniform Resource Names (URNs), with URIs encompassing both locators (URLs) and names (URNs). HTML and HTTP, two cornerstone technologies of the web, are then brought into focus. HTML provides the structural foundation for web pages, enabling the inclusion of text, multimedia elements, hyperlinks, and forms. HTTP, on the other hand, serves as the communication protocol between web browsers and web servers, facilitating the exchange of requests and responses. The concept of sessions and cookies is introduced to address the stateless nature of the HTTP protocol. Cookies, small text files stored on the user's computer by the web server, enable the persistence of information across multiple interactions with a website. This mechanism is commonly employed for user authentication, session management, and personalization. Web browsers, the user's gateway to the World Wide Web, are software applications designed to retrieve, interpret, and display web pages. They handle the rendering of HTML, CSS, and JavaScript code, providing an interactive and visually appealing representation of web content. Web servers, on the other hand, are responsible for storing, processing, and delivering web pages and other resources in response to requests from web browsers. Web services, a key enabler of distributed computing, allow applications to communicate and exchange data over a network, typically using the HTTP protocol and XML or JSON as data formats. Scripting for Web Applications Scripting, a programming paradigm that involves embedding executable code within a web page or on the server, is introduced as a powerful technique for enhancing the functionality and interactivity of web applications. Scripts can be executed on the client-side (by the user's web browser) or on the server-side (by the web server). Client-side scripting, often implemented using JavaScript, focuses on enhancing the user interface and providing dynamic interactions within the web browser. JavaScript code can manipulate the Document Object Model (DOM) of a web page, validate user input, make asynchronous requests to the web server, and create rich, interactive elements. Server-side scripting, on the other hand, involves executing code on the web server before sending the response back to the client. This paradigm is commonly used for tasks such as database interaction, business logic processing, and dynamic content generation. Popular server-side scripting languages include PHP, Python, Ruby, and Java Servlets. Servlets Servlets, Java programs that run within a web server environment, provide a robust and scalable framework for building server-side web applications. They handle client requests, process data, interact with databases, and generate dynamic responses. The Java Servlet API defines a set of interfaces and classes that facilitate communication between servlets and the web server. JavaServer Pages (JSP) JavaServer Pages (JSP) provide a simplified approach to creating dynamic web pages by embedding Java code within HTML markup. JSP pages are compiled into servlets by the web server, enabling developers to leverage the power and portability of the Java platform for web development. PHP PHP, a widely-used server-side scripting language, is highlighted for its ease of use, extensive library support, and strong database connectivity features. PHP code can be embedded within HTML markup, allowing for the creation of dynamic web pages with minimal effort. The lecture concludes by reiterating the key takeaways, emphasizing the fundamental concepts of web technologies, scripting, and servlets. It underscores the importance of understanding these technologies for building robust, scalable, and interactive web applications.","title":"Web Fundamentals"},{"location":"week%207/Lecture%207.2%20-%20Application%20Design%20and%20Development2/#web-fundamentals","text":"Summary Web Fundamentals The World Wide Web, a distributed information system built upon the foundation of hypertext, takes center stage. Hypertext documents, primarily formatted using the HyperText Markup Language (HTML), serve as the building blocks of the web. HTML empowers developers to structure content, specify font styles and formatting instructions, embed hypertext links connecting to other documents, and design forms enabling user input, which can be transmitted back to the web server for processing. Uniform Resource Locators (URLs) provide a standardized mechanism for locating resources on the web. A URL comprises several components: the protocol used to access the document (e.g., HTTP), the domain name of the web server hosting the document, and the path to the specific resource on the server. URLs can be further categorized as Uniform Resource Identifiers (URIs) or Uniform Resource Names (URNs), with URIs encompassing both locators (URLs) and names (URNs). HTML and HTTP, two cornerstone technologies of the web, are then brought into focus. HTML provides the structural foundation for web pages, enabling the inclusion of text, multimedia elements, hyperlinks, and forms. HTTP, on the other hand, serves as the communication protocol between web browsers and web servers, facilitating the exchange of requests and responses. The concept of sessions and cookies is introduced to address the stateless nature of the HTTP protocol. Cookies, small text files stored on the user's computer by the web server, enable the persistence of information across multiple interactions with a website. This mechanism is commonly employed for user authentication, session management, and personalization. Web browsers, the user's gateway to the World Wide Web, are software applications designed to retrieve, interpret, and display web pages. They handle the rendering of HTML, CSS, and JavaScript code, providing an interactive and visually appealing representation of web content. Web servers, on the other hand, are responsible for storing, processing, and delivering web pages and other resources in response to requests from web browsers. Web services, a key enabler of distributed computing, allow applications to communicate and exchange data over a network, typically using the HTTP protocol and XML or JSON as data formats. Scripting for Web Applications Scripting, a programming paradigm that involves embedding executable code within a web page or on the server, is introduced as a powerful technique for enhancing the functionality and interactivity of web applications. Scripts can be executed on the client-side (by the user's web browser) or on the server-side (by the web server). Client-side scripting, often implemented using JavaScript, focuses on enhancing the user interface and providing dynamic interactions within the web browser. JavaScript code can manipulate the Document Object Model (DOM) of a web page, validate user input, make asynchronous requests to the web server, and create rich, interactive elements. Server-side scripting, on the other hand, involves executing code on the web server before sending the response back to the client. This paradigm is commonly used for tasks such as database interaction, business logic processing, and dynamic content generation. Popular server-side scripting languages include PHP, Python, Ruby, and Java Servlets. Servlets Servlets, Java programs that run within a web server environment, provide a robust and scalable framework for building server-side web applications. They handle client requests, process data, interact with databases, and generate dynamic responses. The Java Servlet API defines a set of interfaces and classes that facilitate communication between servlets and the web server. JavaServer Pages (JSP) JavaServer Pages (JSP) provide a simplified approach to creating dynamic web pages by embedding Java code within HTML markup. JSP pages are compiled into servlets by the web server, enabling developers to leverage the power and portability of the Java platform for web development. PHP PHP, a widely-used server-side scripting language, is highlighted for its ease of use, extensive library support, and strong database connectivity features. PHP code can be embedded within HTML markup, allowing for the creation of dynamic web pages with minimal effort. The lecture concludes by reiterating the key takeaways, emphasizing the fundamental concepts of web technologies, scripting, and servlets. It underscores the importance of understanding these technologies for building robust, scalable, and interactive web applications.","title":"Web Fundamentals"},{"location":"week%207/Lecture%207.4%20-%20Application%20Design%20and%20Development4/","text":"Summary of Lecture 7.4 - Application Design and Development4.pdf Summary Module 34: Application Design and Development/4: Python and PostgreSQL Objectives To understand how to access PostgreSQL database from Python To understand Python Web Application with PostgresSQL Outline Accessing PostgreSQL from Python Developing Web Application with Python Working with PostgreSQL and Python Python Modules for PostgreSQL psycopg2 pg8000 py-postgresql PyGreSQL ocpgdb bpgsql SQLAlchemy (needs any of the above to be installed separately) Package psycopg2 Advantages of psycopg2 Most popular and stable module to work with PostgreSQL Used in most of the Python and Postgres frameworks An actively maintained package and supports Python 2.x and 3.x Thread-safe and designed for heavily multi-threaded applications. Steps to access PostgresSQL from Python Create connection Create cursor Execute the query Commit/rollback Close the cursor Close the connection Python psycopg2 Module APIs: connection objects psycopg2.connect(database=\"mydb\", user=\"myuser\", password=\"mypass\" host=\"127.0.0.1\", port=\"5432\") This API opens a connection to the PostgreSQL database. If database is opened successfully, it returns a connection object. connection.close() This method closes the database connection. Important psycopg2 module routines for managing cursor object: connection.cursor() This routine creates a cursor which will be used throughout the program. cursor.close() This method closes the cursor. Python psycopg2 Module APIs: insert, delete, update & stored procedures cursor.execute(sql [, optional parameters]) This routine executes an SQL statement. The SQL statement may be parameterized (i.e., placeholders instead of SQL literals). The psycopg2 module supports placeholder using %s sign. For example: cursor.execute(\"insert into people values (%s,\\r\\n %s)\", (who, age)) cursor.executemany(sql, seq of parameters) This routine executes an SQL command against all parameter sequences or mappings found in the sequence SQL. cursor.callproc(procname[, parameters]) This routine executes a stored database procedure with the given name. The sequence of parameters must contain one entry for each argument that the procedure expects. cursor.rowcount This is a read-only attribute which returns the total number of database rows that have been modified, inserted, or deleted by the last execute(). Python psycopg2 Module APIs: select cursor.fetchone() This method fetches the next row of a query result set, returning a single sequence, or None when no more data is available. cursor.fetchmany([size=cursor.arraysize]) This routine fetches the next set of rows of a query result, returning a list. An empty list is returned when no more rows are available. The method tries to fetch as many rows as indicated by the size parameter. cursor.fetchall() This routine fetches all (remaining) rows of a query result, returning a list. An empty list is returned when no rows are available. Python psycopg2 Module APIs: commit & rollback connection.commit() This method commits the current transaction. If you do not call this method, anything you did since the last call to commit() is not visible to other database connections. connection.rollback() This method rolls back any changes to the database since the last call to commit(). Connect to a PostgreSQL Database Server import psycopg2 def connectDb ( dbname , usrname , pwd , address , portnum ): conn = None try : # connect to the PostgreSQL database conn = psycopg2 . connect ( database = dbname , user = usrname , \\ password = pwd , host = address , port = portnum ) print ( \"Database connected successfully\" ) except ( Exception , psycopg2 . DatabaseError ) as error : print ( error ) finally : conn . close () # close the connection connectDb ( \"mydb\" , \"myuser\" , \"mypass\" , \"127.0.0.1\" , \"5432\" ) # function call Steps to execute SQL commands Use the psycopg2.connect() method with the required arguments to connect Post gresSQL. It would return an Connection object if the connection established successfully. Create a cursor object using the cursor() method of connection object. The execute() methods run the SQL commands and return the result. Use cursor.fetchall() or fetchone() or fetchmany() to read query result. Use commit() to make the changes in database persistent, or use rollback() to revert the database changes. Use cursor.close() and connection.close() method to close the cursor and Post gresSQL connection. CREATE new PostgreSQL tables import psycopg2 def createTable (): conn = None try : conn = psycopg2 . connect ( database = \"mydb\" , user = \"myuser\" , \\ password = \"mypass\" , host = \"127.0.0.1\" , port = \"5432\" ) # connect to the database cur = conn . cursor () # create a new cursor cur . execute ( '''CREATE TABLE EMPLOYEE \\ (emp_num INT PRIMARY KEY NOT NULL, \\ emp_name VARCHAR(40) NOT NULL, \\ department VARCHAR(40) NOT NULL)''' ) # execute the CREATE TABLE statement conn . commit () # commit the changes to the database print ( \"Table created successfully\" ) cur . close () # close the cursor except ( Exception , psycopg2 . DatabaseError ) as error : print ( error ) finally : if conn is not None : conn . close () # close the connection createTable () #function call Executing INSERT statement from Python import psycopg2 def insertRecord ( num , name , dept ): conn = None try : # connect to the PostgreSQL database conn = psycopg2 . connect ( database = \"mydb\" , user = \"myuser\" , \\ password = \"mypass\" , host = \"127.0.0.1\" , port = \"5432\" ) cur = conn . cursor () # create a new cursor # execute the INSERT statement cur . execute ( \"INSERT INTO EMPLOYEE (emp_num, emp_name, department) \\ VALUES ( %s , %s , %s )\" , ( num , name , dept )) conn . commit () # commit the changes to the database print ( \"Total number of rows inserted :\" , cur . rowcount ); cur . close () # close the cursor except ( Exception , psycopg2 . DatabaseError ) as error : print ( error ) finally : if conn is not None : conn . close () # close the connection insertRecord ( 110 , \u2018 Bhaskar \u2019 , \u2019 HR \u2019 ) #function call Executing DELETE statement from Python import psycopg2 def deleteRecord ( num ): conn = None try : # connect to the PostgreSQL database conn = psycopg2 . connect ( database = \"mydb\" , user = \"myuser\" , \\ password = \"mypass\" , host = \"127.0.0.1\" , port = \"5432\" ) cur = conn . cursor () # create a new cursor # execute the DELETE statement cur . execute ( \"DELETE FROM EMPLOYEE WHERE emp_num = %s \" , ( num ,)) conn . commit () # commit the changes to the database print ( \"Total number of rows deleted :\" , cur . rowcount ) cur . close () # close the cursor except ( Exception , psycopg2 . DatabaseError ) as error : print ( error ) finally : conn . close () # close the connection deleteRecord ( 110 ) #function call Executing UPDATE statement from Python import psycopg2 def updateRecord ( num , dept ): conn = None try : # connect to the PostgreSQL database conn = psycopg2 . connect ( database = \"mydb\" , user = \"myuser\" , \\ password = \"mypass\" , host = \"127.0.0.1\" , port = \"5432\" ) cur = conn . cursor () # create a new cursor # execute the UPDATE statement cur . execute ( \"UPDATE EMPLOYEE set department = %s where emp_num = \\ %s \" , ( dept , num )) conn . commit () # commit the changes to the database print ( \"Total number of rows updated :\" , cur . rowcount ) cur . close () # close the cursor except ( Exception , psycopg2 . DatabaseError ) as error : print ( error ) finally : conn . close () # close the connection updateRecord ( 110 , \"Finance\" ) #function call Executing SELECT statement from Python ```python import psycopg2 def selectAll():","title":"Summary of Lecture 7.4 - Application Design and Development4.pdf"},{"location":"week%207/Lecture%207.4%20-%20Application%20Design%20and%20Development4/#summary-of-lecture-74-application-design-and-development4pdf","text":"Summary Module 34: Application Design and Development/4: Python and PostgreSQL Objectives To understand how to access PostgreSQL database from Python To understand Python Web Application with PostgresSQL Outline Accessing PostgreSQL from Python Developing Web Application with Python Working with PostgreSQL and Python Python Modules for PostgreSQL psycopg2 pg8000 py-postgresql PyGreSQL ocpgdb bpgsql SQLAlchemy (needs any of the above to be installed separately) Package psycopg2 Advantages of psycopg2 Most popular and stable module to work with PostgreSQL Used in most of the Python and Postgres frameworks An actively maintained package and supports Python 2.x and 3.x Thread-safe and designed for heavily multi-threaded applications. Steps to access PostgresSQL from Python Create connection Create cursor Execute the query Commit/rollback Close the cursor Close the connection Python psycopg2 Module APIs: connection objects psycopg2.connect(database=\"mydb\", user=\"myuser\", password=\"mypass\" host=\"127.0.0.1\", port=\"5432\") This API opens a connection to the PostgreSQL database. If database is opened successfully, it returns a connection object. connection.close() This method closes the database connection. Important psycopg2 module routines for managing cursor object: connection.cursor() This routine creates a cursor which will be used throughout the program. cursor.close() This method closes the cursor. Python psycopg2 Module APIs: insert, delete, update & stored procedures cursor.execute(sql [, optional parameters]) This routine executes an SQL statement. The SQL statement may be parameterized (i.e., placeholders instead of SQL literals). The psycopg2 module supports placeholder using %s sign. For example: cursor.execute(\"insert into people values (%s,\\r\\n %s)\", (who, age)) cursor.executemany(sql, seq of parameters) This routine executes an SQL command against all parameter sequences or mappings found in the sequence SQL. cursor.callproc(procname[, parameters]) This routine executes a stored database procedure with the given name. The sequence of parameters must contain one entry for each argument that the procedure expects. cursor.rowcount This is a read-only attribute which returns the total number of database rows that have been modified, inserted, or deleted by the last execute(). Python psycopg2 Module APIs: select cursor.fetchone() This method fetches the next row of a query result set, returning a single sequence, or None when no more data is available. cursor.fetchmany([size=cursor.arraysize]) This routine fetches the next set of rows of a query result, returning a list. An empty list is returned when no more rows are available. The method tries to fetch as many rows as indicated by the size parameter. cursor.fetchall() This routine fetches all (remaining) rows of a query result, returning a list. An empty list is returned when no rows are available. Python psycopg2 Module APIs: commit & rollback connection.commit() This method commits the current transaction. If you do not call this method, anything you did since the last call to commit() is not visible to other database connections. connection.rollback() This method rolls back any changes to the database since the last call to commit(). Connect to a PostgreSQL Database Server import psycopg2 def connectDb ( dbname , usrname , pwd , address , portnum ): conn = None try : # connect to the PostgreSQL database conn = psycopg2 . connect ( database = dbname , user = usrname , \\ password = pwd , host = address , port = portnum ) print ( \"Database connected successfully\" ) except ( Exception , psycopg2 . DatabaseError ) as error : print ( error ) finally : conn . close () # close the connection connectDb ( \"mydb\" , \"myuser\" , \"mypass\" , \"127.0.0.1\" , \"5432\" ) # function call Steps to execute SQL commands Use the psycopg2.connect() method with the required arguments to connect Post gresSQL. It would return an Connection object if the connection established successfully. Create a cursor object using the cursor() method of connection object. The execute() methods run the SQL commands and return the result. Use cursor.fetchall() or fetchone() or fetchmany() to read query result. Use commit() to make the changes in database persistent, or use rollback() to revert the database changes. Use cursor.close() and connection.close() method to close the cursor and Post gresSQL connection. CREATE new PostgreSQL tables import psycopg2 def createTable (): conn = None try : conn = psycopg2 . connect ( database = \"mydb\" , user = \"myuser\" , \\ password = \"mypass\" , host = \"127.0.0.1\" , port = \"5432\" ) # connect to the database cur = conn . cursor () # create a new cursor cur . execute ( '''CREATE TABLE EMPLOYEE \\ (emp_num INT PRIMARY KEY NOT NULL, \\ emp_name VARCHAR(40) NOT NULL, \\ department VARCHAR(40) NOT NULL)''' ) # execute the CREATE TABLE statement conn . commit () # commit the changes to the database print ( \"Table created successfully\" ) cur . close () # close the cursor except ( Exception , psycopg2 . DatabaseError ) as error : print ( error ) finally : if conn is not None : conn . close () # close the connection createTable () #function call Executing INSERT statement from Python import psycopg2 def insertRecord ( num , name , dept ): conn = None try : # connect to the PostgreSQL database conn = psycopg2 . connect ( database = \"mydb\" , user = \"myuser\" , \\ password = \"mypass\" , host = \"127.0.0.1\" , port = \"5432\" ) cur = conn . cursor () # create a new cursor # execute the INSERT statement cur . execute ( \"INSERT INTO EMPLOYEE (emp_num, emp_name, department) \\ VALUES ( %s , %s , %s )\" , ( num , name , dept )) conn . commit () # commit the changes to the database print ( \"Total number of rows inserted :\" , cur . rowcount ); cur . close () # close the cursor except ( Exception , psycopg2 . DatabaseError ) as error : print ( error ) finally : if conn is not None : conn . close () # close the connection insertRecord ( 110 , \u2018 Bhaskar \u2019 , \u2019 HR \u2019 ) #function call Executing DELETE statement from Python import psycopg2 def deleteRecord ( num ): conn = None try : # connect to the PostgreSQL database conn = psycopg2 . connect ( database = \"mydb\" , user = \"myuser\" , \\ password = \"mypass\" , host = \"127.0.0.1\" , port = \"5432\" ) cur = conn . cursor () # create a new cursor # execute the DELETE statement cur . execute ( \"DELETE FROM EMPLOYEE WHERE emp_num = %s \" , ( num ,)) conn . commit () # commit the changes to the database print ( \"Total number of rows deleted :\" , cur . rowcount ) cur . close () # close the cursor except ( Exception , psycopg2 . DatabaseError ) as error : print ( error ) finally : conn . close () # close the connection deleteRecord ( 110 ) #function call Executing UPDATE statement from Python import psycopg2 def updateRecord ( num , dept ): conn = None try : # connect to the PostgreSQL database conn = psycopg2 . connect ( database = \"mydb\" , user = \"myuser\" , \\ password = \"mypass\" , host = \"127.0.0.1\" , port = \"5432\" ) cur = conn . cursor () # create a new cursor # execute the UPDATE statement cur . execute ( \"UPDATE EMPLOYEE set department = %s where emp_num = \\ %s \" , ( dept , num )) conn . commit () # commit the changes to the database print ( \"Total number of rows updated :\" , cur . rowcount ) cur . close () # close the cursor except ( Exception , psycopg2 . DatabaseError ) as error : print ( error ) finally : conn . close () # close the connection updateRecord ( 110 , \"Finance\" ) #function call Executing SELECT statement from Python ```python import psycopg2 def selectAll():","title":"Summary of Lecture 7.4 - Application Design and Development4.pdf"},{"location":"week%207/Lecture%207.5%20-%20Application%20Design%20and%20Development5/","text":"Summary of Lecture 7.5 - Application Design and Development5.pdf Summary Rapid Application Development Rapid Application Development (RAD) aims to accelerate application development. It employs techniques such as: Function libraries for generating user interface elements Drag-and-drop features in IDEs Automatic code generation from declarative specifications RAD tools include G Suite, Google App Engine, Microsoft Azure, Amazon Elastic Compute Cloud (EC2), and AWS Elastic Beanstalk. ASP.NET and Visual Studio provide drag-and-drop development using server-interpreted controls. Validators can be added to form input fields, and data can be displayed in tabular format using DataGrid. Application Performance and Security Application Performance Caching techniques are used to reduce the cost of serving pages by exploiting commonalities between requests. Caching can be implemented at the server site (e.g., JDBC connection pooling, query result caching, HTML caching) or at the client's network (e.g., page caching by Web proxy). Application Security SQL Injection: Constructing queries using user inputs without proper parameterization can lead to SQL injection attacks. Password Leakage: Passwords should never be stored in clear text, and database access should be restricted by IP address. Authentication: Single-factor authentication (e.g., passwords) is risky. Two-factor authentication (e.g., password plus one-time password) is more secure. Application-Level Authorization: Current SQL standards do not allow fine-grained authorization. Workarounds include using views or extensions like Oracle Virtual Private Database (VPD). Audit Trails: Applications should log actions to detect security breaches and identify perpetrators. Audit trails are needed at both database and application levels. Challenges in Web Application Development User Interface and User Experience Scalability Performance Knowledge of Framework and Platforms Security Mobile Apps Definition: Software applications designed for mobile devices (e.g., smartphones, tablets). Characteristics: Designed for small, wireless devices Limited form factor, memory, computing power, power, and bandwidth May include sensors like accelerometers and touchscreens Types of Mobile Apps: Native Apps: Written in the native language of a platform (e.g., Objective-C for iOS, Java for Android). Platform-specific. Web Apps: Run inside Web browsers. Feature HTML/CSS interfaces and are powered by Web programming languages. Portable across devices. Hybrid Apps: Combine attributes of both native and Web apps. Use common code across platforms while tailoring required attributes to the native system. Design Issues for Mobile Apps: Determine device and resources Consider bandwidth Decide on architecture layers Select technology Define user interface Select navigation Maintain flow","title":"Summary of Lecture 7.5 - Application Design and Development5.pdf"},{"location":"week%207/Lecture%207.5%20-%20Application%20Design%20and%20Development5/#summary-of-lecture-75-application-design-and-development5pdf","text":"Summary Rapid Application Development Rapid Application Development (RAD) aims to accelerate application development. It employs techniques such as: Function libraries for generating user interface elements Drag-and-drop features in IDEs Automatic code generation from declarative specifications RAD tools include G Suite, Google App Engine, Microsoft Azure, Amazon Elastic Compute Cloud (EC2), and AWS Elastic Beanstalk. ASP.NET and Visual Studio provide drag-and-drop development using server-interpreted controls. Validators can be added to form input fields, and data can be displayed in tabular format using DataGrid. Application Performance and Security Application Performance Caching techniques are used to reduce the cost of serving pages by exploiting commonalities between requests. Caching can be implemented at the server site (e.g., JDBC connection pooling, query result caching, HTML caching) or at the client's network (e.g., page caching by Web proxy). Application Security SQL Injection: Constructing queries using user inputs without proper parameterization can lead to SQL injection attacks. Password Leakage: Passwords should never be stored in clear text, and database access should be restricted by IP address. Authentication: Single-factor authentication (e.g., passwords) is risky. Two-factor authentication (e.g., password plus one-time password) is more secure. Application-Level Authorization: Current SQL standards do not allow fine-grained authorization. Workarounds include using views or extensions like Oracle Virtual Private Database (VPD). Audit Trails: Applications should log actions to detect security breaches and identify perpetrators. Audit trails are needed at both database and application levels. Challenges in Web Application Development User Interface and User Experience Scalability Performance Knowledge of Framework and Platforms Security Mobile Apps Definition: Software applications designed for mobile devices (e.g., smartphones, tablets). Characteristics: Designed for small, wireless devices Limited form factor, memory, computing power, power, and bandwidth May include sensors like accelerometers and touchscreens Types of Mobile Apps: Native Apps: Written in the native language of a platform (e.g., Objective-C for iOS, Java for Android). Platform-specific. Web Apps: Run inside Web browsers. Feature HTML/CSS interfaces and are powered by Web programming languages. Portable across devices. Hybrid Apps: Combine attributes of both native and Web apps. Use common code across platforms while tailoring required attributes to the native system. Design Issues for Mobile Apps: Determine device and resources Consider bandwidth Decide on architecture layers Select technology Define user interface Select navigation Maintain flow","title":"Summary of Lecture 7.5 - Application Design and Development5.pdf"},{"location":"week%208/Lecture%208.1%20-%20Algorithms%20and%20Data%20Structures1/","text":"Summary of Lecture 8.1 - Algorithms and Data Structures1.pdf Summary Module 36: Algorithms and Data Structures/1: Algorithms and Complexity Analysis Objectives Define Algorithms and their difference with Programs Analyze algorithms for performance of time, space, power, etc. Introduce Asymptotic notation for representation of complexity Consider complexity of common algorithms Outline Algorithms and Programs Analysis of Algorithms Complexity Chart Algorithms and Programs Algorithm: A finite sequence of well-defined, computer-implementable instructions, typically to solve a class of specific problems or to perform a computation. Program: A collection of instructions that can be executed by a computer to perform a specific task. A program implements an algorithm. A program may or may not terminate. Analysis of Algorithms Why Analyze? Practical reasons: Resources are scarce Greed to do more with less Avoid performance bugs Core Issues: Predict performance Compare algorithms Provide guarantees Understand theoretical basis What to Analyze? Time Space How to Analyze? Counting Models Asymptotic Analysis Where to Analyze? Best Case Worst Case Average Case Probabilistic Case Amortized Case Counting Models Core Idea: Total running time = Sum of cost \u00d7 frequency for all operations Need to analyze program to determine set of operations Cost depends on machine, compiler Frequency depends on algorithm, input data Asymptotic Analysis Core Idea: Cannot compare actual times; hence compare Growth or how time increases with input size Function Approximation (tilde (\u02dc) notation) Common Growth Functions Big-Oh (O(.)), Big-Omega (\u2126(.)), and Big-Theta (\u0398(.)) Notations Solve recurrence with Growth Functions Complexity Chart Presents the complexity of common algorithms in terms of time and space requirements Big-O notation is used to represent the worst-case complexity of an algorithm Different complexities are denoted by different symbols, e.g., O(1), O(log n), O(n), O(n log n), O(n^2), O(2^n) Module Summary Importance of analyzing algorithms for performance and efficiency Asymptotic notation for representing algorithm complexity Different types of analysis scenarios (best case, worst case, etc.) Complexity Chart for comparing common algorithms","title":"Summary of Lecture 8.1 - Algorithms and Data Structures1.pdf"},{"location":"week%208/Lecture%208.1%20-%20Algorithms%20and%20Data%20Structures1/#summary-of-lecture-81-algorithms-and-data-structures1pdf","text":"Summary Module 36: Algorithms and Data Structures/1: Algorithms and Complexity Analysis Objectives Define Algorithms and their difference with Programs Analyze algorithms for performance of time, space, power, etc. Introduce Asymptotic notation for representation of complexity Consider complexity of common algorithms Outline Algorithms and Programs Analysis of Algorithms Complexity Chart Algorithms and Programs Algorithm: A finite sequence of well-defined, computer-implementable instructions, typically to solve a class of specific problems or to perform a computation. Program: A collection of instructions that can be executed by a computer to perform a specific task. A program implements an algorithm. A program may or may not terminate. Analysis of Algorithms Why Analyze? Practical reasons: Resources are scarce Greed to do more with less Avoid performance bugs Core Issues: Predict performance Compare algorithms Provide guarantees Understand theoretical basis What to Analyze? Time Space How to Analyze? Counting Models Asymptotic Analysis Where to Analyze? Best Case Worst Case Average Case Probabilistic Case Amortized Case Counting Models Core Idea: Total running time = Sum of cost \u00d7 frequency for all operations Need to analyze program to determine set of operations Cost depends on machine, compiler Frequency depends on algorithm, input data Asymptotic Analysis Core Idea: Cannot compare actual times; hence compare Growth or how time increases with input size Function Approximation (tilde (\u02dc) notation) Common Growth Functions Big-Oh (O(.)), Big-Omega (\u2126(.)), and Big-Theta (\u0398(.)) Notations Solve recurrence with Growth Functions Complexity Chart Presents the complexity of common algorithms in terms of time and space requirements Big-O notation is used to represent the worst-case complexity of an algorithm Different complexities are denoted by different symbols, e.g., O(1), O(log n), O(n), O(n log n), O(n^2), O(2^n) Module Summary Importance of analyzing algorithms for performance and efficiency Asymptotic notation for representing algorithm complexity Different types of analysis scenarios (best case, worst case, etc.) Complexity Chart for comparing common algorithms","title":"Summary of Lecture 8.1 - Algorithms and Data Structures1.pdf"},{"location":"week%208/Lecture%208.2%20-%20Algorithms%20and%20Data%20Structures2/","text":"Summary of Lecture 8.2 - Algorithms and Data Structures2.pdf Summary Objectives and Outline Learning Objectives: Understand the concept of data structures Review linear data structures: arrays, lists, stacks, queues Review search algorithms: linear and binary Module Outline: Data structures Linear data structures Search Linear search Binary search Data Structure A data structure specifies the way of organizing and storing in-memory data that enables efficient access and modification of the data. There are two main types of data structures: Linear data structures Non-linear data structures Most data structures have a container for the data and typical operations that it needs to perform: Create Insert Delete Find/Search Close Efficiency is measured in terms of time and space taken for these operations. Linear Data Structures Linear data structures have data elements arranged in a linear or sequential manner such that each member element is connected to its previous and next element. Examples of linear data structures include: Array Linked list Queue Stack Array An array is a data structure that stores elements in contiguous memory locations. This allows for fast random access using its index. Advantages of Arrays: Efficient random access (O(1)) Simple implementation Disadvantages of Arrays: Fixed size Costly insertion/deletion of elements in the middle of the array Linked List A linked list is a data structure that stores elements in non-contiguous memory locations. Instead, each element stores a link (a pointer or a reference) to the location of the next element. Advantages of Linked Lists: Flexible size Efficient insertion/deletion of elements at any position Disadvantages of Linked Lists: Slower random access than arrays More complex implementation Search Search algorithms are used to find an element in a data structure. Linear Search Linear search is a sequential search algorithm that starts with the first element and compares each element with the given key until a match is found or the full list is traversed. Complexity of Linear Search: Best case: O(1) (when the element is found at the beginning of the list) Worst case: O(n) (when the element is not found or is at the end of the list) Binary Search Binary search is a more efficient search algorithm that works only on sorted lists. Binary search compares the key with the middle element of the list and based on the comparison, it narrows down the search by dividing the list into two halves. Complexity of Binary Search: Best case: O(1) (when the element is found at the middle of the list) Worst case: O(log n) (when the element is not found or is in either half)","title":"Summary of Lecture 8.2 - Algorithms and Data Structures2.pdf"},{"location":"week%208/Lecture%208.2%20-%20Algorithms%20and%20Data%20Structures2/#summary-of-lecture-82-algorithms-and-data-structures2pdf","text":"Summary","title":"Summary of Lecture 8.2 - Algorithms and Data Structures2.pdf"},{"location":"week%208/Lecture%208.2%20-%20Algorithms%20and%20Data%20Structures2/#objectives-and-outline","text":"Learning Objectives: Understand the concept of data structures Review linear data structures: arrays, lists, stacks, queues Review search algorithms: linear and binary Module Outline: Data structures Linear data structures Search Linear search Binary search","title":"Objectives and Outline"},{"location":"week%208/Lecture%208.2%20-%20Algorithms%20and%20Data%20Structures2/#data-structure","text":"A data structure specifies the way of organizing and storing in-memory data that enables efficient access and modification of the data. There are two main types of data structures: Linear data structures Non-linear data structures Most data structures have a container for the data and typical operations that it needs to perform: Create Insert Delete Find/Search Close Efficiency is measured in terms of time and space taken for these operations.","title":"Data Structure"},{"location":"week%208/Lecture%208.2%20-%20Algorithms%20and%20Data%20Structures2/#linear-data-structures","text":"Linear data structures have data elements arranged in a linear or sequential manner such that each member element is connected to its previous and next element. Examples of linear data structures include: Array Linked list Queue Stack","title":"Linear Data Structures"},{"location":"week%208/Lecture%208.2%20-%20Algorithms%20and%20Data%20Structures2/#array","text":"An array is a data structure that stores elements in contiguous memory locations. This allows for fast random access using its index. Advantages of Arrays: Efficient random access (O(1)) Simple implementation Disadvantages of Arrays: Fixed size Costly insertion/deletion of elements in the middle of the array","title":"Array"},{"location":"week%208/Lecture%208.2%20-%20Algorithms%20and%20Data%20Structures2/#linked-list","text":"A linked list is a data structure that stores elements in non-contiguous memory locations. Instead, each element stores a link (a pointer or a reference) to the location of the next element. Advantages of Linked Lists: Flexible size Efficient insertion/deletion of elements at any position Disadvantages of Linked Lists: Slower random access than arrays More complex implementation","title":"Linked List"},{"location":"week%208/Lecture%208.2%20-%20Algorithms%20and%20Data%20Structures2/#search","text":"Search algorithms are used to find an element in a data structure.","title":"Search"},{"location":"week%208/Lecture%208.2%20-%20Algorithms%20and%20Data%20Structures2/#linear-search","text":"Linear search is a sequential search algorithm that starts with the first element and compares each element with the given key until a match is found or the full list is traversed. Complexity of Linear Search: Best case: O(1) (when the element is found at the beginning of the list) Worst case: O(n) (when the element is not found or is at the end of the list)","title":"Linear Search"},{"location":"week%208/Lecture%208.2%20-%20Algorithms%20and%20Data%20Structures2/#binary-search","text":"Binary search is a more efficient search algorithm that works only on sorted lists. Binary search compares the key with the middle element of the list and based on the comparison, it narrows down the search by dividing the list into two halves. Complexity of Binary Search: Best case: O(1) (when the element is found at the middle of the list) Worst case: O(log n) (when the element is not found or is in either half)","title":"Binary Search"},{"location":"week%208/Lecture%208.3%20-%20Algorithms%20and%20Data%20Structures3/","text":"Summary of Lecture 8.3 - Algorithms and Data Structures3.pdf Summary Module 38: Algorithms and Data Structures Objectives and Outline Objective: Introduce non-linear data structures: graph, tree, hash table Explore Binary Search Tree (BST) Compare linear and non-linear data structures Outline: Non-linear Data Structures Binary Search Trees Comparison of Linear and Non-Linear Data Structures Data Structure A data structure organizes and stores data in memory for efficient access and modification. Types of data structures: Linear Data Structures (e.g., array, list, stack, queue) Non-linear Data Structures (e.g., graph, tree, hash table) Common operations: create, insert, delete, find/search, close Efficiency measured in time and space for these operations Non-linear Data Structures Non-linear data structures allow for multiple paths to connect elements. Elements may not have a single path to connect to others. Common types include: Graph: Undirected or Directed, Unweighted or Weighted Tree: Rooted or Unrooted, Binary or n-ary, Balanced or Unbalanced Hash Table: Array with lists and hash functions Skip List: Multi-layered interconnected linked lists Non-linear Data Structures: Why? Provide a balanced trade-off between extreme complexities of linear data structures. Offer satisfactory complexity for multiple operations. Graph Collection of vertices (which store elements) and connecting edges (links): G =< V, E > where E \u2286 V \u00d7 V Types: undirected/directed, unweighted/weighted, cyclic/acyclic, disconnected/connected Tree Connected acyclic graph representing hierarchical relationships. Types: rooted/unrooted, binary/n-ary, balanced/unbalanced, disconnected (forest)/connected Tree Terminology Root: topmost node Parent: node with a predecessor Child: node with a descendant Leaf: node with no children Level: distance of a node from the root Hash Table Implements associative array abstract data type Uses a hash function to compute an index into an array of buckets or slots Types: static or dynamic schemes, open addressing, 2-choice hashing Binary Search Tree Tree where all nodes hold the following properties: Value of each node in the left subtree is less than the value of its root Value of each node in the right subtree is greater than the value of its root Searching a Key in BST Compare key with the root element. Recursively search the left or right subtree depending on the comparison result. Worst-case complexity: O(n) Best-case complexity: O(lg n) Comparison of Linear and Non-Linear Data Structures Feature Linear Data Structure Non-Linear Data Structure Data Arrangement Linear order Hierarchical or networked Levels Single level Multiple levels Implementation Complexity Easy Complex Traversability One way Multiple ways Examples Array, stack, queue, linked list Trees, graphs, skip list, hash map Module Summary Introduced non-linear data structures: graph, tree, hash table Explored Binary Search Tree as an adaptation of binary search Compared Linear and Non-Linear Data Structures","title":"Summary of Lecture 8.3 - Algorithms and Data Structures3.pdf"},{"location":"week%208/Lecture%208.3%20-%20Algorithms%20and%20Data%20Structures3/#summary-of-lecture-83-algorithms-and-data-structures3pdf","text":"Summary Module 38: Algorithms and Data Structures Objectives and Outline Objective: Introduce non-linear data structures: graph, tree, hash table Explore Binary Search Tree (BST) Compare linear and non-linear data structures Outline: Non-linear Data Structures Binary Search Trees Comparison of Linear and Non-Linear Data Structures Data Structure A data structure organizes and stores data in memory for efficient access and modification. Types of data structures: Linear Data Structures (e.g., array, list, stack, queue) Non-linear Data Structures (e.g., graph, tree, hash table) Common operations: create, insert, delete, find/search, close Efficiency measured in time and space for these operations Non-linear Data Structures Non-linear data structures allow for multiple paths to connect elements. Elements may not have a single path to connect to others. Common types include: Graph: Undirected or Directed, Unweighted or Weighted Tree: Rooted or Unrooted, Binary or n-ary, Balanced or Unbalanced Hash Table: Array with lists and hash functions Skip List: Multi-layered interconnected linked lists Non-linear Data Structures: Why? Provide a balanced trade-off between extreme complexities of linear data structures. Offer satisfactory complexity for multiple operations. Graph Collection of vertices (which store elements) and connecting edges (links): G =< V, E > where E \u2286 V \u00d7 V Types: undirected/directed, unweighted/weighted, cyclic/acyclic, disconnected/connected Tree Connected acyclic graph representing hierarchical relationships. Types: rooted/unrooted, binary/n-ary, balanced/unbalanced, disconnected (forest)/connected Tree Terminology Root: topmost node Parent: node with a predecessor Child: node with a descendant Leaf: node with no children Level: distance of a node from the root Hash Table Implements associative array abstract data type Uses a hash function to compute an index into an array of buckets or slots Types: static or dynamic schemes, open addressing, 2-choice hashing Binary Search Tree Tree where all nodes hold the following properties: Value of each node in the left subtree is less than the value of its root Value of each node in the right subtree is greater than the value of its root Searching a Key in BST Compare key with the root element. Recursively search the left or right subtree depending on the comparison result. Worst-case complexity: O(n) Best-case complexity: O(lg n) Comparison of Linear and Non-Linear Data Structures Feature Linear Data Structure Non-Linear Data Structure Data Arrangement Linear order Hierarchical or networked Levels Single level Multiple levels Implementation Complexity Easy Complex Traversability One way Multiple ways Examples Array, stack, queue, linked list Trees, graphs, skip list, hash map Module Summary Introduced non-linear data structures: graph, tree, hash table Explored Binary Search Tree as an adaptation of binary search Compared Linear and Non-Linear Data Structures","title":"Summary of Lecture 8.3 - Algorithms and Data Structures3.pdf"},{"location":"week%208/Lecture%208.4%20-%20Storage%20and%20File%20Structure1/","text":"Summary of Lecture 8.4 - Storage and File Structure1.pdf Summary Introduction Physical storage media provides high-volume, fast, reliable, and inexpensive options for data storage for databases. Tertiary storage options are available for high-volume, inexpensive backup. Physical Storage Media Physical storage media can be classified based on: Speed: How quickly data can be accessed. Cost: Cost per unit of data. Reliability: Data loss potential on power failure or system crash. Volatility: Whether contents persist even when power is switched off. Types of Physical Storage Media 1. Cache Fastest and most costly form of storage. Volatile. Managed by the computer system hardware. 2. Main Memory Fast access (nanoseconds). Generally too small to store the entire database. Volatile. 3. Flash Memory Data survives power failure. Data can be written at a location only once, but location can be erased and written to again. Reads are roughly as fast as main memory. Writes are slow, erase is slower. 4. Magnetic Disk Data is stored on spinning disk and read/written magnetically. Primary medium for long-term storage of data. Slower access than main memory. Direct-access: possible to read data on disk in any order. 5. Optical Storage Non-volatile, data is read optically from a spinning disk using a laser. CD-ROM and DVD most popular forms. Blu-ray disks: larger capacity. Reads and writes are slower than with magnetic disk. 6. Tape Storage Non-volatile, used primarily for backup and archival data. Sequential-access: much slower than disk. Very high capacity. Tape jukeboxes provide massive storage capacities. Storage Hierarchy Storage media can be organized into a hierarchy based on performance and cost: Primary Storage: Cache, main memory, flash memory. Secondary Storage: Magnetic disk. Tertiary Storage: Magnetic tape. Magnetic Disks Magnetic disks are the primary medium for long-term storage of data in databases. Disk Mechanism Read-write head: Reads or writes magnetically encoded information. Surface divided into circular tracks. Each track is divided into sectors (smallest unit of data read or written). To read/write a sector, head moves to right track and platter spins. Magnetic Disk Performance Measures Access Time: Time to start data transfer. Data-Transfer Rate: Rate at which data can be retrieved or stored. Mean Time To Failure (MTTF): Average time disk is expected to run continuously without failure. Magnetic Tapes Magnetic tapes hold large volumes of data and provide high transfer rates. Capacity: Few GB to 330 GB Transfer rates: Few to 10's of MB/s Cheap, but tape drives are expensive. Limited to sequential access. Cloud Storage Cloud storage is purchased from a third-party cloud vendor who manages capacity, security, and durability. Accessed via traditional storage protocols or API. Scalable and flexible. Cost-effective for large storage requirements. Other Storage Optical Disk: CD-ROM, DVD, Blu-ray. Flash Drives: Removable and rewritable storage devices. SD Cards: Secure Digital cards, used in mobile devices. Future of Storage DNA Digital Storage: High storage density and stability. Quantum Memory: Quantum-mechanical version of ordinary memory. Summary Physical Storage Media provides various options for data storage based on speed, cost, reliability, and volatility. Magnetic Disks are the primary medium for long-term storage. Magnetic Tapes offer high-volume, inexpensive backup. Cloud Storage offers scalable and cost-effective storage. Future storage technologies, such as DNA Digital Storage and Quantum Memory, have potential for even greater storage capacity and performance.","title":"Summary of Lecture 8.4 - Storage and File Structure1.pdf"},{"location":"week%208/Lecture%208.4%20-%20Storage%20and%20File%20Structure1/#summary-of-lecture-84-storage-and-file-structure1pdf","text":"Summary Introduction Physical storage media provides high-volume, fast, reliable, and inexpensive options for data storage for databases. Tertiary storage options are available for high-volume, inexpensive backup. Physical Storage Media Physical storage media can be classified based on: Speed: How quickly data can be accessed. Cost: Cost per unit of data. Reliability: Data loss potential on power failure or system crash. Volatility: Whether contents persist even when power is switched off. Types of Physical Storage Media 1. Cache Fastest and most costly form of storage. Volatile. Managed by the computer system hardware. 2. Main Memory Fast access (nanoseconds). Generally too small to store the entire database. Volatile. 3. Flash Memory Data survives power failure. Data can be written at a location only once, but location can be erased and written to again. Reads are roughly as fast as main memory. Writes are slow, erase is slower. 4. Magnetic Disk Data is stored on spinning disk and read/written magnetically. Primary medium for long-term storage of data. Slower access than main memory. Direct-access: possible to read data on disk in any order. 5. Optical Storage Non-volatile, data is read optically from a spinning disk using a laser. CD-ROM and DVD most popular forms. Blu-ray disks: larger capacity. Reads and writes are slower than with magnetic disk. 6. Tape Storage Non-volatile, used primarily for backup and archival data. Sequential-access: much slower than disk. Very high capacity. Tape jukeboxes provide massive storage capacities. Storage Hierarchy Storage media can be organized into a hierarchy based on performance and cost: Primary Storage: Cache, main memory, flash memory. Secondary Storage: Magnetic disk. Tertiary Storage: Magnetic tape. Magnetic Disks Magnetic disks are the primary medium for long-term storage of data in databases. Disk Mechanism Read-write head: Reads or writes magnetically encoded information. Surface divided into circular tracks. Each track is divided into sectors (smallest unit of data read or written). To read/write a sector, head moves to right track and platter spins. Magnetic Disk Performance Measures Access Time: Time to start data transfer. Data-Transfer Rate: Rate at which data can be retrieved or stored. Mean Time To Failure (MTTF): Average time disk is expected to run continuously without failure. Magnetic Tapes Magnetic tapes hold large volumes of data and provide high transfer rates. Capacity: Few GB to 330 GB Transfer rates: Few to 10's of MB/s Cheap, but tape drives are expensive. Limited to sequential access. Cloud Storage Cloud storage is purchased from a third-party cloud vendor who manages capacity, security, and durability. Accessed via traditional storage protocols or API. Scalable and flexible. Cost-effective for large storage requirements. Other Storage Optical Disk: CD-ROM, DVD, Blu-ray. Flash Drives: Removable and rewritable storage devices. SD Cards: Secure Digital cards, used in mobile devices. Future of Storage DNA Digital Storage: High storage density and stability. Quantum Memory: Quantum-mechanical version of ordinary memory. Summary Physical Storage Media provides various options for data storage based on speed, cost, reliability, and volatility. Magnetic Disks are the primary medium for long-term storage. Magnetic Tapes offer high-volume, inexpensive backup. Cloud Storage offers scalable and cost-effective storage. Future storage technologies, such as DNA Digital Storage and Quantum Memory, have potential for even greater storage capacity and performance.","title":"Summary of Lecture 8.4 - Storage and File Structure1.pdf"},{"location":"week%208/Lecture%208.5%20-%20Storage%20and%20File%20Structure2/","text":"Summary of Lecture 8.5 - Storage and File Structure2.pdf Summary File Organization A database is essentially a collection of files, where each file is a sequence of records, and each record is a sequence of fields. The traditional approach assumes that each file contains records of only one type, and uses different files for different relations. This approach is simple to implement, but can be inefficient for certain types of queries. Fixed-Length Records In a fixed-length record, the size of each record is fixed. This simplifies record storage and access, but can lead to wasted space if records contain variable-length attributes. Free Lists A free list is a data structure that keeps track of the addresses of deleted records. When a record is deleted, its address is added to the free list. When a new record is created, the system can reuse an address from the free list, reducing the need for compaction. Variable-Length Records Variable-length records are used when the length of a record is not known in advance. This can arise due to the storage of multiple record types in a file, variable-length attributes, or repeating fields. Organization of Records in Files There are several ways to organize records in files: Heap: Records can be placed anywhere in the file where there is space. Sequential: Records are stored in sequential order, based on the value of the search key of each record. This is suitable for applications that require sequential processing of the entire file. Hashing: Records are assigned to blocks based on a hash function computed on some attribute of each record. This allows for efficient retrieval of records based on the hash value. Multitable Clustering: Records of several different relations can be stored in the same file. This can be beneficial for queries involving related records, but can be inefficient for queries involving only a single relation. Data Dictionary Storage The data dictionary stores metadata about the database, including: Information about relations, including names, attributes, and integrity constraints User and accounting information Statistical and descriptive data Physical file organization information Information about indices Storage Access To access data from a database file, the system uses a buffer manager. The buffer manager keeps a portion of the file in memory, and handles the transfer of data between memory and disk. Buffer Replacement Policy When the buffer is full and a new block needs to be loaded, the buffer manager must decide which block to replace. Common buffer replacement policies include: Least Recently Used (LRU): Replace the block that has not been used for the longest period of time. Most Recently Used (MRU): Replace the block that has been used most recently. Toss-Immediate: Free the space occupied by a block as soon as the final tuple of that block has been processed. Pinned Block: Do not allow a block to be replaced, even if it has not been used recently. The choice of buffer replacement policy depends on the access patterns of the database.","title":"Summary of Lecture 8.5 - Storage and File Structure2.pdf"},{"location":"week%208/Lecture%208.5%20-%20Storage%20and%20File%20Structure2/#summary-of-lecture-85-storage-and-file-structure2pdf","text":"Summary File Organization A database is essentially a collection of files, where each file is a sequence of records, and each record is a sequence of fields. The traditional approach assumes that each file contains records of only one type, and uses different files for different relations. This approach is simple to implement, but can be inefficient for certain types of queries. Fixed-Length Records In a fixed-length record, the size of each record is fixed. This simplifies record storage and access, but can lead to wasted space if records contain variable-length attributes. Free Lists A free list is a data structure that keeps track of the addresses of deleted records. When a record is deleted, its address is added to the free list. When a new record is created, the system can reuse an address from the free list, reducing the need for compaction. Variable-Length Records Variable-length records are used when the length of a record is not known in advance. This can arise due to the storage of multiple record types in a file, variable-length attributes, or repeating fields. Organization of Records in Files There are several ways to organize records in files: Heap: Records can be placed anywhere in the file where there is space. Sequential: Records are stored in sequential order, based on the value of the search key of each record. This is suitable for applications that require sequential processing of the entire file. Hashing: Records are assigned to blocks based on a hash function computed on some attribute of each record. This allows for efficient retrieval of records based on the hash value. Multitable Clustering: Records of several different relations can be stored in the same file. This can be beneficial for queries involving related records, but can be inefficient for queries involving only a single relation. Data Dictionary Storage The data dictionary stores metadata about the database, including: Information about relations, including names, attributes, and integrity constraints User and accounting information Statistical and descriptive data Physical file organization information Information about indices Storage Access To access data from a database file, the system uses a buffer manager. The buffer manager keeps a portion of the file in memory, and handles the transfer of data between memory and disk. Buffer Replacement Policy When the buffer is full and a new block needs to be loaded, the buffer manager must decide which block to replace. Common buffer replacement policies include: Least Recently Used (LRU): Replace the block that has not been used for the longest period of time. Most Recently Used (MRU): Replace the block that has been used most recently. Toss-Immediate: Free the space occupied by a block as soon as the final tuple of that block has been processed. Pinned Block: Do not allow a block to be replaced, even if it has not been used recently. The choice of buffer replacement policy depends on the access patterns of the database.","title":"Summary of Lecture 8.5 - Storage and File Structure2.pdf"},{"location":"week9/Lecture%209.1/","text":"Summary of Lecture 9.1.pdf Summary Introduction to Indexing Indexing is a technique used to speed up data retrieval in databases by organizing data in a way that enables efficient searching. An index contains a set of entries, each consisting of a search key and a pointer to the corresponding data record. Metrics for Evaluating Indices Indices can be evaluated based on: Access types supported, such as equality, range, or partial match queries Access time, the time taken to retrieve data using the index Insertion time, the time taken to add a new entry to the index Deletion time, the time taken to remove an entry from the index Space overhead, the additional storage space required for the index Ordered Indices Ordered indices store index entries in sorted order based on the search key. They include: Primary index: Specifies the sequential order of the file and is usually but not necessarily the primary key. Secondary index: Specifies an order different from the sequential order of the file. Dense Index Files Dense index files contain an index record for every search key value in the file, providing fast access to all data records. Sparse Index Files Sparse index files contain index records only for some search key values. They are applicable when records are sequentially ordered on the search key. Primary and Secondary Indices Primary index: Enables efficient sequential scanning but is expensive for non-sequential access. Secondary index: Supports non-sequential access but is less efficient than a primary index for sequential scanning. Multilevel Index When the primary index is too large to fit in memory, a multilevel index can be created with a sparse index of the primary index called an outer index and the primary index as the inner index. Index Update Indices must be updated when data records are modified, which can affect the performance and overhead of database operations. Example Consider a table with attributes \"Name\" and \"Phone\". An index on \"Name\" allows for efficient retrieval of phone numbers given a name, while an index on \"Phone\" enables the lookup of names by phone number. Benefits and Limitations of Indexing Benefits include faster data retrieval, improved query performance, and reduced I/O operations. Limitations include additional storage requirements, maintenance overhead during data modifications, and potential performance degradation for non-indexed data access patterns. Secondary Indices for Non-Key Fields Secondary indices can be created on non-key fields to facilitate efficient retrieval of records based on specific attributes, such as finding all instructors in a particular department or with a specified salary range.","title":"Summary of Lecture 9.1.pdf"},{"location":"week9/Lecture%209.1/#summary-of-lecture-91pdf","text":"Summary Introduction to Indexing Indexing is a technique used to speed up data retrieval in databases by organizing data in a way that enables efficient searching. An index contains a set of entries, each consisting of a search key and a pointer to the corresponding data record. Metrics for Evaluating Indices Indices can be evaluated based on: Access types supported, such as equality, range, or partial match queries Access time, the time taken to retrieve data using the index Insertion time, the time taken to add a new entry to the index Deletion time, the time taken to remove an entry from the index Space overhead, the additional storage space required for the index Ordered Indices Ordered indices store index entries in sorted order based on the search key. They include: Primary index: Specifies the sequential order of the file and is usually but not necessarily the primary key. Secondary index: Specifies an order different from the sequential order of the file. Dense Index Files Dense index files contain an index record for every search key value in the file, providing fast access to all data records. Sparse Index Files Sparse index files contain index records only for some search key values. They are applicable when records are sequentially ordered on the search key. Primary and Secondary Indices Primary index: Enables efficient sequential scanning but is expensive for non-sequential access. Secondary index: Supports non-sequential access but is less efficient than a primary index for sequential scanning. Multilevel Index When the primary index is too large to fit in memory, a multilevel index can be created with a sparse index of the primary index called an outer index and the primary index as the inner index. Index Update Indices must be updated when data records are modified, which can affect the performance and overhead of database operations. Example Consider a table with attributes \"Name\" and \"Phone\". An index on \"Name\" allows for efficient retrieval of phone numbers given a name, while an index on \"Phone\" enables the lookup of names by phone number. Benefits and Limitations of Indexing Benefits include faster data retrieval, improved query performance, and reduced I/O operations. Limitations include additional storage requirements, maintenance overhead during data modifications, and potential performance degradation for non-indexed data access patterns. Secondary Indices for Non-Key Fields Secondary indices can be created on non-key fields to facilitate efficient retrieval of records based on specific attributes, such as finding all instructors in a particular department or with a specified salary range.","title":"Summary of Lecture 9.1.pdf"},{"location":"week9/Lecture%209.2/","text":"Summary of Lecture 9.2.pdf Summary Module 42: Indexing and Hashing/2: Indexing/2 Objectives To recap Balanced Binary Search Trees as options for optimal in-memory search data structures To understand the issues relating to external search data structures for persistent data To study 2-3-4 Tree as a precursor to B/B+-Tree for an efficient external data structure for database and index tables Module Recap Appreciated the reasons for indexing database tables Understood the ordered indexes Module Outline Balanced Binary Search Trees 2-3-4 Tree Balanced Binary Search Trees Search Data Structures Linear Search: O(n) Binary Search: O(lg n) Balanced Binary Search Trees Height of the tree (h) \u223c O(lg n) Worst case time (n data items in the data structure): Search: O(h) \u223c O(lg n) Balancing Guarantees Worst-case: AVL Tree Randomized: Randomized BST, Skip List Amortized: Splay 2-3-4 Tree All leaves are at the same depth (the bottom level): Height, h \u223c O(lg n) All data is kept in sorted order Every node (leaf or internal) is a 2-node, 3-node or a 4-node (based on the number of links or children), and holds one, two, or three data elements, respectively Generalizes easily to larger nodes Extends to external data structures 2-3-4 Trees: Search Simple and natural extension of search in BST 2-3-4 Trees: Insert Search to find expected location If it is a 2 node, change to 3 node and insert If it is a 3 node, change to 4 node and insert If it is a 4 node, split the node by moving the middle item to parent node, then insert Node Splitting A 4-node is split as soon as it is encountered during a search from the root to a leaf The 4-node that is split will Be the root, or Have a 2-node parent, or Have a 3-node parent 2-3-4 Trees: Insert: Example Insert 10, 30, 60, 20, 50, 40, 70, 80, 15, 90, 100 2-3-4 Trees: Delete Locate the node n that contains the item theItem Find theItem\u2019s inorder successor and swap it with theItem (deletion will always be at a leaf) If that leaf is a 3-node or a 4-node, remove theItem To ensure that theItem does not occur in a 2-node Transform each 2-node encountered into a 3-node or a 4-node Reverse different cases illustrated for splitting 2-3-4 Tree Advantages All leaves are at the same depth (the bottom level): Height, h \u223c O(lg n) Complexity of search, insert and delete: O(h) \u223c O(lg n) All data is kept in sorted order Generalizes easily to larger nodes Extends to external data structures Disadvantages Uses variety of node types \u2013 need to destruct and construct multiple nodes for converting a 2 Node to 3 Node, a 3 Node to 4 Node, for splitting etc. 2-3-4 Trees Consider only one node type with space for 3 items and 4 links Internal node (non-root) has 2 to 4 children (links) Leaf node has 1 to 3 items Wastes some space, but has several advantages for external data structure Generalizes easily to larger nodes All paths from root to leaf are of the same length Each node that is not a root or a leaf has between \\(\\frac{n}{2}\\) and n children. A leaf node has between \\((\\frac{n-1}{2})\\) and n \u2212 1 values Special cases: If the root is not a leaf, it has at least 2 children. If the root is a leaf, it can have between 0 and (n \u2212 1) values. Extends to external data structures B-Tree 2-3-4 Tree is a B-Tree where n = 4 Module Summary Recapitulated the notions of Balanced Binary Search Trees as options for optimal in-memory search data structures Understood the issues relating to external data structures for persistent data Explored 2-3-4 Tree in depth as a precursor to B/B+-Tree for an efficient external data structure for database and index tables Slides used in this presentation are borrowed from http://db-book.com/ with kind permission of the authors. Edited and new slides are marked with \u201cPPD\u201d.","title":"Summary of Lecture 9.2.pdf"},{"location":"week9/Lecture%209.2/#summary-of-lecture-92pdf","text":"Summary Module 42: Indexing and Hashing/2: Indexing/2 Objectives To recap Balanced Binary Search Trees as options for optimal in-memory search data structures To understand the issues relating to external search data structures for persistent data To study 2-3-4 Tree as a precursor to B/B+-Tree for an efficient external data structure for database and index tables Module Recap Appreciated the reasons for indexing database tables Understood the ordered indexes Module Outline Balanced Binary Search Trees 2-3-4 Tree Balanced Binary Search Trees Search Data Structures Linear Search: O(n) Binary Search: O(lg n) Balanced Binary Search Trees Height of the tree (h) \u223c O(lg n) Worst case time (n data items in the data structure): Search: O(h) \u223c O(lg n) Balancing Guarantees Worst-case: AVL Tree Randomized: Randomized BST, Skip List Amortized: Splay 2-3-4 Tree All leaves are at the same depth (the bottom level): Height, h \u223c O(lg n) All data is kept in sorted order Every node (leaf or internal) is a 2-node, 3-node or a 4-node (based on the number of links or children), and holds one, two, or three data elements, respectively Generalizes easily to larger nodes Extends to external data structures 2-3-4 Trees: Search Simple and natural extension of search in BST 2-3-4 Trees: Insert Search to find expected location If it is a 2 node, change to 3 node and insert If it is a 3 node, change to 4 node and insert If it is a 4 node, split the node by moving the middle item to parent node, then insert Node Splitting A 4-node is split as soon as it is encountered during a search from the root to a leaf The 4-node that is split will Be the root, or Have a 2-node parent, or Have a 3-node parent 2-3-4 Trees: Insert: Example Insert 10, 30, 60, 20, 50, 40, 70, 80, 15, 90, 100 2-3-4 Trees: Delete Locate the node n that contains the item theItem Find theItem\u2019s inorder successor and swap it with theItem (deletion will always be at a leaf) If that leaf is a 3-node or a 4-node, remove theItem To ensure that theItem does not occur in a 2-node Transform each 2-node encountered into a 3-node or a 4-node Reverse different cases illustrated for splitting 2-3-4 Tree Advantages All leaves are at the same depth (the bottom level): Height, h \u223c O(lg n) Complexity of search, insert and delete: O(h) \u223c O(lg n) All data is kept in sorted order Generalizes easily to larger nodes Extends to external data structures Disadvantages Uses variety of node types \u2013 need to destruct and construct multiple nodes for converting a 2 Node to 3 Node, a 3 Node to 4 Node, for splitting etc. 2-3-4 Trees Consider only one node type with space for 3 items and 4 links Internal node (non-root) has 2 to 4 children (links) Leaf node has 1 to 3 items Wastes some space, but has several advantages for external data structure Generalizes easily to larger nodes All paths from root to leaf are of the same length Each node that is not a root or a leaf has between \\(\\frac{n}{2}\\) and n children. A leaf node has between \\((\\frac{n-1}{2})\\) and n \u2212 1 values Special cases: If the root is not a leaf, it has at least 2 children. If the root is a leaf, it can have between 0 and (n \u2212 1) values. Extends to external data structures B-Tree 2-3-4 Tree is a B-Tree where n = 4 Module Summary Recapitulated the notions of Balanced Binary Search Trees as options for optimal in-memory search data structures Understood the issues relating to external data structures for persistent data Explored 2-3-4 Tree in depth as a precursor to B/B+-Tree for an efficient external data structure for database and index tables Slides used in this presentation are borrowed from http://db-book.com/ with kind permission of the authors. Edited and new slides are marked with \u201cPPD\u201d.","title":"Summary of Lecture 9.2.pdf"},{"location":"week9/Lecture%209.3/","text":"Summary of Lecture 9.3.pdf Summary Introduction B+-Tree Index Files are a generalization of 2-3-4 Tree, allowing for efficient external data storage in database and index tables. Properties of B+-Tree Index Files Balanced binary search trees Multi-level index format Leaf nodes contain actual data pointers Leaf nodes are linked using a linked list, supporting both random access and sequential access Internal nodes contain pointers to child nodes and key values Leaf nodes store record pointers and key values Intermediate nodes branch from a range of values Search Traverses intermediary nodes to locate the leaf node containing the target record Sequential search is performed at the leaf node Insertion Inserts new records by splitting leaf nodes to maintain fill factor, balance, and order If the leaf node is full, it is split, and a new leaf node is created The intermediate node is also split, if necessary, to add a branch to the new leaf node Deletion Removes records by redistributing entries or merging underfull leaf nodes If a node becomes underfull, it is merged with a sibling node Intermediate nodes are also updated to reflect the changes File Organization B+-Tree index files automatically reorganize themselves, unlike indexed-sequential files (ISAM) Small, local changes can be made during insertions and deletions, without the need for periodic reorganization of the entire file Handling Non-Unique Keys Duplicate search keys are allowed Search keys in internal nodes may not be unique, but they are guaranteed to be less than or equal to the following key value Modified find procedure is used to traverse consecutive leaves to find all occurrences of a duplicate key Updates Insertion Finds the leaf node where the search-key value belongs Inserts the new entry into the leaf node If the leaf node is full, splits the node and propagates the split upwards Deletion Removes the record from the main file and from the bucket (if present) Removes the search-key value and pointer from the leaf node Merges underfull nodes or redistributes entries to maintain balance File Organization B+-Tree File Organization uses leaf nodes to store records instead of pointers Leaf nodes are required to be half full Insertion and deletion are handled similarly to B+-Tree index updates Non-Unique Keys Alternatives to storing duplicate keys in buckets include: Buckets on separate blocks (not recommended) List of tuple pointers with each key (extra code, deletion overhead) Making the search key unique by adding a record-identifier (extra storage overhead, simpler code) Record Relocation and Secondary Indices If a record moves, secondary indices using record pointers must be updated To reduce the cost of node splits, the primary-index search key can be used in the secondary index instead of the record pointer Extra traversal of the primary index is required, but node splits are cheaper Indexing Strings Variable length strings as keys introduce challenges: Variable fanout Prefix compression: Key values at internal nodes can be prefixes of full keys Keys in leaf nodes can be compressed by sharing common prefixes","title":"Summary of Lecture 9.3.pdf"},{"location":"week9/Lecture%209.3/#summary-of-lecture-93pdf","text":"Summary Introduction B+-Tree Index Files are a generalization of 2-3-4 Tree, allowing for efficient external data storage in database and index tables. Properties of B+-Tree Index Files Balanced binary search trees Multi-level index format Leaf nodes contain actual data pointers Leaf nodes are linked using a linked list, supporting both random access and sequential access Internal nodes contain pointers to child nodes and key values Leaf nodes store record pointers and key values Intermediate nodes branch from a range of values Search Traverses intermediary nodes to locate the leaf node containing the target record Sequential search is performed at the leaf node Insertion Inserts new records by splitting leaf nodes to maintain fill factor, balance, and order If the leaf node is full, it is split, and a new leaf node is created The intermediate node is also split, if necessary, to add a branch to the new leaf node Deletion Removes records by redistributing entries or merging underfull leaf nodes If a node becomes underfull, it is merged with a sibling node Intermediate nodes are also updated to reflect the changes File Organization B+-Tree index files automatically reorganize themselves, unlike indexed-sequential files (ISAM) Small, local changes can be made during insertions and deletions, without the need for periodic reorganization of the entire file Handling Non-Unique Keys Duplicate search keys are allowed Search keys in internal nodes may not be unique, but they are guaranteed to be less than or equal to the following key value Modified find procedure is used to traverse consecutive leaves to find all occurrences of a duplicate key Updates Insertion Finds the leaf node where the search-key value belongs Inserts the new entry into the leaf node If the leaf node is full, splits the node and propagates the split upwards Deletion Removes the record from the main file and from the bucket (if present) Removes the search-key value and pointer from the leaf node Merges underfull nodes or redistributes entries to maintain balance File Organization B+-Tree File Organization uses leaf nodes to store records instead of pointers Leaf nodes are required to be half full Insertion and deletion are handled similarly to B+-Tree index updates Non-Unique Keys Alternatives to storing duplicate keys in buckets include: Buckets on separate blocks (not recommended) List of tuple pointers with each key (extra code, deletion overhead) Making the search key unique by adding a record-identifier (extra storage overhead, simpler code) Record Relocation and Secondary Indices If a record moves, secondary indices using record pointers must be updated To reduce the cost of node splits, the primary-index search key can be used in the secondary index instead of the record pointer Extra traversal of the primary index is required, but node splits are cheaper Indexing Strings Variable length strings as keys introduce challenges: Variable fanout Prefix compression: Key values at internal nodes can be prefixes of full keys Keys in leaf nodes can be compressed by sharing common prefixes","title":"Summary of Lecture 9.3.pdf"},{"location":"week9/Lecture%209.4/","text":"Summary of Lecture 9.4.pdf Summary Static Hashing Static hashing is a hashing scheme where the hash function maps search-key values to a fixed set of bucket addresses. This means that the number of buckets is determined in advance and does not change as the database grows or shrinks. Advantages of Static Hashing Simplicity: Static hashing is relatively simple to implement and understand. Deterministic: The bucket address for a given search-key value is always the same, which makes it easy to locate records. Space efficiency: Static hashing can be space-efficient, as it only requires a fixed amount of space for the bucket address table. Disadvantages of Static Hashing Fixed number of buckets: The number of buckets is fixed in advance, which can lead to performance problems if the database grows or shrinks significantly. Bucket overflow: If a bucket becomes too full, it can overflow, which can lead to performance problems. Reorganization: If the database grows or shrinks significantly, the hash function may need to be recomputed and the data reorganized, which can be time-consuming. Dynamic Hashing Dynamic hashing is a hashing scheme where the number of buckets can change dynamically as the database grows or shrinks. This means that the hash function can be modified to accommodate the changing number of buckets. Advantages of Dynamic Hashing Flexibility: Dynamic hashing is more flexible than static hashing, as it can accommodate changes in the database size. Reduced overflow: Dynamic hashing can reduce bucket overflow by increasing the number of buckets as the database grows. Improved performance: Dynamic hashing can improve performance by reducing bucket overflow and by allowing the hash function to be modified to accommodate the changing number of buckets. Disadvantages of Dynamic Hashing Complexity: Dynamic hashing is more complex to implement and understand than static hashing. Overhead: Dynamic hashing can incur more overhead than static hashing, as it requires additional data structures to manage the changing number of buckets. Comparison of Static and Dynamic Hashing The following table compares static and dynamic hashing: Feature Static Hashing Dynamic Hashing Number of buckets Fixed Dynamic Bucket overflow Possible Unlikely Reorganization May be required Not required Complexity Simple Complex Overhead Low High Bitmap Indices Bitmap indices are a special type of index that is designed for efficient querying on multiple keys. Bitmap indices are created by storing a bitmap for each value of an attribute. The bitmap has as many bits as records in the relation, and each bit corresponds to a record. If a record has the value v for the attribute, then the corresponding bit in the bitmap for v is set to 1. Otherwise, the bit is set to 0. Bitmap indices can be used to answer queries on multiple attributes very efficiently. For example, to find all records where the gender is male and the income level is L1, we can perform the following operation: (bitmap-gender-male) AND (bitmap-income-level-L1) This operation will return a bitmap with 1s for all records that satisfy the query. We can then retrieve the records corresponding to the 1s in the bitmap. Bitmap indices are particularly useful for queries on attributes that take on a relatively small number of distinct values. This is because the size of a bitmap is proportional to the number of distinct values. Applications of Hashing and Bitmap Indices Hashing and bitmap indices are used in a variety of applications, including: Database management systems: Hashing and bitmap indices are used to improve the performance of database queries. Search engines: Hashing is used to index web pages and bitmap indices are used to filter search results. Data mining: Hashing and bitmap indices are used to identify patterns and trends in data. Computer security: Hashing is used to store passwords and other sensitive information in a secure way.","title":"Summary of Lecture 9.4.pdf"},{"location":"week9/Lecture%209.4/#summary-of-lecture-94pdf","text":"Summary","title":"Summary of Lecture 9.4.pdf"},{"location":"week9/Lecture%209.4/#static-hashing","text":"Static hashing is a hashing scheme where the hash function maps search-key values to a fixed set of bucket addresses. This means that the number of buckets is determined in advance and does not change as the database grows or shrinks.","title":"Static Hashing"},{"location":"week9/Lecture%209.4/#advantages-of-static-hashing","text":"Simplicity: Static hashing is relatively simple to implement and understand. Deterministic: The bucket address for a given search-key value is always the same, which makes it easy to locate records. Space efficiency: Static hashing can be space-efficient, as it only requires a fixed amount of space for the bucket address table.","title":"Advantages of Static Hashing"},{"location":"week9/Lecture%209.4/#disadvantages-of-static-hashing","text":"Fixed number of buckets: The number of buckets is fixed in advance, which can lead to performance problems if the database grows or shrinks significantly. Bucket overflow: If a bucket becomes too full, it can overflow, which can lead to performance problems. Reorganization: If the database grows or shrinks significantly, the hash function may need to be recomputed and the data reorganized, which can be time-consuming.","title":"Disadvantages of Static Hashing"},{"location":"week9/Lecture%209.4/#dynamic-hashing","text":"Dynamic hashing is a hashing scheme where the number of buckets can change dynamically as the database grows or shrinks. This means that the hash function can be modified to accommodate the changing number of buckets.","title":"Dynamic Hashing"},{"location":"week9/Lecture%209.4/#advantages-of-dynamic-hashing","text":"Flexibility: Dynamic hashing is more flexible than static hashing, as it can accommodate changes in the database size. Reduced overflow: Dynamic hashing can reduce bucket overflow by increasing the number of buckets as the database grows. Improved performance: Dynamic hashing can improve performance by reducing bucket overflow and by allowing the hash function to be modified to accommodate the changing number of buckets.","title":"Advantages of Dynamic Hashing"},{"location":"week9/Lecture%209.4/#disadvantages-of-dynamic-hashing","text":"Complexity: Dynamic hashing is more complex to implement and understand than static hashing. Overhead: Dynamic hashing can incur more overhead than static hashing, as it requires additional data structures to manage the changing number of buckets.","title":"Disadvantages of Dynamic Hashing"},{"location":"week9/Lecture%209.4/#comparison-of-static-and-dynamic-hashing","text":"The following table compares static and dynamic hashing: Feature Static Hashing Dynamic Hashing Number of buckets Fixed Dynamic Bucket overflow Possible Unlikely Reorganization May be required Not required Complexity Simple Complex Overhead Low High","title":"Comparison of Static and Dynamic Hashing"},{"location":"week9/Lecture%209.4/#bitmap-indices","text":"Bitmap indices are a special type of index that is designed for efficient querying on multiple keys. Bitmap indices are created by storing a bitmap for each value of an attribute. The bitmap has as many bits as records in the relation, and each bit corresponds to a record. If a record has the value v for the attribute, then the corresponding bit in the bitmap for v is set to 1. Otherwise, the bit is set to 0. Bitmap indices can be used to answer queries on multiple attributes very efficiently. For example, to find all records where the gender is male and the income level is L1, we can perform the following operation: (bitmap-gender-male) AND (bitmap-income-level-L1) This operation will return a bitmap with 1s for all records that satisfy the query. We can then retrieve the records corresponding to the 1s in the bitmap. Bitmap indices are particularly useful for queries on attributes that take on a relatively small number of distinct values. This is because the size of a bitmap is proportional to the number of distinct values.","title":"Bitmap Indices"},{"location":"week9/Lecture%209.4/#applications-of-hashing-and-bitmap-indices","text":"Hashing and bitmap indices are used in a variety of applications, including: Database management systems: Hashing and bitmap indices are used to improve the performance of database queries. Search engines: Hashing is used to index web pages and bitmap indices are used to filter search results. Data mining: Hashing and bitmap indices are used to identify patterns and trends in data. Computer security: Hashing is used to store passwords and other sensitive information in a secure way.","title":"Applications of Hashing and Bitmap Indices"},{"location":"week9/Lecture%209.5/","text":"Summary of Lecture 9.5.pdf Summary Index Definition in SQL In SQL, an index is a data structure that helps speed up the retrieval of data from a database. It is created on a specific column or columns of a table and consists of a set of entries, each of which contains the value of the indexed column(s) and a pointer to the corresponding row in the table. To create an index, you use the CREATE INDEX statement. The syntax of the statement is as follows: CREATE INDEX < index_name > ON < table_name > ( < column_list > ) For example, the following statement creates an index named idx_name on the name column of the student table: CREATE INDEX idx_name ON student ( name ) Multiple-Key Access A composite index is an index that is created on multiple columns of a table. This can be useful for speeding up queries that involve multiple columns, such as a query that searches for rows based on both the first name and last name of a person. To create a composite index, you use the CREATE INDEX statement and specify the multiple columns in the column_list parameter. For example, the following statement creates a composite index named idx_name_age on the name and age columns of the student table: CREATE INDEX idx_name_age ON student ( name , age ) Privileges To create an index, you must have the CREATE INDEX privilege on the table. This privilege is typically granted to database administrators and other users who need to create and manage indexes. Guidelines for Indexing There are a number of factors to consider when deciding whether to create an index on a particular column or set of columns. These factors include: The frequency with which the column(s) are used in queries The number of distinct values in the column(s) The size of the table The impact of the index on the performance of other operations, such as inserts and updates Ground Rules The following are some general guidelines for indexing: Rule 0: Indexes lead to an access-update tradeoff. Creating an index can speed up queries, but it can also slow down inserts, updates, and deletes. Rule 1: Index the correct tables. Only index tables that are frequently queried. Rule 2: Index the correct columns. Index columns that are frequently used in queries and that have a wide range of values. Rule 3: Limit the number of indexes for each table. Too many indexes can slow down updates and inserts. Rule 4: Choose the order of columns in composite indexes. The order of the columns in a composite index can affect the performance of queries. Rule 5: Gather statistics to make index usage more accurate. The database can use indexes more effectively when it has statistical information about the tables involved in the queries. Rule 6: Drop indexes that are no longer required. If an index is no longer used, it should be dropped to free up space and improve performance. Module Summary In this module, we have learned about the following concepts: How to create indexes in SQL The different types of indexes, including single-column indexes and composite indexes The privileges required to create indexes The guidelines for indexing The ground rules for indexing","title":"Summary of Lecture 9.5.pdf"},{"location":"week9/Lecture%209.5/#summary-of-lecture-95pdf","text":"Summary Index Definition in SQL In SQL, an index is a data structure that helps speed up the retrieval of data from a database. It is created on a specific column or columns of a table and consists of a set of entries, each of which contains the value of the indexed column(s) and a pointer to the corresponding row in the table. To create an index, you use the CREATE INDEX statement. The syntax of the statement is as follows: CREATE INDEX < index_name > ON < table_name > ( < column_list > ) For example, the following statement creates an index named idx_name on the name column of the student table: CREATE INDEX idx_name ON student ( name ) Multiple-Key Access A composite index is an index that is created on multiple columns of a table. This can be useful for speeding up queries that involve multiple columns, such as a query that searches for rows based on both the first name and last name of a person. To create a composite index, you use the CREATE INDEX statement and specify the multiple columns in the column_list parameter. For example, the following statement creates a composite index named idx_name_age on the name and age columns of the student table: CREATE INDEX idx_name_age ON student ( name , age ) Privileges To create an index, you must have the CREATE INDEX privilege on the table. This privilege is typically granted to database administrators and other users who need to create and manage indexes. Guidelines for Indexing There are a number of factors to consider when deciding whether to create an index on a particular column or set of columns. These factors include: The frequency with which the column(s) are used in queries The number of distinct values in the column(s) The size of the table The impact of the index on the performance of other operations, such as inserts and updates Ground Rules The following are some general guidelines for indexing: Rule 0: Indexes lead to an access-update tradeoff. Creating an index can speed up queries, but it can also slow down inserts, updates, and deletes. Rule 1: Index the correct tables. Only index tables that are frequently queried. Rule 2: Index the correct columns. Index columns that are frequently used in queries and that have a wide range of values. Rule 3: Limit the number of indexes for each table. Too many indexes can slow down updates and inserts. Rule 4: Choose the order of columns in composite indexes. The order of the columns in a composite index can affect the performance of queries. Rule 5: Gather statistics to make index usage more accurate. The database can use indexes more effectively when it has statistical information about the tables involved in the queries. Rule 6: Drop indexes that are no longer required. If an index is no longer used, it should be dropped to free up space and improve performance. Module Summary In this module, we have learned about the following concepts: How to create indexes in SQL The different types of indexes, including single-column indexes and composite indexes The privileges required to create indexes The guidelines for indexing The ground rules for indexing","title":"Summary of Lecture 9.5.pdf"}]}